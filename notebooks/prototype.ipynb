{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from hashlib import sha256\n",
    "\n",
    "def hash_file(filepath: Path) -> str:\n",
    "    \"\"\"Hash a file's contents using sha256 and return the hash hex digest.\"\"\"\n",
    "    hash = sha256()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash.update(chunk)\n",
    "    return hash.hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the structure of the file tree\n",
    "# User provides the root directory (or an s3-compatible bucket)\n",
    "# Under that root directory, we create:\n",
    "# - a data/ directory to store files, using the pattern: data/<hash>/<filename>\n",
    "# - a datasets/ directory to store datasets, which are versioned collections of files, using the pattern: datasets/<dataset_name>/<version_hash>/files.txt, where `files.txt` is a text file containing a newline-separated list of hashes of the files in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyprojroot import here\n",
    "def initialize_file_tree(root_dir: Path):\n",
    "    root_dir.mkdir(parents=True, exist_ok=True)\n",
    "    data_dir = root_dir / \"data\"\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    datasets_dir = root_dir / \"datasets\"\n",
    "    datasets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "initialize_file_tree(here() / \"data\" / \"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "import json5 as json\n",
    "import shutil\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GitData:\n",
    "    \"\"\"A class for storing data in a git-like structure.\"\"\"\n",
    "\n",
    "    root_dir: Path\n",
    "\n",
    "    def get_dataset(self, dataset_name: str) -> \"Dataset\":\n",
    "        \"\"\"Return a Dataset\"\"\"\n",
    "        dataset_dir = self.root_dir / \"datasets\" / dataset_name\n",
    "        if not dataset_dir.exists():\n",
    "            raise ValueError(f\"Dataset {dataset_name} does not exist.\")\n",
    "        return Dataset(self.root_dir, dataset_name)\n",
    "\n",
    "    def create_dataset(self, dataset_name: str) -> \"Dataset\":\n",
    "        \"\"\"Create a dataset by name.\"\"\"\n",
    "        dataset_dir = self.root_dir / \"datasets\" / dataset_name\n",
    "        if not dataset_dir.exists():\n",
    "            dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "        return Dataset(root_dir=self.root_dir, dataset_name=dataset_name)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetCommit:\n",
    "    \"\"\"The definition of a commit is the state of the dataset as checked in by a user.\n",
    "\n",
    "    The state of the dataset is defined by the files that are present in the dataset.\n",
    "    This is represented by hashing all of the files in the dataset.\n",
    "    The source of truth for the DatasetCommit\n",
    "    is stored in the commit.json file of each commit hash.\n",
    "\n",
    "    A DatasetCommit is always associated with a Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    root_dir: Path\n",
    "    dataset_name: str\n",
    "    version_hash: str = field(default=\"\")\n",
    "    commit_message: str = field(default=\"\")\n",
    "    def __post_init__(self):\n",
    "        self.data_dir = self.root_dir / \"data\"\n",
    "        self.dataset_dir = self.root_dir / \"datasets\" / self.dataset_name\n",
    "\n",
    "        # Special case behavior is needed if this is the first commit,\n",
    "        # in which case\n",
    "\n",
    "\n",
    "    def _read_json(self) -> dict:\n",
    "        with open(self.dataset_dir / self.version_hash / \"commit.json\", \"r+\") as f:\n",
    "            return json.loads(f.read())\n",
    "\n",
    "    def _file_hashes(self) -> List[str]:\n",
    "        try:\n",
    "            return self._read_json()[\"file_hashes\"]\n",
    "        except FileNotFoundError:\n",
    "            return []\n",
    "\n",
    "\n",
    "    def _file_dict(self) -> dict:\n",
    "        # Return a dictionary of the form {filename: file_hash_dir}\n",
    "        file_dict = {}\n",
    "        for file_hash in self._file_hashes():\n",
    "            filepath = sorted((self.data_dir / file_hash).glob(\"*\"))[0]\n",
    "            file_dict[filepath.name] = filepath\n",
    "        return file_dict\n",
    "\n",
    "    def _commit_message(self) -> str:\n",
    "        try:\n",
    "            return self._read_json()[\"commit_message\"]\n",
    "        except FileNotFoundError:\n",
    "            return \"\"\n",
    "\n",
    "    def parent_hash(self) -> str:\n",
    "        try:\n",
    "            return self._read_json()[\"parent_hash\"]\n",
    "        except FileNotFoundError:\n",
    "            return \"\"\n",
    "\n",
    "    def create(\n",
    "        self,\n",
    "        commit_message: str,\n",
    "        add_files: str | Path | List[str | Path] = None,\n",
    "        remove_files: str | Path | List[str | Path] = None,\n",
    "    ) -> str:\n",
    "        # First off, calculate the new state of the dataset,\n",
    "        # i.e. which files are going to be present.\n",
    "\n",
    "        hash_hexes = []\n",
    "        # For each file, create the hash directory and copy the file to it\n",
    "        if add_files is not None:\n",
    "            if isinstance(add_files, Path):\n",
    "                add_files = [add_files]\n",
    "            for filepath in add_files:\n",
    "                hash_hex = hash_file(Path(filepath))\n",
    "                hash_hexes.append(hash_hex)\n",
    "                file_hash_dir = self.data_dir / hash_hex\n",
    "                file_hash_dir.mkdir(parents=True, exist_ok=True)\n",
    "                # Copy the file, not move, to the hash directory.\n",
    "                shutil.copy(filepath, file_hash_dir / filepath.name)\n",
    "\n",
    "        hash_hexes.extend(self._file_hashes())\n",
    "\n",
    "        # Finally, compute the hash of the removed files\n",
    "        # and remove them from the hash_hexes\n",
    "        if remove_files is not None:\n",
    "            if isinstance(remove_files, (str, Path)):\n",
    "                remove_files = [remove_files]\n",
    "\n",
    "            for filename in remove_files:\n",
    "                hash_hex = self._file_dict()[str(filename)].parent.name\n",
    "                hash_hexes.remove(hash_hex)\n",
    "\n",
    "        # Sort the hash_hexes so that the order of the files in the dataset\n",
    "        # is deterministic.\n",
    "        hash_hexes = sorted(hash_hexes)\n",
    "\n",
    "        # Create a new version of the dataset by hashing\n",
    "        # the concatenation of the hash_hexes and the commit message.\n",
    "        hash_concat = \"\\n\".join(hash_hexes) + \"\\n\" + commit_message\n",
    "        hasher = sha256()\n",
    "        hasher.update(hash_concat.encode(\"utf-8\"))\n",
    "        version_hash = hasher.hexdigest()\n",
    "\n",
    "        # Write the new version of the dataset to the dataset directory.\n",
    "        dataset_version_dir = self.dataset_dir / version_hash\n",
    "        dataset_version_dir.mkdir(parents=True, exist_ok=True)\n",
    "        with open(dataset_version_dir / \"commit.json\", \"w+\") as f:\n",
    "            info = {\n",
    "                \"file_hashes\": hash_hexes,\n",
    "                \"parent_hash\": self.parent_hash(),\n",
    "                \"commit_message\": commit_message,\n",
    "            }\n",
    "            f.write(json.dumps(info))\n",
    "        return DatasetCommit(\n",
    "            root_dir=self.root_dir,\n",
    "            dataset_name=self.dataset_name,\n",
    "            version_hash=version_hash,\n",
    "            commit_message=commit_message,\n",
    "        )\n",
    "\n",
    "\n",
    "# A commit with an empty commit hash is an empty commit hash. This is a special case.\n",
    "commit1 = DatasetCommit(root_dir=here() / \"data\" / \"dummy\", dataset_name=\"test_dataset\")\n",
    "print(commit1._file_dict())\n",
    "print(commit1.version_hash)\n",
    "commit2 = commit1.create(commit_message=\"test from empty commit\", add_files=[here() / \"MANIFEST.in\"])\n",
    "print(commit2._file_dict())\n",
    "print(commit2.version_hash)\n",
    "commit3 = commit2.create(commit_message=\"test adding from existing commit\", remove_files=[\"MANIFEST.in\"])\n",
    "print(commit3._file_dict())\n",
    "print(commit3.version_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_commit_creation():\n",
    "    # This test allows me to read an existing commit and create descendent commits.\n",
    "    commit1 = DatasetCommit(\n",
    "        root_dir=here() / \"data\" / \"dummy\",\n",
    "        dataset_name=\"test_dataset\",\n",
    "        version_hash=\"3273234a22aca83f0846473a3b6e5fe5d5ceef1d8733f23947da44c43f4d49d3\",\n",
    "    )\n",
    "    print(commit1.version_hash)\n",
    "    print(commit1._file_hashes())\n",
    "    print(commit1._commit_message())\n",
    "    print(commit1._file_dict())\n",
    "\n",
    "    commit2 = commit1.create(\n",
    "        commit_message=\"test commit\", add_files=[here() / \"MANIFEST.in\"]\n",
    "    )\n",
    "    print(commit2._file_hashes())\n",
    "    print(commit2._commit_message())\n",
    "    print(commit2._file_dict())\n",
    "\n",
    "    commit3 = commit2.create(commit_message=\"test_remove\", remove_files=\"MANIFEST.in\")\n",
    "    print(commit3._file_hashes())\n",
    "    print(commit3._commit_message())\n",
    "    print(commit3._file_dict())\n",
    "\n",
    "    commit4 = commit3.create(\n",
    "        commit_message=\"test add pre-commit config\",\n",
    "        add_files=[here() / \".pre-commit-config.yaml\"],\n",
    "    )\n",
    "    print(commit4._file_hashes())\n",
    "    print(commit4._commit_message())\n",
    "    print(commit4._file_dict())\n",
    "\n",
    "\n",
    "test_commit_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Dataset:\n",
    "    \"\"\"A class for storing data in a git-like structure.\"\"\"\n",
    "    root_dir: Path\n",
    "    dataset_name: str\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self.data_dir = self.root_dir / \"data\"\n",
    "        self.dataset_dir = self.root_dir / \"datasets\" / self.dataset_name\n",
    "\n",
    "        self.commit = DatasetCommit(self.root_dir, self.dataset_name, self.current_version_hash())\n",
    "\n",
    "        metadata_path = self.dataset_dir / \"metadata.json\"\n",
    "        if not metadata_path.exists():\n",
    "            # Create an empty metadata file.\n",
    "            metadata = {\"current_version_hash\": None}\n",
    "            with open(metadata_path, \"w+\") as f:\n",
    "                f.write(json.dumps(metadata))\n",
    "\n",
    "    def current_version_hash(self):\n",
    "        \"\"\"Return the hash of the current version of the dataset.\"\"\"\n",
    "        with open(self.dataset_dir / \"metadata.json\", \"r\") as f:\n",
    "            metadata = json.loads(f.read())\n",
    "        return metadata[\"current_version_hash\"]\n",
    "\n",
    "    def commit(self, commit_message: str, files: Path | List[Path]) -> str:\n",
    "        \"\"\"Commit data files to the dataset.\"\"\"\n",
    "\n",
    "        # Update metadata.json with current hash.\n",
    "        with open(self.dataset_dir / \"metadata.json\", \"r\") as f:\n",
    "            metadata = json.loads(f.read())\n",
    "        metadata[\"current_version_hash\"] = version_hash\n",
    "        with open(self.dataset_dir / \"metadata.json\", \"w+\") as f:\n",
    "            f.write(json.dumps(metadata))\n",
    "\n",
    "    def metadata(self):\n",
    "        \"\"\"Return the metadata for the dataset.\"\"\"\n",
    "        with open(self.dataset_dir / \"metadata.json\", \"r\") as f:\n",
    "            return json.loads(f.read())\n",
    "\n",
    "    def checkout(self, version_hash: str) -> None:\n",
    "        \"\"\"Checkout a version of the dataset.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = GitData(here() / \"data\" / \"dummy\")\n",
    "\n",
    "dataset = datasets.create_dataset(\"test_dataset\")\n",
    "dataset.commit(\"First commit\", here() / \"README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.commit(\"Committing more files\", [here() / \"README.md\", here() / \"mkdocs.yaml\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.checkout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.files, dataset.current_version_hash"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gitdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
