{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kirin Documentation","text":"<p>Welcome to the Kirin documentation! Kirin is a simplified content-addressed storage system for data versioning that provides linear commit history for datasets.</p>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#install-from-source","title":"Install from source","text":"<pre><code>pip install git@github.com:ericmjl/kirin\n</code></pre>"},{"location":"#build-and-preview-docs","title":"Build and preview docs","text":"<pre><code>mkdocs serve\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#architecture-design","title":"Architecture &amp; Design","text":"<ul> <li>Design Document - Overall system architecture and design goals</li> <li>API Documentation - Programmatic API reference</li> </ul>"},{"location":"#technical-decisions","title":"Technical Decisions","text":"<ul> <li>HashFS Evaluation - Detailed analysis of HashFS   integration</li> <li>Ecosystem Due Diligence - Evaluation of   third-party libraries</li> <li>ADR: HashFS Integration - Architectural decision   record</li> </ul>"},{"location":"#cloud-storage","title":"Cloud Storage","text":"<ul> <li>Cloud Storage Authentication - Setting up cloud backends</li> </ul>"},{"location":"#why-kirin-exists","title":"Why Kirin Exists","text":"<p>Kirin addresses critical needs in machine learning and data science workflows:</p> <ul> <li>Linear Data Versioning: Track changes to datasets with simple, linear commits</li> <li>Content-Addressed Storage: Ensure data integrity and enable deduplication</li> <li>Multi-Backend Support: Work with S3, GCS, Azure, local filesystem, and more</li> <li>Serverless Architecture: No dedicated servers required</li> <li>Ergonomic Python API: Focus on ease of use and developer experience</li> <li>File Versioning: Track changes to individual files over time</li> </ul>"},{"location":"#key-benefits","title":"Key Benefits","text":"<ul> <li>Simplified workflows: Linear commit history without branching complexity</li> <li>Backend-agnostic: Works with any storage backend via fsspec</li> <li>Automatic deduplication: Identical files stored once, saving space</li> <li>Content integrity: Files stored by content hash for data integrity</li> <li>Performance optimized: Streaming operations for large files</li> <li>Extensible: Easy to add new backends and features</li> </ul>"},{"location":"adr-hashfs-evaluation/","title":"ADR-001: HashFS Integration Evaluation","text":"<p>Date: 2024-12-19 Status: Rejected Decision Makers: GitData Development Team</p>"},{"location":"adr-hashfs-evaluation/#context","title":"Context","text":"<p>GitData requires a robust content-addressed storage (CAS) system to support data versioning, deduplication, and multi-backend storage. During architecture evaluation, we considered integrating HashFS, a Python library for content-addressable file management.</p>"},{"location":"adr-hashfs-evaluation/#decision","title":"Decision","text":"<p>We will NOT integrate HashFS into GitData.</p> <p>Instead, we will enhance our existing custom content-addressed storage implementation with HashFS-inspired optimizations while maintaining our current fsspec-based architecture.</p>"},{"location":"adr-hashfs-evaluation/#rationale","title":"Rationale","text":""},{"location":"adr-hashfs-evaluation/#why-not-hashfs","title":"Why Not HashFS?","text":"<ol> <li>Architectural Incompatibility</li> <li>HashFS does not implement fsspec interface</li> <li>Would require complete rewrite of storage layer</li> <li> <p>Loss of multi-backend support (S3, GCS, Azure, etc.)</p> </li> <li> <p>Feature Gaps</p> </li> <li>No version control (commits, branches, merging)</li> <li>No metadata tracking (commit messages, authors, timestamps)</li> <li>No usage analytics or SQLite integration</li> <li> <p>Local filesystem only (no cloud scalability)</p> </li> <li> <p>Performance Limitations</p> </li> <li>Memory-intensive (loads entire files)</li> <li>No streaming support for large files</li> <li> <p>Single-threaded operations</p> </li> <li> <p>Maintenance Concerns</p> </li> <li>Last commit 5 years ago (July 2019)</li> <li>Appears unmaintained</li> <li>No community support for fsspec integration</li> </ol>"},{"location":"adr-hashfs-evaluation/#why-current-architecture-is-better","title":"Why Current Architecture is Better","text":"<ol> <li>Multi-backend Support</li> <li>Native fsspec integration</li> <li>Supports S3, GCS, Azure, local, HTTP, and more</li> <li> <p>Easy to add new backends</p> </li> <li> <p>Full Version Control</p> </li> <li>Git-like commits, branches, merging</li> <li>Complete metadata tracking</li> <li> <p>Usage analytics and lineage tracking</p> </li> <li> <p>Performance Optimized</p> </li> <li>Streaming operations for large files</li> <li>Memory-efficient file handling</li> <li> <p>Optimized for data science workflows</p> </li> <li> <p>Feature Complete</p> </li> <li>SQLite usage tracking</li> <li>Web UI for dataset management</li> <li>CLI and programmatic API</li> <li>Serverless architecture</li> </ol>"},{"location":"adr-hashfs-evaluation/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"adr-hashfs-evaluation/#option-1-full-hashfs-integration","title":"Option 1: Full HashFS Integration","text":"<ul> <li>Pros: Proven CAS implementation, automatic deduplication</li> <li>Cons: Complete architectural rewrite, loss of critical features</li> <li>Decision: Rejected - too much risk and effort</li> </ul>"},{"location":"adr-hashfs-evaluation/#option-2-hybrid-architecture","title":"Option 2: Hybrid Architecture","text":"<ul> <li>Pros: Best of both worlds, local deduplication</li> <li>Cons: Complex implementation, maintenance overhead</li> <li>Decision: Rejected - unnecessary complexity</li> </ul>"},{"location":"adr-hashfs-evaluation/#option-3-enhance-current-system","title":"Option 3: Enhance Current System \u2705","text":"<ul> <li>Pros: Maintains all features, incremental improvement</li> <li>Cons: Requires custom implementation</li> <li>Decision: Selected - optimal balance of features and effort</li> </ul>"},{"location":"adr-hashfs-evaluation/#implementation-plan","title":"Implementation Plan","text":""},{"location":"adr-hashfs-evaluation/#phase-1-add-deduplication","title":"Phase 1: Add Deduplication","text":"<pre><code>def commit_with_dedup(self, files: List[str]) -&gt; str:\n    \"\"\"Commit files with automatic deduplication.\"\"\"\n    for file_path in files:\n        hash_hex = self._hash_file(file_path)\n        if not self._file_exists(hash_hex):\n            self._store_file(file_path, hash_hex)\n        self._add_to_commit(hash_hex)\n</code></pre>"},{"location":"adr-hashfs-evaluation/#phase-2-optimize-directory-structure","title":"Phase 2: Optimize Directory Structure","text":"<pre><code>def _get_nested_path(self, hash_hex: str) -&gt; str:\n    \"\"\"Get nested directory path for hash (HashFS-inspired).\"\"\"\n    return f\"{hash_hex[:2]}/{hash_hex[2:4]}/{hash_hex[4:]}\"\n</code></pre>"},{"location":"adr-hashfs-evaluation/#phase-3-add-repair-capabilities","title":"Phase 3: Add Repair Capabilities","text":"<pre><code>def repair_storage(self) -&gt; Dict[str, Any]:\n    \"\"\"Repair and validate storage integrity.\"\"\"\n    # Implementation details...\n</code></pre>"},{"location":"adr-hashfs-evaluation/#consequences","title":"Consequences","text":""},{"location":"adr-hashfs-evaluation/#positive","title":"Positive","text":"<ul> <li>Maintains architecture: No disruption to existing features</li> <li>Adds optimizations: Deduplication, directory nesting, repair</li> <li>Preserves capabilities: Multi-backend, versioning, metadata</li> <li>Incremental improvement: Low-risk enhancement</li> </ul>"},{"location":"adr-hashfs-evaluation/#negative","title":"Negative","text":"<ul> <li>Custom implementation: Requires development effort</li> <li>No community support: HashFS has established patterns</li> <li>Maintenance burden: Need to maintain custom CAS system</li> </ul>"},{"location":"adr-hashfs-evaluation/#risks","title":"Risks","text":"<ul> <li>Performance: Need to ensure optimizations don't impact performance</li> <li>Compatibility: Must maintain fsspec compatibility</li> <li>Testing: Need comprehensive testing of new features</li> </ul>"},{"location":"adr-hashfs-evaluation/#monitoring","title":"Monitoring","text":"<p>We will monitor:</p> <ul> <li>Performance metrics: File storage/retrieval times</li> <li>Storage efficiency: Deduplication effectiveness</li> <li>User feedback: Developer experience with new features</li> <li>Ecosystem changes: New CAS libraries with fsspec support</li> </ul>"},{"location":"adr-hashfs-evaluation/#references","title":"References","text":"<ul> <li>HashFS Evaluation - Detailed technical analysis</li> <li>Ecosystem Due Diligence - Broader ecosystem evaluation</li> <li>GitData Design Document - Overall architecture</li> <li>HashFS GitHub Repository - Library documentation</li> </ul>"},{"location":"api/","title":"Kirin API Reference","text":""},{"location":"api/#core-classes","title":"Core Classes","text":""},{"location":"api/#dataset","title":"Dataset","text":"<p>The main class for working with Kirin datasets.</p> <pre><code>from kirin import Dataset\n\n# Create or load a dataset\ndataset = Dataset(root_dir=\"/path/to/data\", name=\"my-dataset\")\n</code></pre>"},{"location":"api/#constructor-parameters","title":"Constructor Parameters","text":"<pre><code>Dataset(\n    root_dir: Union[str, Path],           # Root directory for the dataset\n    name: str,                           # Name of the dataset\n    description: str = \"\",               # Description of the dataset\n    fs: Optional[fsspec.AbstractFileSystem] = None,  # Filesystem to use\n    # Cloud authentication parameters\n    aws_profile: Optional[str] = None,   # AWS profile for S3 authentication\n    gcs_token: Optional[Union[str, Path]] = None,  # GCS service account token\n    gcs_project: Optional[str] = None,   # GCS project ID\n    azure_account_name: Optional[str] = None,  # Azure account name\n    azure_account_key: Optional[str] = None,    # Azure account key\n    azure_connection_string: Optional[str] = None,  # Azure connection string\n)\n</code></pre>"},{"location":"api/#basic-operations","title":"Basic Operations","text":"<ul> <li><code>commit(message, add_files=None, remove_files=None)</code> - Commit changes to the dataset</li> <li><code>checkout(commit_hash=None)</code> - Switch to a specific commit (latest if None)</li> <li><code>files</code> - Dictionary of files in the current commit</li> <li><code>local_files()</code> - Context manager for accessing files as local paths</li> <li><code>history(limit=None)</code> - Get commit history</li> <li><code>get_file(filename)</code> - Get a file from the current commit</li> <li><code>read_file(filename)</code> - Read file content as text</li> <li><code>download_file(filename, target_path)</code> - Download file to local path</li> </ul>"},{"location":"api/#examples","title":"Examples","text":"<pre><code># Basic usage\ndataset = Dataset(root_dir=\"/data\", name=\"project\")\ndataset.commit(\"Initial commit\", add_files=[\"data.csv\"])\n\n# Cloud storage with authentication\ndataset = Dataset(\n    root_dir=\"s3://my-bucket/data\",\n    name=\"project\",\n    aws_profile=\"my-profile\"\n)\n\n# GCS with service account\ndataset = Dataset(\n    root_dir=\"gs://my-bucket/data\",\n    name=\"project\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"my-project\"\n)\n\n# Azure with connection string\ndataset = Dataset(\n    root_dir=\"az://my-container/data\",\n    name=\"project\",\n    azure_connection_string=\"DefaultEndpointsProtocol=https;...\"\n)\n</code></pre>"},{"location":"api/#catalog","title":"Catalog","text":"<p>The main class for managing collections of datasets.</p> <pre><code>from kirin import Catalog\n\n# Create or load a catalog\ncatalog = Catalog(root_dir=\"/path/to/data\")\n</code></pre>"},{"location":"api/#constructor-parameters_1","title":"Constructor Parameters","text":"<pre><code>Catalog(\n    root_dir: Union[str, fsspec.AbstractFileSystem],  # Root directory for the catalog\n    fs: Optional[fsspec.AbstractFileSystem] = None,  # Filesystem to use\n    # Cloud authentication parameters\n    aws_profile: Optional[str] = None,   # AWS profile for S3 authentication\n    gcs_token: Optional[Union[str, Path]] = None,  # GCS service account token\n    gcs_project: Optional[str] = None,   # GCS project ID\n    azure_account_name: Optional[str] = None,  # Azure account name\n    azure_account_key: Optional[str] = None,    # Azure account key\n    azure_connection_string: Optional[str] = None,  # Azure connection string\n)\n</code></pre>"},{"location":"api/#basic-operations_1","title":"Basic Operations","text":"<ul> <li><code>datasets()</code> - List all datasets in the catalog</li> <li><code>get_dataset(name)</code> - Get a specific dataset</li> <li><code>create_dataset(name, description=\"\")</code> - Create a new dataset</li> <li><code>__len__()</code> - Number of datasets in the catalog</li> </ul>"},{"location":"api/#examples_1","title":"Examples","text":"<pre><code># Basic usage\ncatalog = Catalog(root_dir=\"/data\")\ndatasets = catalog.datasets()\ndataset = catalog.get_dataset(\"my-dataset\")\n\n# Cloud storage with authentication\ncatalog = Catalog(\n    root_dir=\"s3://my-bucket/data\",\n    aws_profile=\"my-profile\"\n)\n\n# GCS with service account\ncatalog = Catalog(\n    root_dir=\"gs://my-bucket/data\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"my-project\"\n)\n</code></pre>"},{"location":"api/#web-ui","title":"Web UI","text":"<p>The web UI provides a graphical interface for Kirin operations.</p>"},{"location":"api/#routes","title":"Routes","text":"<ul> <li><code>/</code> - Home page for catalog management</li> <li><code>/catalogs/add</code> - Add new catalog</li> <li><code>/catalog/{catalog_id}</code> - View catalog and datasets</li> <li><code>/catalog/{catalog_id}/{dataset_name}</code> - View specific dataset</li> <li><code>/catalog/{catalog_id}/{dataset_name}/commit</code> - Commit interface</li> </ul>"},{"location":"api/#catalog-management","title":"Catalog Management","text":"<p>The web UI supports cloud authentication through CatalogConfig:</p> <pre><code>from kirin.web.config import CatalogConfig\n\n# Create catalog config with cloud auth\nconfig = CatalogConfig(\n    id=\"my-catalog\",\n    name=\"My Catalog\",\n    root_dir=\"s3://my-bucket/data\",\n    aws_profile=\"my-profile\"\n)\n\n# Convert to runtime catalog\ncatalog = config.to_catalog()\n</code></pre>"},{"location":"api/#cloud-authentication-in-web-ui","title":"Cloud Authentication in Web UI","text":"<p>The web UI automatically handles cloud authentication when you:</p> <ol> <li>Create a catalog with cloud storage URL (s3://, gs://, az://)</li> <li>The system will prompt for authentication parameters</li> <li>Credentials are stored securely in the catalog configuration</li> </ol>"},{"location":"api/#storage-format","title":"Storage Format","text":"<p>Kirin uses a simplified Git-like storage format:</p> <pre><code>data/\n\u251c\u2500\u2500 data/                 # Content-addressed file storage\n\u2502   \u2514\u2500\u2500 {hash[:2]}/{hash[2:]}\n\u251c\u2500\u2500 datasets/\n\u2502   \u2514\u2500\u2500 my-dataset/\n\u2502       \u2514\u2500\u2500 commits.json  # Linear commit history\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":""},{"location":"api/#common-exceptions","title":"Common Exceptions","text":"<ul> <li><code>ValueError</code> - Invalid operations (file not found, invalid commit hash, etc.)</li> <li><code>FileNotFoundError</code> - File not found in dataset</li> <li><code>HTTPException</code> - Web UI errors (catalog not found, validation errors)</li> </ul>"},{"location":"api/#example-error-handling","title":"Example Error Handling","text":"<pre><code>try:\n    dataset.checkout(\"nonexistent-commit\")\nexcept ValueError as e:\n    print(f\"Checkout failed: {e}\")\n\ntry:\n    content = dataset.read_file(\"nonexistent.txt\")\nexcept FileNotFoundError as e:\n    print(f\"File not found: {e}\")\n</code></pre>"},{"location":"api/#best-practices","title":"Best Practices","text":""},{"location":"api/#dataset-naming","title":"Dataset Naming","text":"<ul> <li>Use descriptive names: <code>user-data</code>, <code>ml-experiments</code>, <code>production-models</code></li> <li>Avoid generic names: <code>test</code>, <code>data</code>, <code>temp</code></li> </ul>"},{"location":"api/#workflow-patterns","title":"Workflow Patterns","text":"<ul> <li>Commit changes regularly with descriptive messages</li> <li>Use linear commit history for simplicity</li> <li>Keep datasets focused on specific use cases</li> <li>Use catalogs to organize related datasets</li> </ul>"},{"location":"api/#file-management","title":"File Management","text":"<ul> <li>Use <code>local_files()</code> context manager for library compatibility</li> <li>Commit changes after adding/removing files</li> <li>Use descriptive commit messages</li> </ul>"},{"location":"api/#advanced-features","title":"Advanced Features","text":""},{"location":"api/#context-managers","title":"Context Managers","text":"<pre><code># Access files as local paths\nwith dataset.local_files() as local_files:\n    df = pd.read_csv(local_files[\"data.csv\"])\n    # Files automatically cleaned up\n</code></pre>"},{"location":"api/#commit-history","title":"Commit History","text":"<pre><code># Get commit history\nhistory = dataset.history(limit=10)\nfor commit in history:\n    print(f\"{commit.hash}: {commit.message}\")\n</code></pre>"},{"location":"api/#file-operations","title":"File Operations","text":"<pre><code># Add files to commit\ndataset.commit(\"Add new data\", add_files=[\"new_data.csv\"])\n\n# Remove files from commit\ndataset.commit(\"Remove old data\", remove_files=[\"old_data.csv\"])\n\n# Combined operations\ndataset.commit(\"Update dataset\",\n              add_files=[\"new_data.csv\"],\n              remove_files=[\"old_data.csv\"])\n</code></pre>"},{"location":"api/#cloud-storage-integration","title":"Cloud Storage Integration","text":"<pre><code># AWS S3 with profile\ndataset = Dataset(\n    root_dir=\"s3://my-bucket/data\",\n    name=\"my-dataset\",\n    aws_profile=\"production\"\n)\n\n# GCS with service account\ndataset = Dataset(\n    root_dir=\"gs://my-bucket/data\",\n    name=\"my-dataset\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"my-project\"\n)\n\n# Azure with connection string\ndataset = Dataset(\n    root_dir=\"az://my-container/data\",\n    name=\"my-dataset\",\n    azure_connection_string=\"DefaultEndpointsProtocol=https;...\"\n)\n</code></pre> <p>For detailed examples and cloud storage setup, see the Cloud Storage Authentication Guide.</p>"},{"location":"cloud-storage-auth/","title":"Cloud Storage Authentication Guide","text":"<p>When using <code>kirin</code> with cloud storage backends (S3, GCS, Azure, etc.), you need to provide authentication credentials. This guide shows you how to authenticate with different cloud providers using the new cloud-agnostic authentication parameters.</p>"},{"location":"cloud-storage-auth/#new-cloud-agnostic-authentication-recommended","title":"New Cloud-Agnostic Authentication (Recommended)","text":"<p>Kirin now supports cloud-agnostic authentication parameters that work across all cloud providers:</p>"},{"location":"cloud-storage-auth/#awss3-authentication","title":"AWS/S3 Authentication","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Using AWS profile\ncatalog = Catalog(\n    root_dir=\"s3://my-bucket/data\",\n    aws_profile=\"my-profile\"\n)\n\n# Using Dataset with AWS profile\ndataset = Dataset(\n    root_dir=\"s3://my-bucket/data\",\n    name=\"my-dataset\",\n    aws_profile=\"my-profile\"\n)\n</code></pre>"},{"location":"cloud-storage-auth/#gcpgcs-authentication","title":"GCP/GCS Authentication","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Using service account key file\ncatalog = Catalog(\n    root_dir=\"gs://my-bucket/data\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"my-project\"\n)\n\n# Using Dataset with GCS credentials\ndataset = Dataset(\n    root_dir=\"gs://my-bucket/data\",\n    name=\"my-dataset\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"my-project\"\n)\n</code></pre>"},{"location":"cloud-storage-auth/#azure-blob-storage-authentication","title":"Azure Blob Storage Authentication","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Using connection string\ncatalog = Catalog(\n    root_dir=\"az://my-container/data\",\n    azure_connection_string=\"DefaultEndpointsProtocol=https;...\"\n)\n\n# Using account name and key\ndataset = Dataset(\n    root_dir=\"az://my-container/data\",\n    name=\"my-dataset\",\n    azure_account_name=\"my-account\",\n    azure_account_key=\"my-key\"\n)\n</code></pre>"},{"location":"cloud-storage-auth/#web-ui-integration","title":"Web UI Integration","text":"<p>The web UI also supports cloud authentication through the <code>CatalogConfig.to_catalog()</code> method:</p> <pre><code>from kirin.web.config import CatalogConfig\n\n# Create catalog config with cloud auth\nconfig = CatalogConfig(\n    id=\"my-catalog\",\n    name=\"My Catalog\",\n    root_dir=\"s3://my-bucket/data\",\n    aws_profile=\"my-profile\"\n)\n\n# Convert to runtime catalog\ncatalog = config.to_catalog()\n</code></pre>"},{"location":"cloud-storage-auth/#legacy-authentication-methods","title":"Legacy Authentication Methods","text":"<p>The following sections show the legacy authentication methods that still work but are less convenient than the new cloud-agnostic approach.</p>"},{"location":"cloud-storage-auth/#google-cloud-storage-gcs","title":"Google Cloud Storage (GCS)","text":""},{"location":"cloud-storage-auth/#error-you-might-see","title":"Error You Might See","text":"<pre><code>gcsfs.retry.HttpError: Anonymous caller does not have storage.objects.list access\nto the Google Cloud Storage bucket. Permission 'storage.objects.list' denied on\nresource (or it may not exist)., 401\n</code></pre>"},{"location":"cloud-storage-auth/#solutions","title":"Solutions","text":""},{"location":"cloud-storage-auth/#option-1-application-default-credentials-recommended","title":"Option 1: Application Default Credentials (Recommended)","text":"<p>Use <code>gcloud</code> CLI to set up credentials:</p> <pre><code># Install gcloud CLI first, then:\ngcloud auth application-default login\n</code></pre> <p>Then use kirin normally:</p> <pre><code>from kirin.dataset import Dataset\n\n# Will automatically use your gcloud credentials\nds = Dataset(root_dir=\"gs://my-bucket/datasets\", dataset_name=\"my_data\")\n</code></pre>"},{"location":"cloud-storage-auth/#option-2-service-account-key-file","title":"Option 2: Service Account Key File","text":"<pre><code>from kirin.dataset import Dataset\nimport fsspec\n\n# Create filesystem with service account\nfs = fsspec.filesystem(\n    'gs',\n    token='/path/to/service-account-key.json'\n)\n\n# Pass it to Dataset\nds = Dataset(\n    root_dir=\"gs://my-bucket/datasets\",\n    dataset_name=\"my_data\",\n    fs=fs  # Use authenticated filesystem\n)\n</code></pre>"},{"location":"cloud-storage-auth/#option-3-environment-variable","title":"Option 3: Environment Variable","text":"<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/service-account-key.json\"\n</code></pre> <pre><code>from kirin.dataset import Dataset\n\n# Will automatically use the credentials from environment variable\nds = Dataset(root_dir=\"gs://my-bucket/datasets\", dataset_name=\"my_data\")\n</code></pre>"},{"location":"cloud-storage-auth/#option-4-pass-credentials-directly","title":"Option 4: Pass Credentials Directly","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    'gs',\n    project='my-project-id',\n    token='cloud'  # Uses gcloud credentials\n)\n\nds = Dataset(root_dir=\"gs://my-bucket/datasets\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#amazon-s3","title":"Amazon S3","text":""},{"location":"cloud-storage-auth/#option-1-aws-cli-credentials-recommended","title":"Option 1: AWS CLI Credentials (Recommended)","text":"<pre><code># Configure AWS credentials\naws configure\n</code></pre> <p>Then use normally:</p> <pre><code>from kirin.dataset import Dataset\n\nds = Dataset(root_dir=\"s3://my-bucket/datasets\", dataset_name=\"my_data\")\n</code></pre>"},{"location":"cloud-storage-auth/#option-2-environment-variables","title":"Option 2: Environment Variables","text":"<pre><code>export AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\nexport AWS_DEFAULT_REGION=\"us-east-1\"\n</code></pre>"},{"location":"cloud-storage-auth/#option-3-pass-credentials-explicitly","title":"Option 3: Pass Credentials Explicitly","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    's3',\n    key='your-access-key',\n    secret='your-secret-key',\n    client_kwargs={'region_name': 'us-east-1'}\n)\n\nds = Dataset(root_dir=\"s3://my-bucket/datasets\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#s3-compatible-services-minio-backblaze-b2-digitalocean-spaces","title":"S3-Compatible Services (Minio, Backblaze B2, DigitalOcean Spaces)","text":""},{"location":"cloud-storage-auth/#minio-example","title":"Minio Example","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    's3',\n    key='your-access-key',\n    secret='your-secret-key',\n    client_kwargs={\n        'endpoint_url': 'http://localhost:9000'  # Your Minio endpoint\n    }\n)\n\nds = Dataset(root_dir=\"s3://my-bucket/datasets\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#backblaze-b2-example","title":"Backblaze B2 Example","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    's3',\n    key='your-application-key-id',\n    secret='your-application-key',\n    client_kwargs={\n        'endpoint_url': 'https://s3.us-west-002.backblazeb2.com'\n    }\n)\n\nds = Dataset(root_dir=\"s3://my-bucket/datasets\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#azure-blob-storage","title":"Azure Blob Storage","text":""},{"location":"cloud-storage-auth/#option-1-azure-cli-credentials","title":"Option 1: Azure CLI Credentials","text":"<pre><code>az login\n</code></pre>"},{"location":"cloud-storage-auth/#option-2-connection-string","title":"Option 2: Connection String","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    'az',\n    connection_string='your-connection-string'\n)\n\nds = Dataset(root_dir=\"az://container/path\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#option-3-account-key","title":"Option 3: Account Key","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    'az',\n    account_name='your-account-name',\n    account_key='your-account-key'\n)\n\nds = Dataset(root_dir=\"az://container/path\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#general-pattern","title":"General Pattern","text":"<p>For any cloud provider, the recommended pattern is:</p> <ol> <li>New Cloud-Agnostic Approach (Recommended):</li> </ol> <pre><code>from kirin import Catalog, Dataset\n\n# For AWS/S3\ncatalog = Catalog(root_dir=\"s3://bucket/path\", aws_profile=\"my-profile\")\ndataset = Dataset(root_dir=\"s3://bucket/path\", name=\"my_data\", aws_profile=\"my-profile\")\n\n# For GCS\ncatalog = Catalog(root_dir=\"gs://bucket/path\", gcs_token=\"/path/to/key.json\", gcs_project=\"my-project\")\ndataset = Dataset(root_dir=\"gs://bucket/path\", name=\"my_data\", gcs_token=\"/path/to/key.json\", gcs_project=\"my-project\")\n\n# For Azure\ncatalog = Catalog(root_dir=\"az://container/path\", azure_connection_string=\"...\")\ndataset = Dataset(root_dir=\"az://container/path\", name=\"my_data\", azure_connection_string=\"...\")\n</code></pre> <ol> <li>Auto-detect (works if credentials are already configured):</li> </ol> <pre><code>ds = Dataset(root_dir=\"protocol://bucket/path\", name=\"my_data\")\n</code></pre> <ol> <li>Legacy explicit authentication (still supported):</li> </ol> <pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\n# Create authenticated filesystem\nfs = fsspec.filesystem('protocol', **auth_kwargs)\n\n# Pass to Dataset\nds = Dataset(root_dir=\"protocol://bucket/path\", name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#testing-without-credentials","title":"Testing Without Credentials","text":"<p>For local development/testing without cloud access, you can use:</p> <pre><code># In-memory filesystem (no cloud needed)\nfrom kirin.dataset import Dataset\n\nds = Dataset(root_dir=\"memory://test-data\", dataset_name=\"my_data\")\n</code></pre>"},{"location":"cloud-storage-auth/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cloud-storage-auth/#issue-anonymous-caller-or-access-denied","title":"Issue: \"Anonymous caller\" or \"Access Denied\"","text":"<ul> <li>Cause: No credentials provided</li> <li>Solution: Set up credentials using one of the methods above</li> </ul>"},{"location":"cloud-storage-auth/#issue-permission-denied","title":"Issue: \"Permission denied\"","text":"<ul> <li>Cause: Credentials don't have required permissions</li> <li>Solution: Ensure your IAM role/service account has read/write permissions   on the bucket</li> </ul>"},{"location":"cloud-storage-auth/#issue-bucket-does-not-exist","title":"Issue: \"Bucket does not exist\"","text":"<ul> <li>Cause: Bucket name is incorrect or doesn't exist</li> <li>Solution: Create the bucket first or check the bucket name</li> </ul>"},{"location":"cloud-storage-auth/#issue-import-errors-like-no-module-named-s3fs","title":"Issue: Import errors like \"No module named 's3fs'\"","text":"<ul> <li>Cause: Cloud storage package not installed</li> <li>Solution: Install required package:</li> </ul> <pre><code>pip install s3fs      # For S3\npip install gcsfs     # For GCS\npip install adlfs     # For Azure\n</code></pre>"},{"location":"cloud-storage-auth/#known-issues","title":"Known Issues","text":""},{"location":"cloud-storage-auth/#macos-python-313-ssl-certificate-verification","title":"macOS Python 3.13 SSL Certificate Verification","text":"<p>On macOS with Python 3.13, you may encounter SSL certificate verification errors when using cloud storage backends. This is a known Python/macOS issue.</p> <p>Workaround: The web UI skips connection testing during backend creation. Backends are validated when actually used. If you encounter SSL errors during actual usage, install certificates:</p> <pre><code>/Applications/Python\\ 3.13/Install\\ Certificates.command\n</code></pre> <p>Or use <code>certifi</code>:</p> <pre><code>pip install --upgrade certifi\n</code></pre>"},{"location":"design/","title":"Kirin: Simplified Content-Addressed Storage for Data Versioning","text":""},{"location":"design/#design-document","title":"Design Document","text":""},{"location":"design/#1-introduction","title":"1. Introduction","text":"<p>Kirin is a simplified tool for version-controlling data using content-addressed storage. The primary goals are to enable file versioning and file set versioning with a linear commit history that is backend-agnostic, serverless, and focuses on ergonomic Python API.</p>"},{"location":"design/#11-problem-statement","title":"1.1 Problem Statement","text":"<p>Data versioning is a critical need in machine learning and data science workflows. Current solutions often:</p> <ul> <li>Are tied to specific storage backends</li> <li>Lack the flexibility of Git's content-addressed model</li> <li>Require server infrastructure</li> <li>Do not track data usage effectively</li> <li>Are difficult to integrate with existing workflows and tools</li> <li>Fail to maintain lineage between derived files and their sources</li> <li>Inefficiently copy data when performing operations, leading to excessive   memory usage and slow performance</li> </ul>"},{"location":"design/#12-design-goals","title":"1.2 Design Goals","text":"<p>Kirin aims to provide a simplified, robust solution with the following properties:</p> <ol> <li>Backend-agnostic storage: Support any storage backend (local filesystem,    S3, Dropbox, Google Drive, SharePoint, etc.)</li> <li>Content-addressed storage: Use hashing to ensure data integrity and    deduplication</li> <li>Linear commit history: Simple, linear versioning without branching    complexity</li> <li>Serverless architecture: No need for dedicated servers; all logic    runs client-side</li> <li>Ergonomic Python API: Focus on ease of use and developer experience</li> <li>File versioning: Track changes to individual files over time</li> <li>Zero-copy operations: Minimize data copying through memory mapping and    streaming operations whenever possible</li> <li>Clean API: Provide a programmatic API optimized for Python workflows</li> </ol>"},{"location":"design/#13-jobs-to-be-done","title":"1.3 Jobs to be Done","text":"<p>Using Clayton Christensen's \"Jobs to be Done\" framework, we can identify the key user personas and the specific jobs they need to accomplish with Kirin:</p>"},{"location":"design/#131-data-scientist-ml-engineer","title":"1.3.1 Data Scientist / ML Engineer","text":"<p>Jobs to be Done:</p> <ol> <li>Track Experiment Data: \"I need to keep track of which datasets were used    in which experiments so I can reproduce my results.\"</li> <li>Kirin enables this through content-addressed storage and automatic lineage    tracking.</li> <li> <p>Example: \"When my model performed exceptionally well, I could trace back    exactly which version of the dataset was used and recreate the conditions.\"</p> </li> <li> <p>Find and Use the Right Data Version: \"I need to identify and access    specific versions of datasets for training models.\"</p> </li> <li>Kirin's versioning system with explicit commits makes this possible.</li> <li> <p>Example: \"I needed to compare model performance on data from Q1 vs Q2, and    Kirin let me checkout each version effortlessly.\"</p> </li> <li> <p>Collaborate with Team Members: \"I need to share datasets with colleagues    in a way that ensures we're all using the same exact data.\"</p> </li> <li>The content-addressed storage ensures data integrity across team members.</li> <li> <p>Example: \"My colleague in another country could reproduce my analysis    because Kirin guaranteed we had identical datasets.\"</p> </li> <li> <p>Document Data Transformations: \"I need to track how raw data is    transformed into model-ready data.\"</p> </li> <li>The lineage tracking captures the entire transformation pipeline.</li> <li>Example: \"When questioned about our preprocessing steps, I could show the    exact sequence of transformations from raw to processed data.\"</li> </ol>"},{"location":"design/#132-data-engineer","title":"1.3.2 Data Engineer","text":"<p>Jobs to be Done:</p> <ol> <li>Manage Data Pipelines: \"I need to ensure data pipelines produce consistent, traceable outputs.\"</li> <li>Kirin's automatic tracking of inputs and outputs creates a clear audit trail.</li> <li> <p>Example: \"When a downstream process broke, I could trace back through the pipeline to identify which transformation introduced the issue.\"</p> </li> <li> <p>Optimize Storage Usage: \"I need to handle large datasets efficiently without wasting storage.\"</p> </li> <li>Content-addressed storage with deduplication and zero-copy operations reduces storage overhead.</li> <li> <p>Example: \"Despite having multiple versions of our 500GB dataset, we only used 600GB of storage because Kirin only stored the changed portions.\"</p> </li> <li> <p>Support Multiple Storage Solutions: \"I need to work with data across various storage systems our organization uses.\"</p> </li> <li>The backend-agnostic design allows seamless work across storage solutions.</li> <li> <p>Example: \"We migrated from local storage to S3 without changing our workflows because Kirin abstracted away the storage layer.\"</p> </li> <li> <p>Ensure Data Governance: \"I need to track who accesses what data and how it's used.\"</p> </li> <li>Usage tracking provides comprehensive audit logs.</li> <li>Example: \"For compliance reporting, I could generate a complete report of which teams accessed sensitive datasets.\"</li> </ol>"},{"location":"design/#133-data-team-manager-lead","title":"1.3.3 Data Team Manager / Lead","text":"<p>Jobs to be Done:</p> <ol> <li>Ensure Reproducibility: \"I need to guarantee that our team's work is reproducible for scientific integrity and audit purposes.\"</li> <li>End-to-end versioning and lineage tracking supports full reproducibility.</li> <li> <p>Example: \"When preparing a paper for submission, we could include Kirin references that allowed reviewers to verify our results.\"</p> </li> <li> <p>Manage Technical Debt: \"I need to understand data dependencies to prevent cascading failures when data changes.\"</p> </li> <li>Lineage visualization helps identify dependencies.</li> <li> <p>Example: \"Before making a major change to our core dataset, I could see all downstream analyses that would be affected.\"</p> </li> <li> <p>Accelerate Onboarding: \"I need new team members to quickly understand our data ecosystem.\"</p> </li> <li>Data catalogs and lineage visualization provide a map of available data and relationships.</li> <li> <p>Example: \"New hires could browse our data catalog to understand available datasets and how they relate to each other.\"</p> </li> <li> <p>Support Regulatory Compliance: \"I need to demonstrate data provenance for regulatory compliance.\"</p> </li> <li>Complete tracking of data origins and transformations provides necessary documentation.</li> <li>Example: \"During an audit, we could show the complete history of how customer data was processed and anonymized.\"</li> </ol>"},{"location":"design/#134-mlops-engineer","title":"1.3.4 MLOps Engineer","text":"<p>Jobs to be Done:</p> <ol> <li>Deploy Models with Data Dependencies: \"I need to package models with their exact data dependencies.\"</li> <li>Content-addressed references ensure exact data versions are specified.</li> <li> <p>Example: \"When deploying to production, our CI/CD pipeline could pull the exact data version used in validation.\"</p> </li> <li> <p>Monitor Data Drift: \"I need to compare production data against training data to detect drift.\"</p> </li> <li>Versioned datasets make it easy to compare current data with historical versions.</li> <li> <p>Example: \"Our monitoring system could compare daily production inputs against the original training data to detect shifts.\"</p> </li> <li> <p>Implement Data-Centric CI/CD: \"I need automated tests that verify data quality across pipeline stages.\"</p> </li> <li>Lineage tracking enables validation at each transformation step.</li> <li> <p>Example: \"Our CI pipeline ran tests against each version of the data to ensure transformations preserved key statistical properties.\"</p> </li> <li> <p>Roll Back Data When Needed: \"I need to quickly revert to previous data versions if issues arise.\"</p> </li> <li>Version control provides the ability to checkout any previous state.</li> <li>Example: \"When we discovered an issue with the latest data processing, we could roll back to the previous version in minutes while debugging.\"</li> </ol>"},{"location":"design/#135-laboratory-scientist","title":"1.3.5 Laboratory Scientist","text":"<p>Jobs to be Done:</p> <ol> <li>Ensure Experimental Reproducibility: \"I need to document and version all data associated with my laboratory experiments.\"</li> <li>Content-addressed storage ensures data integrity and versioning for all experimental outputs.</li> <li> <p>Example: \"Six months after publication, we could precisely reconstruct our experimental dataset when responding to reviewer questions.\"</p> </li> <li> <p>Track Sample Lineage: \"I need to track how samples and their derivatives are processed through multiple analyses.\"</p> </li> <li>File lineage features provide a complete chain of custody for samples through the analytical pipeline.</li> <li> <p>Example: \"When an unusual pattern appeared in our final results, we could trace it back to the original sample and specific processing steps.\"</p> </li> <li> <p>Manage Collaborative Research: \"I need to share experimental data with collaborators while maintaining version control.\"</p> </li> <li>Dataset versioning and backend-agnostic storage facilitate secure collaboration.</li> <li> <p>Example: \"Our collaborators at another institution could access the exact data versions we used, ensuring consistent analysis across research groups.\"</p> </li> <li> <p>Document Methods and Parameters: \"I need to record the exact parameters used for each instrument and analysis.\"</p> </li> <li>Transformation tracking records all processing parameters alongside the data.</li> <li>Example: \"Years later, we could verify the exact instrument settings and analysis parameters used to generate each data file.\"</li> </ol>"},{"location":"design/#14-feature-to-job-mapping","title":"1.4 Feature-to-Job Mapping","text":"<p>The table below maps Kirin features to the specific jobs they help users accomplish:</p> Kirin Feature Primary Jobs Addressed Key User Personas Content-Addressed Storage \u2022 Track Experiment Data\u2022 Find and Use the Right Data Version\u2022 Collaborate with Team Members\u2022 Ensure Reproducibility\u2022 Ensure Experimental Reproducibility Data Scientist, ML Engineer, Team Lead, Laboratory Scientist Automatic Lineage Tracking \u2022 Document Data Transformations\u2022 Manage Data Pipelines\u2022 Track Sample Lineage\u2022 Manage Technical Debt Data Scientist, Data Engineer, Laboratory Scientist Backend-Agnostic Storage \u2022 Support Multiple Storage Solutions\u2022 Optimize Storage Usage\u2022 Manage Collaborative Research Data Engineer, MLOps Engineer, Laboratory Scientist Dataset Versioning \u2022 Deploy Models with Data Dependencies\u2022 Roll Back Data When Needed\u2022 Monitor Data Drift\u2022 Ensure Experimental Reproducibility MLOps Engineer, Data Engineer, Laboratory Scientist Usage Tracking \u2022 Document Data Usage\u2022 Ensure Data Governance\u2022 Support Regulatory Compliance\u2022 Document Methods and Parameters Team Lead, Laboratory Scientist Zero-Copy Operations \u2022 Optimize Storage Usage\u2022 Handle Large Datasets Data Engineer, MLOps Engineer Data Catalog \u2022 Accelerate Onboarding\u2022 Find the Right Data Version\u2022 Manage Collaborative Research Team Lead, Data Scientist, Laboratory Scientist Path-Based API \u2022 Implement Data-Centric CI/CD\u2022 Manage Data Pipelines MLOps Engineer, Data Engineer"},{"location":"design/#15-user-workflows","title":"1.5 User Workflows","text":"<p>To illustrate how Kirin supports common workflows, here are examples of how different users accomplish their tasks:</p>"},{"location":"design/#model-development-workflow","title":"Model Development Workflow","text":"<p>A data scientist developing a new model:</p> <ol> <li>Discover and Access Data:</li> </ol> <pre><code># Browse the catalog to find relevant datasets\ndatasets = repo.list_datasets(tags=[\"customer\", \"transactions\"])\n\n# Checkout a specific version for reproducibility\ntransactions = repo.get_dataset(\"transactions\").checkout(\"2023-q2\")\n</code></pre> <ol> <li>Prepare and Transform Data:</li> </ol> <pre><code># All transformations are automatically tracked\nwith repo.track_processing(description=\"Preprocess transactions\"):\n    # Read input data\n    df = pd.read_csv(repo.Path(\"transactions/daily.csv\"))\n\n    # Transform data\n    df_clean = clean_transactions(df)\n\n    # Save processed version\n    df_clean.to_csv(repo.Path(\"transactions/clean.csv\"))\n</code></pre> <ol> <li>Train Model with Version Awareness:</li> </ol> <pre><code># Train using specific data versions, capturing exact data dependencies\nmodel = train_model(repo.Path(\"transactions/clean.csv\"))\n\n# Record model with data lineage\nwith repo.track_processing(description=\"Train transaction model\"):\n    model.save(repo.Path(\"models/transaction_classifier.pkl\"))\n</code></pre> <ol> <li>Document and Share Results:</li> </ol> <pre><code># Get full lineage information for reporting\ndata_lineage = repo.get_file_ancestors(\"models/transaction_classifier.pkl\")\n\n# Generate visualizations of data flow\nrepo.visualize_lineage(\"models/transaction_classifier.pkl\", output=\"report.svg\")\n</code></pre>"},{"location":"design/#data-pipeline-workflow","title":"Data Pipeline Workflow","text":"<p>A data engineer building an ETL pipeline:</p> <ol> <li>Set Up Extract Stage:</li> </ol> <pre><code># Extract from source and track provenance\nwith repo.track_processing(description=\"Extract from OLTP\"):\n    extract_data_from_source(\n        source_db=\"production_db\",\n        output_path=repo.Path(\"raw/daily_extract.parquet\")\n    )\n</code></pre> <ol> <li>Implement Transform Stage:</li> </ol> <pre><code># Transform with full tracking\nwith repo.track_processing(description=\"Transform daily data\"):\n    # Use zero-copy operations for large files\n    with repo.Path(\"raw/daily_extract.parquet\").open_stream() as stream:\n        # Process incrementally\n        result = transform_stream(stream)\n        result.to_parquet(repo.Path(\"transformed/daily.parquet\"))\n</code></pre> <ol> <li>Execute Load Stage:</li> </ol> <pre><code># Load with tracking\nwith repo.track_processing(description=\"Load to data warehouse\"):\n    load_to_warehouse(\n        source=repo.Path(\"transformed/daily.parquet\"),\n        target=\"warehouse.daily_facts\"\n    )\n</code></pre> <ol> <li>Verify Pipeline Integrity:</li> </ol> <pre><code># Verify the complete lineage from source to destination\nlineage = repo.get_file_ancestors(\"transformed/daily.parquet\")\n\n# Check for data quality at each stage\nfor stage in lineage:\n    validate_data_quality(stage)\n</code></pre>"},{"location":"design/#laboratory-research-workflow","title":"Laboratory Research Workflow","text":"<p>A laboratory scientist conducting experimental research:</p> <ol> <li>Capture Experimental Data:</li> </ol> <pre><code># Create a dataset for the experiment with metadata\nexperiment = repo.create_dataset(\n    \"experiment_2023_06\",\n    description=\"Protein binding kinetics experiment\",\n    metadata={\n        \"researcher\": \"Dr. Smith\",\n        \"equipment\": \"Mass Spectrometer Model XYZ\",\n        \"temperature\": \"22C\",\n        \"protocol_version\": \"v2.3\"\n    }\n)\n\n# Commit raw instrument output files\nexperiment.commit(\n    add_files=[\n        \"spectrometer/run_001.raw\",\n        \"spectrometer/run_002.raw\"\n    ],\n    commit_message=\"Initial spectrometer runs\"\n)\n</code></pre> <ol> <li>Process and Analyze Experimental Data:</li> </ol> <pre><code># All analysis steps are automatically tracked\nwith repo.track_processing(\n    description=\"Mass spec data processing\",\n    parameters={\"baseline_correction\": \"adaptive\", \"peak_detection\": \"centroid\"}\n):\n    # Read raw data\n    raw_data = read_spectrometer_data(repo.Path(\"spectrometer/run_001.raw\"))\n\n    # Process data with analysis software\n    processed_data = process_spectrometer_data(raw_data)\n\n    # Save processed results\n    processed_data.to_csv(repo.Path(\"processed/peaks_001.csv\"))\n\n    # Generate visualization\n    plot_spectrum(processed_data).savefig(repo.Path(\"figures/spectrum_001.png\"))\n</code></pre> <ol> <li>Generate Publication-Ready Results:</li> </ol> <pre><code># Combine and analyze processed data\nwith repo.track_processing(description=\"Binding kinetics analysis\"):\n    # Load all processed runs\n    runs = []\n    for file_path in repo.Path(\"processed\").glob(\"peaks_*.csv\"):\n        runs.append(pd.read_csv(file_path))\n\n    # Calculate binding kinetics\n    kinetics = calculate_binding_parameters(runs)\n\n    # Save final results\n    kinetics.to_csv(repo.Path(\"results/binding_kinetics.csv\"))\n\n    # Generate publication figure\n    create_publication_figure(kinetics).savefig(\n        repo.Path(\"figures/figure_3_binding_curve.png\"), dpi=300\n    )\n</code></pre> <ol> <li>Document and Share Research:</li> </ol> <pre><code># Generate complete provenance for publication\nlineage = repo.get_file_ancestors(\"results/binding_kinetics.csv\")\n\n# Create research package for collaborators\npackage = repo.create_snapshot(\n    files=[\"results/*\", \"figures/*\", \"processed/*\"],\n    include_lineage=True,\n    output=\"research_package.zip\"\n)\n\n# Generate methods section with exact parameters\nmethods_text = repo.generate_methods_text(\n    \"results/binding_kinetics.csv\",\n    template=\"templates/methods_section.md\"\n)\n</code></pre>"},{"location":"design/#laboratory-comparative-analysis-workflow","title":"Laboratory Comparative Analysis Workflow","text":"<p>A laboratory scientist comparing results across multiple experiments:</p> <ol> <li>Identify Relevant Experiments:</li> </ol> <pre><code># Find all experiments with a specific characteristic\nexperiments = repo.search_datasets(\n    metadata={\"protocol_version\": \"v2.*\"},\n    tags=[\"protein-binding\"]\n)\n\n# Create a collection of related experiments\ncollection = repo.create_collection(\n    \"binding_kinetics_comparison\",\n    datasets=[exp.name for exp in experiments]\n)\n</code></pre> <ol> <li>Standardize and Compare Results:</li> </ol> <pre><code># Process data from multiple experiments with standard methods\nwith repo.track_processing(description=\"Comparative analysis\"):\n    # Collect results from each experiment\n    all_results = []\n    for experiment in collection.datasets:\n        # Access the specific version used in the experiment\n        kinetics_file = experiment.get_file(\"results/binding_kinetics.csv\")\n        all_results.append(pd.read_csv(kinetics_file))\n\n    # Perform comparative analysis\n    comparison = compare_experimental_results(all_results)\n\n    # Save comparative results\n    comparison.to_csv(repo.Path(\"comparative/binding_comparison.csv\"))\n\n    # Generate comparison visualization\n    plot_comparison(comparison).savefig(\n        repo.Path(\"comparative/binding_comparison.png\")\n    )\n</code></pre> <ol> <li>Validate Protocol Improvements:</li> </ol> <pre><code># Analyze protocol version impact\nprotocol_impact = repo.analyze_metadata_impact(\n    collection=collection,\n    target_file=\"results/binding_kinetics.csv\",\n    groupby=\"protocol_version\",\n    metrics=[\"affinity\", \"specificity\"]\n)\n\n# Generate protocol evolution report\ngenerate_protocol_report(\n    protocol_impact,\n    output=repo.Path(\"reports/protocol_evolution.pdf\")\n)\n</code></pre>"},{"location":"design/#2-system-architecture","title":"2. System Architecture","text":""},{"location":"design/#21-core-components","title":"2.1 Core Components","text":""},{"location":"design/#211-storage-layer","title":"2.1.1 Storage Layer","text":"<p>The Storage Layer uses fsspec for backend-agnostic storage. It provides a unified interface for:</p> <ul> <li>Reading/writing files</li> <li>Listing files</li> <li>Getting file metadata</li> <li>Streaming data access</li> <li>Memory-mapped access where supported</li> </ul> <p>Key Abstractions:</p> <pre><code>class ContentStore:\n    \"\"\"Manages content-addressed storage.\"\"\"\n\n    def store_file(self, file_path: Path) -&gt; str: ...  # Returns content hash\n    def store_content(self, content: bytes) -&gt; str: ...  # Store content bytes\n    def retrieve(self, content_hash: str) -&gt; bytes: ...  # Retrieve by hash\n    def exists(self, content_hash: str) -&gt; bool: ...  # Check existence\n    def open_stream(self, content_hash: str, mode: str = \"rb\") -&gt; IO[bytes]: ...\n</code></pre> <p>Built-in fsspec backends:</p> <ul> <li>Local filesystem</li> <li>S3</li> <li>GCS</li> <li>Azure Blob Storage</li> <li>Dropbox</li> <li>Google Drive</li> </ul>"},{"location":"design/#212-versioning-layer","title":"2.1.2 Versioning Layer","text":"<p>The Versioning Layer manages the linear version history of files and file sets:</p> <ul> <li>Tracks changes to individual files</li> <li>Manages collections of files as atomic units</li> <li>Maintains a linear commit history (no branching)</li> <li>Simple parent-child relationships between commits</li> </ul> <p>Key Abstractions:</p> <pre><code>@dataclass(frozen=True)\nclass File:\n    \"\"\"Represents a versioned file with content-addressed storage.\"\"\"\n\n    hash: str\n    name: str\n    size: int\n    content_type: Optional[str] = None\n\n    def read_bytes(self) -&gt; bytes: ...\n    def read_text(self, encoding: str = \"utf-8\") -&gt; str: ...\n    def open(self, mode: str = \"rb\") -&gt; Union[BinaryIO, TextIO]: ...\n    def download_to(self, path: Union[str, Path]) -&gt; str: ...\n\n@dataclass(frozen=True)\nclass Commit:\n    \"\"\"Represents an immutable snapshot of files at a point in time.\"\"\"\n\n    hash: str\n    message: str\n    timestamp: datetime\n    parent_hash: Optional[str]  # Linear history - single parent\n    files: Dict[str, File]  # filename -&gt; File mapping\n</code></pre>"},{"location":"design/#213-dataset-layer","title":"2.1.3 Dataset Layer","text":"<p>The Dataset Layer provides high-level abstractions for managing datasets:</p> <ul> <li>Organizes files into logical datasets</li> <li>Maintains linear commit history</li> <li>Provides simple file operations</li> <li>Enables dataset discovery</li> </ul> <p>Key Abstractions:</p> <pre><code>class Dataset:\n    \"\"\"Represents a logical collection of files with linear history.\"\"\"\n\n    def __init__(self, root_dir: Union[str, Path], name: str, description: str = \"\",\n                 fs: Optional[fsspec.AbstractFileSystem] = None): ...\n    def commit(self, message: str, add_files: List[Union[str, Path]] = None,\n               remove_files: List[str] = None) -&gt; str: ...  # Returns commit hash\n    def checkout(self, commit_hash: Optional[str] = None) -&gt; None: ...  # None = latest commit\n    def get_file(self, name: str) -&gt; Optional[File]: ...\n    def list_files(self) -&gt; List[str]: ...\n    def has_file(self, name: str) -&gt; bool: ...  # Check if file exists\n    def read_file(self, name: str, mode: str = \"r\") -&gt; Union[str, bytes]: ...\n    def download_file(self, name: str, target_path: Union[str, Path]) -&gt; str: ...  # Download to local path\n    def open_file(self, name: str, mode: str = \"rb\"): ...  # Open as file handle\n    def local_files(self): ...  # Context manager for local file access\n    def history(self, limit: Optional[int] = None) -&gt; List[Commit]: ...\n    def get_commit(self, commit_hash: str) -&gt; Optional[Commit]: ...  # Get specific commit\n    def get_commits(self) -&gt; List[Commit]: ...  # Get all commits\n    def is_empty(self) -&gt; bool: ...  # Check if dataset has commits\n    def cleanup_orphaned_files(self) -&gt; int: ...  # Remove unreferenced files\n    def get_info(self) -&gt; dict: ...  # Get dataset information\n    def to_dict(self) -&gt; dict: ...  # Convert to dictionary\n</code></pre>"},{"location":"design/#214-catalog-layer","title":"2.1.4 Catalog Layer","text":"<p>The Catalog Layer provides high-level management of multiple datasets:</p> <ul> <li>Organizes datasets into logical collections</li> <li>Provides dataset discovery and management</li> <li>Enables multi-dataset workflows</li> <li>Supports catalog-level operations</li> </ul> <p>Key Abstractions:</p> <pre><code>class Catalog:\n    \"\"\"Represents a collection of datasets.\"\"\"\n\n    def __init__(self, root_dir: Union[str, Path], fs: Optional[fsspec.AbstractFileSystem] = None): ...\n    def datasets(self) -&gt; List[str]: ...  # List dataset names\n    def get_dataset(self, dataset_name: str) -&gt; Dataset: ...  # Get existing dataset\n    def create_dataset(self, dataset_name: str, description: str = \"\") -&gt; Dataset: ...  # Create new dataset\n    def __len__(self) -&gt; int: ...  # Number of datasets\n</code></pre> <p>Use Cases:</p> <ul> <li>Multi-dataset project management</li> <li>Dataset discovery and browsing</li> <li>Cross-dataset analysis workflows</li> <li>Organizational data governance</li> </ul>"},{"location":"design/#22-system-flow","title":"2.2 System Flow","text":"<ol> <li>Data Ingestion Flow:</li> <li>User provides files to be tracked</li> <li>Files are hashed and stored in content store</li> <li>A commit is created with references to file versions</li> <li> <p>The commit is recorded in the linear history</p> </li> <li> <p>Data Access Flow:</p> </li> <li>User requests a specific version of a file or dataset</li> <li>System resolves the logical path to a content hash</li> <li>Content is retrieved from the storage backend</li> <li> <p>Content is provided to the user</p> </li> <li> <p>Data Processing Flow:</p> </li> <li>User accesses input data files</li> <li>Processing is performed</li> <li>Output files are stored in Kirin</li> <li>New commit is created with updated files</li> </ol>"},{"location":"design/#3-data-structures-and-storage","title":"3. Data Structures and Storage","text":""},{"location":"design/#31-content-store-layout","title":"3.1 Content Store Layout","text":"<p>The content store is organized as follows:</p> <pre><code>&lt;root&gt;/\n  \u251c\u2500\u2500 data/                     # Content-addressed storage\n  \u2502   \u251c\u2500\u2500 ab/                  # First two characters of hash\n  \u2502   \u2502   \u2514\u2500\u2500 cdef1234...       # Rest of the hash (no file extensions)\n  \u2502   \u2514\u2500\u2500 ...\n  \u2514\u2500\u2500 datasets/                 # Dataset storage\n      \u251c\u2500\u2500 dataset1/             # Dataset directory\n      \u2502   \u2514\u2500\u2500 commits.json       # Linear commit history\n      \u2514\u2500\u2500 ...\n</code></pre> <p>Critical Storage Design: Files are stored without file extensions in the content-addressed storage system. The original file extensions are preserved as metadata in the <code>File</code> entity's <code>name</code> attribute and are restored when files are downloaded or accessed. This ensures:</p> <ul> <li>Content Integrity: Files are identified purely by content hash</li> <li>Deduplication: Identical content (regardless of original filename) is stored only once</li> <li>Extension Restoration: Original filenames are maintained for user experience</li> </ul>"},{"location":"design/#32-commit-history-format","title":"3.2 Commit History Format","text":"<p>Each dataset maintains a single JSON file with linear commit history:</p> <pre><code>{\n  \"dataset_name\": \"my_dataset\",\n  \"commits\": [\n    {\n      \"hash\": \"abc123...\",\n      \"message\": \"Initial commit\",\n      \"timestamp\": \"2024-01-01T12:00:00\",\n      \"parent_hash\": null,\n      \"files\": {\n        \"data.csv\": {\n          \"hash\": \"def456...\",\n          \"name\": \"data.csv\",\n          \"size\": 1024,\n          \"content_type\": \"text/csv\"\n        }\n      }\n    },\n    {\n      \"hash\": \"ghi789...\",\n      \"message\": \"Add processed data\",\n      \"timestamp\": \"2024-01-01T13:00:00\",\n      \"parent_hash\": \"abc123...\",\n      \"files\": {\n        \"data.csv\": {\n          \"hash\": \"def456...\",\n          \"name\": \"data.csv\",\n          \"size\": 1024,\n          \"content_type\": \"text/csv\"\n        },\n        \"processed.csv\": {\n          \"hash\": \"jkl012...\",\n          \"name\": \"processed.csv\",\n          \"size\": 2048,\n          \"content_type\": \"text/csv\"\n        }\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"design/#4-api-design","title":"4. API Design","text":""},{"location":"design/#41-python-api","title":"4.1 Python API","text":"<p>The Python API is designed for simplicity and ease of use:</p> <pre><code># Basic usage\nfrom kirin import Dataset, File, Commit\n\n# Initialize dataset\ndataset = Dataset(root_dir=\"/path/to/data\", name=\"my_dataset\")\n\n# Commit files\ncommit_hash = dataset.commit(\n    message=\"Initial commit\",\n    add_files=[\"file1.csv\", \"file2.json\"]\n)\n\n# Access files from current commit\nfiles = dataset.files\nprint(f\"Files: {list(files.keys())}\")\n\n# Read a file\ncontent = dataset.read_file(\"file1.csv\", mode=\"r\")  # text mode\nbinary_content = dataset.read_file(\"file1.csv\", mode=\"rb\")  # binary mode\n\n# Get a specific file object\nfile_obj = dataset.get_file(\"file1.csv\")\nif file_obj:\n    print(f\"File size: {file_obj.size} bytes\")\n    print(f\"Content hash: {file_obj.short_hash}\")\n\n    # Read file content\n    content = file_obj.read_text()\n\n    # Download to local path\n    file_obj.download_to(\"/tmp/file1.csv\")\n\n    # Open as file handle\n    with file_obj.open(\"r\") as f:\n        data = f.read()\n\n# Checkout a specific commit\ndataset.checkout(commit_hash)\n\n# Get commit history\nhistory = dataset.history(limit=10)\nfor commit in history:\n    print(f\"{commit.short_hash}: {commit.message}\")\n\n# Get specific commit\ncommit = dataset.get_commit(commit_hash)\nif commit:\n    print(f\"Commit: {commit.short_hash}\")\n    print(f\"Message: {commit.message}\")\n    print(f\"Files: {commit.list_files()}\")\n    print(f\"Total size: {commit.get_total_size()} bytes\")\n\n# Local file access for processing\nwith dataset.local_files() as local_files:\n    for filename, local_path in local_files.items():\n        print(f\"{filename} -&gt; {local_path}\")\n        # Process files locally\n        df = pd.read_csv(local_path)\n\n# Remove files in a commit\ndataset.commit(\n    message=\"Remove old file\",\n    remove_files=[\"old_file.csv\"]\n)\n\n# Add and remove files in same commit\ndataset.commit(\n    message=\"Update dataset\",\n    add_files=[\"new_file.csv\"],\n    remove_files=[\"old_file.csv\"]\n)\n\n# Working with Catalogs (multi-dataset management)\nfrom kirin import Catalog\n\n# Create or access a catalog\ncatalog = Catalog(root_dir=\"/path/to/data\")\n\n# List all datasets in the catalog\ndataset_names = catalog.datasets()\nprint(f\"Available datasets: {dataset_names}\")\n\n# Create a new dataset\nnew_dataset = catalog.create_dataset(\"experiment_2024\", \"ML experiment data\")\n\n# Get an existing dataset\nexisting_dataset = catalog.get_dataset(\"my_dataset\")\n\n# Multi-dataset workflow\nfor dataset_name in catalog.datasets():\n    dataset = catalog.get_dataset(dataset_name)\n    print(f\"Dataset {dataset_name}: {len(dataset.files)} files\")\n\n    # Process each dataset\n    with dataset.local_files() as local_files:\n        for filename, local_path in local_files.items():\n            # Analyze files from each dataset\n            print(f\"Processing {filename} from {dataset_name}\")\n\n# Authentication helpers for cloud storage\nfrom kirin import get_s3_filesystem, get_gcs_filesystem\n\n# S3 with AWS profile\ns3_fs = get_s3_filesystem(profile=\"my-aws-profile\")\ns3_dataset = Dataset(root_dir=\"s3://my-bucket/datasets\", name=\"cloud_dataset\", fs=s3_fs)\n\n# GCS with service account\ngcs_fs = get_gcs_filesystem(token=\"/path/to/service-account.json\")\ngcs_dataset = Dataset(root_dir=\"gs://my-bucket/datasets\", name=\"gcs_dataset\", fs=gcs_fs)\n\n# Advanced file operations\ndataset = Dataset(root_dir=\"/path/to/data\", name=\"advanced_dataset\")\n\n# Check if dataset is empty\nif not dataset.is_empty():\n    # Get dataset information\n    info = dataset.get_info()\n    print(f\"Dataset info: {info}\")\n\n    # Clean up orphaned files\n    removed_count = dataset.cleanup_orphaned_files()\n    print(f\"Removed {removed_count} orphaned files\")\n\n    # Convert to dictionary for serialization\n    dataset_dict = dataset.to_dict()\n    print(f\"Serialized dataset: {dataset_dict}\")\n</code></pre>"},{"location":"design/#42-cloud-storage-support","title":"4.2 Cloud Storage Support","text":"<p>Kirin supports multiple cloud storage backends through fsspec:</p> <pre><code># S3\ndataset = Dataset(root_dir=\"s3://my-bucket/datasets\", name=\"my_dataset\")\n\n# Google Cloud Storage\ndataset = Dataset(root_dir=\"gs://my-bucket/datasets\", name=\"my_dataset\")\n\n# Azure Blob Storage\ndataset = Dataset(root_dir=\"az://my-container/datasets\", name=\"my_dataset\")\n\n# With custom filesystem\nfrom kirin import get_s3_filesystem\nfs = get_s3_filesystem(profile=\"my-profile\")\ndataset = Dataset(root_dir=\"s3://my-bucket/datasets\", name=\"my_dataset\", fs=fs)\n</code></pre>"},{"location":"design/#43-web-ui","title":"4.3 Web UI","text":"<p>Kirin includes a comprehensive web interface built with FastAPI and HTMX:</p> <p>Architecture:</p> <ul> <li>Backend: FastAPI application with Jinja2 templates</li> <li>Frontend: HTMX for dynamic interactions without JavaScript frameworks</li> <li>Styling: shadcn/ui design system with CSS custom properties</li> <li>Configuration: CatalogManager for managing multiple data catalogs</li> </ul> <p>Key Features:</p> <ul> <li>Catalog Management: Create, configure, and manage multiple data catalogs</li> <li>Dataset Browsing: View datasets, commits, and file history</li> <li>File Operations: Upload files, remove files, and commit changes</li> <li>Commit History: Browse commit history with detailed file information</li> <li>File Preview: View file contents directly in the browser</li> <li>Cloud Integration: Support for S3, GCS, Azure, and other cloud storage backends</li> </ul> <p>Routes and Functionality:</p> <pre><code># Main application routes\n@app.get(\"/\")  # Catalog listing\n@app.get(\"/{catalog_id}\")  # Dataset listing for catalog\n@app.get(\"/{catalog_id}/{dataset}\")  # Dataset view with files and history\n@app.post(\"/{catalog_id}/{dataset}/commit\")  # Commit file changes\n@app.get(\"/{catalog_id}/{dataset}/files/{filename}\")  # File preview\n</code></pre> <p>Configuration System:</p> <ul> <li>CatalogManager: Manages catalog configurations stored in <code>~/.kirin/config.json</code></li> <li>AWS Profile Integration: Automatic detection of AWS profiles</li> <li>Cloud Authentication: Support for various authentication methods</li> <li>SSL Certificate Management: Automatic setup for isolated Python environments</li> </ul>"},{"location":"design/#44-authentication-helpers","title":"4.4 Authentication Helpers","text":"<p>Kirin provides comprehensive authentication helpers for cloud storage:</p> <p>Cloud Storage Authentication:</p> <pre><code>from kirin import get_s3_filesystem, get_gcs_filesystem, get_azure_filesystem\n\n# S3 with profile\nfs = get_s3_filesystem(profile=\"my-profile\")\ndataset = Dataset(root_dir=\"s3://my-bucket/datasets\", name=\"my_dataset\", fs=fs)\n\n# GCS with service account\nfs = get_gcs_filesystem(token=\"/path/to/key.json\")\ndataset = Dataset(root_dir=\"gs://my-bucket/datasets\", name=\"my_dataset\", fs=fs)\n\n# Azure with connection string\nfs = get_azure_filesystem(connection_string=\"...\")\ndataset = Dataset(root_dir=\"az://my-container/datasets\", name=\"my_dataset\", fs=fs)\n</code></pre> <p>Keyring Integration:</p> <ul> <li>Credential Storage: Secure storage of cloud credentials using keyring</li> <li>Automatic Retrieval: Credentials are automatically retrieved when needed</li> <li>Multi-Provider Support: Works with AWS, GCS, Azure, and other providers</li> </ul> <p>SSL Certificate Management:</p> <ul> <li>Automatic Setup: <code>python -m kirin.setup_ssl</code> for isolated environments</li> <li>Environment Detection: Automatically detects pixi, uv, conda environments</li> <li>Certificate Copying: Copies system certificates to Python environment</li> </ul>"},{"location":"design/#5-performance-considerations","title":"5. Performance Considerations","text":""},{"location":"design/#51-zero-copy-architecture","title":"5.1 Zero-Copy Architecture","text":"<p>Kirin is designed with a zero-copy philosophy wherever possible:</p> <ul> <li>Memory-mapped files: When working with local files, memory mapping is used to avoid loading entire files into memory</li> <li>Streaming operations: For operations on large files, streaming interfaces are provided to process data incrementally</li> <li>Direct transfers: When copying between storage backends, data is streamed directly without loading into application memory</li> <li>Reference-based operations: Operations like checkouts use references instead of copying file content</li> </ul>"},{"location":"design/#52-caching","title":"5.2 Caching","text":"<p>Local caching of frequently accessed files will improve performance when working with remote storage backends.</p>"},{"location":"design/#53-lazy-loading","title":"5.3 Lazy Loading","text":"<p>Content will be loaded only when needed, reducing unnecessary network traffic.</p>"},{"location":"design/#54-optimized-hashing","title":"5.4 Optimized Hashing","text":"<p>To improve performance when hashing large files:</p> <ul> <li>Incremental hashing is used for streaming data</li> <li>Parallel chunk processing for multi-core systems</li> <li>Optional content-based chunking for improved deduplication</li> </ul>"},{"location":"design/#6-current-implementation-status","title":"6. Current Implementation Status","text":""},{"location":"design/#61-implemented-features","title":"6.1 Implemented Features","text":"<p>Core Architecture (Complete):</p> <ul> <li>\u2705 Content-addressed storage with fsspec backends</li> <li>\u2705 Linear commit history without branching</li> <li>\u2705 Dataset and File entities with full API</li> <li>\u2705 Catalog system for multi-dataset management</li> <li>\u2705 Web UI with FastAPI and HTMX</li> <li>\u2705 Authentication helpers for cloud storage</li> <li>\u2705 SSL certificate management for isolated environments</li> </ul> <p>CLI Interface (Implemented): The command-line interface is available through the <code>kirin.cli</code> module:</p> <pre><code># Start web UI\nkirin ui\n\n# Or use the gitdata command (when installed via uv)\ngitdata ui\n</code></pre> <p>Web UI (Complete):</p> <ul> <li>\u2705 Catalog management interface</li> <li>\u2705 Dataset browsing and file operations</li> <li>\u2705 File upload and removal</li> <li>\u2705 Commit history visualization</li> <li>\u2705 File preview functionality</li> <li>\u2705 Cloud storage integration</li> </ul>"},{"location":"design/#62-future-extensions","title":"6.2 Future Extensions","text":"<p>Integration with ML Frameworks: Direct integration with popular ML frameworks like PyTorch and TensorFlow can streamline data loading.</p> <p>Native Format Handlers: Format-specific handlers could be developed to enable operations directly on file contents:</p> <ul> <li>Parquet/Arrow operations for columnar data</li> <li>HDF5/Zarr for array data</li> <li>SQLite for tabular data</li> </ul> <p>Enhanced CLI:</p> <p>Additional CLI commands could be developed for:</p> <ul> <li>Dataset initialization and management</li> <li>Batch operations across multiple datasets</li> <li>Integration with CI/CD pipelines</li> </ul>"},{"location":"design/#7-conclusion","title":"7. Conclusion","text":"<p>Kirin's simplified architecture provides a robust, flexible system for data versioning that meets the specified requirements:</p> <ol> <li>It supports multiple storage backends through fsspec</li> <li>It uses content-addressed storage for integrity and deduplication</li> <li>It provides linear commit history without branching complexity</li> <li>It operates in a serverless manner</li> <li>It focuses on ergonomic Python API design</li> <li>It employs zero-copy operations wherever possible, optimizing for performance and resource efficiency</li> </ol> <p>This design document outlines a simplified path forward that prioritizes ease of use and maintainability while providing the core functionality needed for data versioning.</p>"},{"location":"ecosystem-diligence/","title":"Ecosystem Due Diligence","text":"<p>This document tracks our evaluation of third-party libraries and tools for GitData's content-addressed storage and version control needs.</p>"},{"location":"ecosystem-diligence/#content-addressed-storage-libraries","title":"Content-Addressed Storage Libraries","text":""},{"location":"ecosystem-diligence/#hashfs-not-recommended","title":"<code>hashfs</code> - \u274c Not Recommended","text":"<p>Status: Evaluated in detail - see HashFS Evaluation</p> <p>Key Findings:</p> <ul> <li>Maintenance: Last commit 5 years ago (July 2019), appears unmaintained</li> <li>Architecture: Local filesystem only, no cloud backend support</li> <li>Integration: No fsspec support, would require complete architectural rewrite</li> <li>Features: Missing version control, metadata tracking, usage analytics</li> <li>Performance: Memory-intensive, no streaming support for large files</li> </ul> <p>Decision: Do not integrate - GitData's current architecture is more sophisticated and feature-complete.</p> <p>Rationale:</p> <ul> <li>HashFS lacks critical features (versioning, cloud support, metadata)</li> <li>Would require complete rewrite of storage layer</li> <li>Loss of multi-backend support (S3, GCS, Azure, etc.)</li> <li>Current GitData implementation already provides content addressing</li> <li>Better to enhance current system with HashFS-inspired optimizations</li> </ul>"},{"location":"ecosystem-diligence/#s3-cas-not-suitable","title":"<code>s3-cas</code> - \u274c Not Suitable","text":"<p>Status: Brief evaluation</p> <p>Key Findings:</p> <ul> <li>Maintenance: Appears unmaintained</li> <li>Scope: S3-specific, not flexible for multi-backend architecture</li> <li>Features: Basic content addressing, no version control or metadata</li> <li>Use Case: Good for learning S3 CAS patterns, not production use</li> </ul> <p>Decision: Not suitable - Too narrow in scope, doesn't align with GitData's multi-backend requirements.</p>"},{"location":"ecosystem-diligence/#current-implementation-strategy","title":"Current Implementation Strategy","text":"<p>Approach: Custom content-addressed storage built on fsspec</p> <p>Rationale:</p> <ul> <li>Multi-backend support: Native fsspec integration for S3, GCS, Azure,   local, etc.</li> <li>Version control: Full Git-like versioning with commits, branches, merging</li> <li>Metadata tracking: Commit messages, authors, timestamps, usage analytics</li> <li>Performance: Streaming operations, memory-efficient large file handling</li> <li>Extensibility: Easy to add new backends and features</li> </ul> <p>Current Architecture:</p> <pre><code>&lt;root&gt;/\n\u251c\u2500\u2500 data/                    # Content-addressed file storage\n\u2502   \u251c\u2500\u2500 {hash}/             # SHA256 hash directory\n\u2502   \u2502   \u2514\u2500\u2500 filename        # Original filename preserved\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 datasets/               # Dataset metadata and versioning\n\u2502   \u2514\u2500\u2500 {dataset_name}/\n\u2502       \u251c\u2500\u2500 refs/heads/     # Branch references\n\u2502       \u251c\u2500\u2500 HEAD            # Current branch pointer\n\u2502       \u2514\u2500\u2500 {commit_hash}/   # Individual commits\n\u2502           \u2514\u2500\u2500 commit.json # Commit metadata\n\u2514\u2500\u2500 usage.db               # Planned SQLite usage tracking\n</code></pre> <p>Enhancement Opportunities:</p> <ul> <li>Deduplication: Add automatic file deduplication</li> <li>Directory optimization: Consider nested directory structure for large   file counts</li> <li>Repair capabilities: Add storage repair and validation</li> <li>Performance: Optimize for large-scale deployments</li> </ul>"},{"location":"ecosystem-diligence/#alternative-approaches-considered","title":"Alternative Approaches Considered","text":""},{"location":"ecosystem-diligence/#git-lfs-large-file-storage","title":"Git LFS (Large File Storage)","text":"<ul> <li>Pros: Git-native, well-supported, handles large files</li> <li>Cons: Server-dependent, complex setup, not content-addressed</li> <li>Decision: Not suitable for serverless, content-addressed architecture</li> </ul>"},{"location":"ecosystem-diligence/#ipfs-interplanetary-file-system","title":"IPFS (InterPlanetary File System)","text":"<ul> <li>Pros: Distributed, content-addressed, peer-to-peer</li> <li>Cons: Complex setup, performance overhead, not Git-like</li> <li>Decision: Overkill for GitData's use case, adds unnecessary complexity</li> </ul>"},{"location":"ecosystem-diligence/#custom-git-objects","title":"Custom Git Objects","text":"<ul> <li>Pros: Git-native, proven scalability, familiar to developers</li> <li>Cons: Complex implementation, not optimized for data workflows</li> <li>Decision: Current fsspec-based approach is more suitable for data science   use cases</li> </ul>"},{"location":"ecosystem-diligence/#lessons-learned","title":"Lessons Learned","text":"<ol> <li>Architecture matters more than features: GitData's multi-backend,    version-controlled architecture is more valuable than simple content addressing</li> <li>fsspec is the right abstraction: Provides the flexibility needed for    multi-backend support</li> <li>Version control is essential: Content addressing alone isn't sufficient    for data versioning</li> <li>Metadata tracking is critical: Usage analytics and lineage tracking are    key differentiators</li> <li>Performance optimization: Streaming and memory efficiency are crucial    for large datasets</li> </ol>"},{"location":"ecosystem-diligence/#future-considerations","title":"Future Considerations","text":"<ul> <li>Monitor ecosystem: Watch for new CAS libraries with fsspec support</li> <li>Performance optimization: Consider implementing HashFS-inspired directory nesting</li> <li>Deduplication: Add automatic file deduplication to current system</li> <li>Repair capabilities: Implement storage integrity checking and repair</li> <li>Cloud optimization: Optimize for specific cloud backends (S3, GCS, etc.)</li> </ul>"},{"location":"hashfs-evaluation/","title":"HashFS Integration Evaluation","text":""},{"location":"hashfs-evaluation/#executive-summary","title":"Executive Summary","text":"<p>After thorough evaluation of HashFS as a potential content-addressed storage (CAS) solution for GitData, we have determined that HashFS is not a suitable replacement for our current architecture. While HashFS provides excellent content-addressed storage capabilities, it lacks critical features required by GitData and would require a complete architectural rewrite.</p>"},{"location":"hashfs-evaluation/#current-gitdata-architecture","title":"Current GitData Architecture","text":"<p>GitData implements a sophisticated content-addressed storage system with the following characteristics:</p>"},{"location":"hashfs-evaluation/#core-features","title":"Core Features","text":"<ul> <li>Multi-backend support: Built on fsspec for S3, GCS, Azure, local   filesystem, and more</li> <li>Version control: Full Git-like versioning with commits, branches,   and merging</li> <li>Metadata tracking: Commit messages, authors, timestamps, and usage   analytics</li> <li>Content addressing: SHA256-based file hashing with flat directory   structure</li> <li>Streaming operations: Memory-efficient file handling for large datasets</li> <li>Usage tracking: Planned SQLite database integration for analytics</li> </ul>"},{"location":"hashfs-evaluation/#storage-format","title":"Storage Format","text":"<pre><code>&lt;root&gt;/\n\u251c\u2500\u2500 data/                    # Content-addressed file storage\n\u2502   \u251c\u2500\u2500 {hash}/             # SHA256 hash directory\n\u2502   \u2502   \u2514\u2500\u2500 filename        # Original filename preserved\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 datasets/               # Dataset metadata and versioning\n\u2502   \u2514\u2500\u2500 {dataset_name}/\n\u2502       \u251c\u2500\u2500 refs/heads/     # Branch references\n\u2502       \u251c\u2500\u2500 HEAD            # Current branch pointer\n\u2502       \u2514\u2500\u2500 {commit_hash}/   # Individual commits\n\u2502           \u2514\u2500\u2500 commit.json # Commit metadata\n\u2514\u2500\u2500 usage.db               # Planned SQLite usage tracking\n</code></pre>"},{"location":"hashfs-evaluation/#hashfs-capabilities","title":"HashFS Capabilities","text":"<p>HashFS is a well-designed content-addressed storage library with these features:</p>"},{"location":"hashfs-evaluation/#strengths","title":"Strengths","text":"<ul> <li>Automatic deduplication: Files with identical content stored once</li> <li>Configurable directory nesting: Optimized for large file counts</li> <li>Repair capabilities: Can rebuild index from existing files</li> <li>Multiple hash algorithms: Support for any hashlib algorithm</li> <li>Simple API: Clean interface for file storage/retrieval</li> </ul>"},{"location":"hashfs-evaluation/#limitations-for-gitdata","title":"Limitations for GitData","text":"<ul> <li>Local filesystem only: No cloud backend support</li> <li>No version control: No concept of commits, branches, or history</li> <li>No metadata tracking: No commit messages, authors, or timestamps</li> <li>No fsspec integration: Would require custom backend development</li> <li>No usage analytics: No database integration (planned for GitData)</li> <li>Memory limitations: Loads entire files into memory</li> </ul>"},{"location":"hashfs-evaluation/#integration-challenges","title":"Integration Challenges","text":""},{"location":"hashfs-evaluation/#1-architectural-incompatibility","title":"1. Architectural Incompatibility","text":"<p>Problem: HashFS does not implement the fsspec interface, which is fundamental to GitData's multi-backend architecture.</p> <p>Impact:</p> <ul> <li>Would require complete rewrite of storage layer</li> <li>Loss of S3, GCS, Azure, and other cloud backend support</li> <li>Custom fsspec backend development required</li> <li>Significant maintenance overhead</li> </ul> <p>Current GitData Code:</p> <pre><code># Multi-backend support via fsspec\ndef get_filesystem(path: str | Path) -&gt; fsspec.AbstractFileSystem:\n    \"\"\"Get the appropriate filesystem based on the path URI.\"\"\"\n    # Supports s3://, gs://, az://, file://, http://, etc.\n</code></pre>"},{"location":"hashfs-evaluation/#2-feature-gaps","title":"2. Feature Gaps","text":"<p>Missing Critical Features:</p> <ul> <li>Version control: No commits, branches, or merging</li> <li>Metadata: No commit messages, authors, timestamps</li> <li>Usage tracking: No SQLite integration</li> <li>Cloud scalability: Local filesystem only</li> <li>Streaming: No memory-efficient large file handling</li> </ul> <p>GitData Requirements:</p> <pre><code># Version control with metadata\n@dataclass\nclass DatasetCommit:\n    version_hash: str\n    commit_message: str\n    file_hashes: list[str]\n    parent_hash: str\n    timestamp: datetime\n    author: str\n</code></pre>"},{"location":"hashfs-evaluation/#3-performance-considerations","title":"3. Performance Considerations","text":"<p>HashFS Limitations:</p> <ul> <li>No streaming support for large files</li> <li>No memory mapping capabilities</li> <li>Single-threaded operations</li> <li>Memory-intensive file loading</li> </ul> <p>GitData Optimizations:</p> <pre><code># Streaming file operations\ndef hash_file(filepath: str, fs: fsspec.AbstractFileSystem) -&gt; str:\n    hash = sha256()\n    with fs.open(str(filepath), \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash.update(chunk)\n    return hash.hexdigest()\n</code></pre>"},{"location":"hashfs-evaluation/#detailed-comparison","title":"Detailed Comparison","text":"Feature GitData Current HashFS Impact Multi-backend support \u2705 fsspec-based \u274c Local only Critical loss Version control \u2705 Full Git-like \u274c None Critical loss Metadata tracking \u2705 Complete \u274c None Critical loss Usage analytics \u26a0\ufe0f Planned \u274c None Future feature Content addressing \u2705 SHA256 \u2705 Configurable Maintained Deduplication \u274c Manual \u2705 Automatic Potential gain Repair capabilities \u274c None \u2705 Built-in Potential gain Directory optimization \u274c Flat structure \u2705 Nested Potential gain Memory efficiency \u2705 Streaming \u274c Loads all Performance loss Cloud scalability \u2705 Native \u274c Not supported Critical loss"},{"location":"hashfs-evaluation/#alternative-approaches","title":"Alternative Approaches","text":""},{"location":"hashfs-evaluation/#option-1-enhance-current-system-recommended","title":"Option 1: Enhance Current System (Recommended)","text":"<p>Instead of replacing the architecture, enhance GitData with HashFS-inspired features:</p> <pre><code># Add deduplication to current system\ndef store_file_with_dedup(self, filepath: str,\n                          fs: fsspec.AbstractFileSystem) -&gt; str:\n    \"\"\"Store file with automatic deduplication.\"\"\"\n    hash_hex = hash_file(filepath, fs)\n\n    # Check if file already exists\n    if self.fs.exists(f\"{self.data_dir}/{hash_hex}\"):\n        return hash_hex  # Return existing hash\n\n    # Store new file\n    self._store_file(filepath, hash_hex, fs)\n    return hash_hex\n</code></pre> <p>Benefits:</p> <ul> <li>Maintains all existing features</li> <li>Adds deduplication capabilities</li> <li>Preserves multi-backend support</li> <li>Incremental enhancement</li> </ul>"},{"location":"hashfs-evaluation/#option-2-hybrid-architecture","title":"Option 2: Hybrid Architecture","text":"<p>Use HashFS for local caching while maintaining fsspec for cloud storage:</p> <pre><code>class HybridContentStore:\n    def __init__(self, local_fs: HashFS, cloud_fs: fsspec.AbstractFileSystem):\n        self.local_cache = local_fs\n        self.cloud_storage = cloud_fs\n\n    def store(self, data: bytes) -&gt; str:\n        # Store in both local cache and cloud\n        local_hash = self.local_cache.put(data)\n        self.cloud_storage.put(f\"objects/{local_hash}\", data)\n        return local_hash\n</code></pre> <p>Benefits:</p> <ul> <li>Best of both worlds</li> <li>Local deduplication via HashFS</li> <li>Cloud scalability via fsspec</li> <li>Complex implementation</li> </ul>"},{"location":"hashfs-evaluation/#option-3-custom-cas-implementation","title":"Option 3: Custom CAS Implementation","text":"<p>Build a custom content-addressed storage system optimized for GitData:</p> <pre><code>class GitDataContentStore:\n    def __init__(self, fs: fsspec.AbstractFileSystem):\n        self.fs = fs\n        self.dedup_cache = {}\n\n    def store_with_dedup(self, data: bytes) -&gt; str:\n        \"\"\"Store with automatic deduplication and repair capabilities.\"\"\"\n        hash_hex = sha256(data).hexdigest()\n\n        if hash_hex in self.dedup_cache:\n            return hash_hex\n\n        # Store with nested directory structure\n        nested_path = self._get_nested_path(hash_hex)\n        self.fs.put(nested_path, data)\n        self.dedup_cache[hash_hex] = nested_path\n        return hash_hex\n</code></pre>"},{"location":"hashfs-evaluation/#recommendation","title":"Recommendation","text":"<p>Do not integrate HashFS into GitData. Instead, enhance the current system with HashFS-inspired features:</p>"},{"location":"hashfs-evaluation/#immediate-actions","title":"Immediate Actions","text":"<ol> <li>Add deduplication: Implement automatic file deduplication</li> <li>Optimize directory structure: Consider nested directories for large    file counts</li> <li>Add repair capabilities: Implement storage repair and validation</li> <li>Maintain architecture: Keep fsspec-based multi-backend support</li> </ol>"},{"location":"hashfs-evaluation/#implementation-plan","title":"Implementation Plan","text":"<pre><code># Phase 1: Add deduplication\ndef commit_with_dedup(self, files: List[str]) -&gt; str:\n    \"\"\"Commit files with automatic deduplication.\"\"\"\n    for file_path in files:\n        hash_hex = self._hash_file(file_path)\n        if not self._file_exists(hash_hex):\n            self._store_file(file_path, hash_hex)\n        self._add_to_commit(hash_hex)\n\n# Phase 2: Optimize directory structure\ndef _get_nested_path(self, hash_hex: str) -&gt; str:\n    \"\"\"Get nested directory path for hash.\"\"\"\n    return f\"{hash_hex[:2]}/{hash_hex[2:4]}/{hash_hex[4:]}\"\n\n# Phase 3: Add repair capabilities\ndef repair_storage(self) -&gt; Dict[str, Any]:\n    \"\"\"Repair and validate storage integrity.\"\"\"\n    # Implementation details...\n</code></pre>"},{"location":"hashfs-evaluation/#conclusion","title":"Conclusion","text":"<p>HashFS is an excellent content-addressed storage library, but it's not suitable for GitData's requirements. The architectural incompatibilities, feature gaps, and performance limitations make integration impractical.</p> <p>GitData's current architecture is more sophisticated and feature-complete than what HashFS provides. Instead of replacing it, we should enhance the existing system with HashFS-inspired optimizations while maintaining the multi-backend, version-controlled, metadata-rich capabilities that make GitData unique.</p> <p>This approach preserves GitData's competitive advantages while adding the performance benefits of content-addressed storage optimization.</p>"},{"location":"ui-design-choices/","title":"File Removal UI Design Choices","text":""},{"location":"ui-design-choices/#visual-hierarchy-layout","title":"Visual Hierarchy &amp; Layout","text":""},{"location":"ui-design-choices/#card-based-design","title":"Card-Based Design","text":"<p>We chose a card-based design for file removal items to create clear visual separation and hierarchy. Each file item is presented as an individual card with:</p> <ul> <li>Background: Clean white/card background using <code>hsl(var(--card))</code></li> <li>Subtle shadows: <code>box-shadow: 0 1px 2px hsl(var(--border) / 0.1)</code> for   depth</li> <li>Rounded corners: 8px border radius for modern, friendly appearance</li> <li>Generous padding: 1rem vertical, 1.25rem horizontal for breathing room</li> </ul> <p>This approach makes each file feel like a distinct, interactive element rather than just text in a list.</p>"},{"location":"ui-design-choices/#horizontal-layout","title":"Horizontal Layout","text":"<p>We implemented a strict horizontal layout with:</p> <ul> <li>Flexbox alignment: <code>display: flex</code>, <code>flex-direction: row</code>,   <code>align-items: center</code></li> <li>Consistent spacing: 1rem gap between checkbox, icon, and text</li> <li>Proper element sizing: Checkbox and icon are <code>flex-shrink: 0</code> to maintain   size</li> </ul> <p>This creates a clean, scannable interface where users can quickly identify and select files.</p>"},{"location":"ui-design-choices/#interactive-design","title":"Interactive Design","text":""},{"location":"ui-design-choices/#checkbox-design","title":"Checkbox Design","text":"<p>We chose a larger, more prominent checkbox design:</p> <ul> <li>Size: 24px \u00d7 24px (larger than standard 16px) for better visibility</li> <li>Color: Red accent color (<code>hsl(var(--destructive))</code>) to signal destructive   action</li> <li>Focus states: Red outline for accessibility compliance</li> <li>Smooth transitions: 0.2s ease for all state changes</li> </ul> <p>The larger size and red color immediately communicate that this is a removal action, reducing user confusion.</p>"},{"location":"ui-design-choices/#hover-selection-states","title":"Hover &amp; Selection States","text":"<p>We implemented sophisticated interaction feedback:</p> <ul> <li>Hover effects: Subtle lift animation (<code>translateY(-1px)</code>) with enhanced   shadows</li> <li>Checked state: Red-tinted background with red border and icon</li> <li>Smooth transitions: All interactions animate smoothly for polished feel</li> </ul> <p>These states provide clear visual feedback about user actions and system state.</p>"},{"location":"ui-design-choices/#color-typography","title":"Color &amp; Typography","text":""},{"location":"ui-design-choices/#color-system","title":"Color System","text":"<p>We leveraged the existing design system colors:</p> <ul> <li>Destructive actions: Red color scheme for removal operations</li> <li>Muted elements: File icons use muted colors that brighten on interaction</li> <li>Consistent theming: All colors use CSS custom properties for consistency</li> </ul>"},{"location":"ui-design-choices/#typography-hierarchy","title":"Typography Hierarchy","text":"<p>We refined the text presentation:</p> <ul> <li>Filename: 0.95rem, medium weight, proper line height for readability</li> <li>File size: 0.8rem, muted color, with proper spacing from filename</li> <li>Color contrast: Ensured proper contrast ratios for accessibility</li> </ul>"},{"location":"ui-design-choices/#animation-transitions","title":"Animation &amp; Transitions","text":""},{"location":"ui-design-choices/#smooth-interactions","title":"Smooth Interactions","text":"<p>We implemented consistent 0.2s ease transitions for:</p> <ul> <li>Hover states: Background, border, and shadow changes</li> <li>Selection states: Color and opacity changes</li> <li>Focus states: Outline and accent color changes</li> </ul> <p>This creates a polished, professional feel that responds immediately to user input.</p>"},{"location":"ui-design-choices/#micro-interactions","title":"Micro-interactions","text":"<p>We added subtle animations:</p> <ul> <li>Lift effect: Items slightly rise on hover for depth perception</li> <li>Color transitions: Icons and backgrounds smoothly change color</li> <li>Shadow evolution: Shadows grow and change color based on state</li> </ul> <p>These micro-interactions provide delightful feedback without being distracting.</p>"},{"location":"ui-design-choices/#accessibility-considerations","title":"Accessibility Considerations","text":""},{"location":"ui-design-choices/#focus-management","title":"Focus Management","text":"<p>We ensured proper keyboard navigation:</p> <ul> <li>Focus outlines: Red outline on checkbox focus for visibility</li> <li>Proper tab order: Logical navigation through interactive elements</li> <li>Color contrast: Sufficient contrast ratios for all text elements</li> </ul>"},{"location":"ui-design-choices/#visual-clarity","title":"Visual Clarity","text":"<p>We prioritized clear visual communication:</p> <ul> <li>Larger touch targets: 24px checkbox for easier interaction</li> <li>Clear state indication: Obvious visual difference between   checked/unchecked</li> <li>Consistent spacing: Predictable layout for screen readers</li> </ul>"},{"location":"ui-design-choices/#design-system-integration","title":"Design System Integration","text":""},{"location":"ui-design-choices/#css-custom-properties","title":"CSS Custom Properties","text":"<p>We used the existing design system variables:</p> <ul> <li>Color tokens: <code>--destructive</code>, <code>--card</code>, <code>--border</code>, <code>--foreground</code></li> <li>Consistent theming: All colors adapt to light/dark themes</li> <li>Maintainable code: Easy to update colors across the entire system</li> </ul>"},{"location":"ui-design-choices/#component-classes","title":"Component Classes","text":"<p>We created reusable component classes:</p> <ul> <li><code>.file-remove-item</code>: Main container styling</li> <li><code>.file-remove-checkbox</code>: Checkbox-specific styling</li> <li>Modular approach: Easy to apply to other similar interfaces</li> </ul> <p>This approach ensures consistency and maintainability across the application.</p>"},{"location":"bug-fixes/catalog-commit-count-bug/","title":"Catalog Landing Page Commit Count Bug Fix","text":""},{"location":"bug-fixes/catalog-commit-count-bug/#bug-description","title":"Bug Description","text":"<p>Issue: Datasets on the catalog landing page (<code>/catalog/{catalog_id}</code>) displayed \"0 commits\" even when they had actual commits.</p> <p>Symptoms: - Catalog landing page showed \"0 commits\" for all datasets - Individual dataset pages showed correct commit counts (e.g., \"2 commits\") - Misleading user experience - users thought datasets were empty</p> <p>Root Cause: The <code>list_datasets()</code> route handler in <code>kirin/web/app.py</code> was hardcoding commit counts to 0 instead of loading actual dataset data.</p>"},{"location":"bug-fixes/catalog-commit-count-bug/#technical-details","title":"Technical Details","text":""},{"location":"bug-fixes/catalog-commit-count-bug/#problematic-code-before-fix","title":"Problematic Code (Before Fix)","text":"<pre><code># kirin/web/app.py lines 245-257\ndatasets = []\nfor dataset_name in dataset_names:\n    datasets.append(\n        {\n            \"name\": dataset_name,\n            \"description\": \"\",  # Will load when viewing\n            \"commit_count\": 0,  # Will load when viewing \u2190 HARDCODED TO 0\n            \"current_commit\": None,  # Will load when viewing\n            \"total_size\": 0,  # Will load when viewing\n            \"last_updated\": None,  # Will load when viewing\n        }\n    )\n</code></pre>"},{"location":"bug-fixes/catalog-commit-count-bug/#root-cause-analysis","title":"Root Cause Analysis","text":"<ol> <li>Performance Optimization Gone Wrong: Code was designed to avoid \"expensive operations\" by not loading dataset objects</li> <li>Incomplete Lazy Loading: Comments suggested lazy loading was intended but never implemented</li> <li>Hardcoded Values: All dataset metadata was hardcoded to default/empty values</li> </ol>"},{"location":"bug-fixes/catalog-commit-count-bug/#fix-implementation","title":"Fix Implementation","text":"<pre><code># kirin/web/app.py lines 245-257 (After Fix)\ndatasets = []\nfor dataset_name in dataset_names:\n    dataset = kirin_catalog.get_dataset(dataset_name)  # Load dataset object\n    datasets.append(\n        {\n            \"name\": dataset_name,\n            \"description\": dataset.description,\n            \"commit_count\": len(dataset.history()),  # Calculate actual count\n            \"current_commit\": dataset.current_commit.hash if dataset.current_commit else None,\n            \"total_size\": 0,  # Can calculate if needed\n            \"last_updated\": dataset.current_commit.timestamp.isoformat() if dataset.current_commit else None,\n        }\n    )\n</code></pre>"},{"location":"bug-fixes/catalog-commit-count-bug/#fix-benefits","title":"Fix Benefits","text":"<ul> <li>Accurate Information: Users see correct commit counts</li> <li>Better UX: Current commit hashes and timestamps displayed</li> <li>Consistent Behavior: Landing page matches individual dataset pages</li> <li>Minimal Performance Impact: Slight increase in page load time for accurate data</li> </ul>"},{"location":"bug-fixes/catalog-commit-count-bug/#testing","title":"Testing","text":""},{"location":"bug-fixes/catalog-commit-count-bug/#test-coverage","title":"Test Coverage","text":"<p>The fix includes comprehensive tests in <code>tests/web_ui/test_catalog_commit_count_bug.py</code>:</p> <ol> <li>Basic Functionality: Datasets with commits show correct counts</li> <li>Empty Datasets: Datasets without commits show 0 correctly</li> <li>Mixed Scenarios: Catalogs with both empty and populated datasets</li> <li>Performance: Multiple datasets don't cause performance issues</li> </ol>"},{"location":"bug-fixes/catalog-commit-count-bug/#test-execution","title":"Test Execution","text":"<pre><code># Run the specific bug fix tests\npixi run -e tests python -m pytest tests/web_ui/test_catalog_commit_count_bug.py -v\n\n# Run all web UI tests\npixi run -e tests python -m pytest tests/web_ui/ -v\n</code></pre>"},{"location":"bug-fixes/catalog-commit-count-bug/#prevention","title":"Prevention","text":""},{"location":"bug-fixes/catalog-commit-count-bug/#how-to-avoid-similar-bugs","title":"How to Avoid Similar Bugs","text":"<ol> <li>Avoid Hardcoded Values: Never hardcode data that should be calculated</li> <li>Complete Lazy Loading: If implementing lazy loading, ensure it's actually implemented</li> <li>Test Data Accuracy: Always test that displayed data matches actual data</li> <li>Performance vs Accuracy: Balance performance optimizations with data accuracy</li> </ol>"},{"location":"bug-fixes/catalog-commit-count-bug/#code-review-checklist","title":"Code Review Checklist","text":"<ul> <li>[ ] Are displayed values calculated from actual data?</li> <li>[ ] Do performance optimizations maintain data accuracy?</li> <li>[ ] Are there tests that verify displayed data matches actual data?</li> <li>[ ] Are lazy loading features actually implemented?</li> </ul>"},{"location":"bug-fixes/catalog-commit-count-bug/#related-files","title":"Related Files","text":"<ul> <li>Bug Location: <code>kirin/web/app.py</code> lines 245-257</li> <li>Tests: <code>tests/web_ui/test_catalog_commit_count_bug.py</code></li> <li>Template: <code>kirin/web/templates/datasets.html</code> (displays the data)</li> <li>Individual Dataset View: <code>kirin/web/app.py</code> lines 507-563 (shows correct implementation)</li> </ul>"},{"location":"bug-fixes/catalog-commit-count-bug/#impact","title":"Impact","text":"<ul> <li>User Experience: Significantly improved - users see accurate information</li> <li>Performance: Minimal impact - slight increase in page load time</li> <li>Maintenance: Better code - no more hardcoded values</li> <li>Testing: Comprehensive test coverage prevents regression</li> </ul>"}]}