{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Kirin Documentation","text":"<p>Welcome to Kirin - simplified \"git\" for data versioning!</p>"},{"location":"#what-is-kirin","title":"What is Kirin?","text":"<p>Kirin is a simplified tool for version-controlling data using content-addressed storage. It provides linear commit history for datasets without the complexity of branching and merging.</p> <p>Key Benefits:</p> <ul> <li>Linear versioning: Simple, Git-like commits without branching   complexity</li> <li>Content-addressed storage: Files stored by content hash for   integrity and deduplication</li> <li>Cloud support: Works with S3, GCS, Azure, and more</li> <li>Ergonomic API: Designed for data science workflows</li> <li>Zero-copy operations: Efficient handling of large files</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Create a catalog (works with local and cloud storage)\ncatalog = Catalog(root_dir=\"/path/to/data\")  # Local storage\ncatalog = Catalog(root_dir=\"s3://my-bucket\")  # S3 storage\n\n# Get or create a dataset\nds = catalog.get_dataset(\"my_dataset\")\n\n# Commit files\ncommit_hash = ds.commit(message=\"Initial commit\", add_files=[\"file1.csv\"])\n\n# Work with files locally\nwith ds.local_files() as local_files:\n    csv_path = local_files[\"file1.csv\"]\n    content = Path(csv_path).read_text()\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Quickstart - Get up and running in 5   minutes</li> <li>Installation - Installation options   and setup</li> <li>Core Concepts - Understanding   datasets, commits, and content-addressing</li> </ul>"},{"location":"#user-guides","title":"User Guides","text":"<ul> <li>Basic Usage - Essential workflows for working   with datasets</li> <li>Cloud Storage - Set up and use cloud storage   backends</li> <li>Working with Files - File operations and   data science integration</li> <li>Commit Management - Understanding and   working with commit history</li> </ul>"},{"location":"#web-ui","title":"Web UI","text":"<ul> <li>Web UI Overview - Getting started with the web   interface</li> <li>Catalog Management - Advanced catalog   configuration</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Storage Format - Technical storage details</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<ul> <li>Architecture Overview - System architecture   and design principles</li> </ul>"},{"location":"#why-kirin-exists","title":"Why Kirin Exists","text":"<p>Kirin addresses critical needs in machine learning and data science workflows:</p> <ul> <li>Linear Data Versioning: Track changes to datasets with simple, linear commits</li> <li>Content-Addressed Storage: Ensure data integrity and enable deduplication</li> <li>Multi-Backend Support: Work with S3, GCS, Azure, local filesystem, and more</li> <li>Serverless Architecture: No dedicated servers required</li> <li>Ergonomic Python API: Focus on ease of use and developer experience</li> <li>File Versioning: Track changes to individual files over time</li> </ul>"},{"location":"#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Experiment tracking: Version your training data and model inputs</li> <li>Data pipeline versioning: Track changes in ETL processes</li> <li>Collaborative research: Share datasets with exact version control</li> <li>Reproducible analysis: Ensure you can recreate your results</li> <li>MLOps workflows: Deploy models with exact data dependencies</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code># Development (with pixi)\ngit clone git@github.com:nll-ai/kirin\ncd kirin\npixi install\npixi run kirin ui\n\n# Production (with uv)\nuv tool install kirin\nuv run kirin ui\n\n# One-time use (with uvx)\nuvx kirin ui\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ol> <li>Quickstart - Try Kirin with a simple example</li> <li>Core Concepts - Understand how Kirin works</li> <li>Basic Usage - Learn common workflows</li> <li>Cloud Storage - Set up cloud storage</li> </ol>"},{"location":"cloud-storage-auth/","title":"Cloud Storage Authentication Guide","text":"<p>When using <code>kirin</code> with cloud storage backends (S3, GCS, Azure, etc.), you need to provide authentication credentials. This guide shows you how to authenticate with different cloud providers using the new cloud-agnostic authentication parameters.</p>"},{"location":"cloud-storage-auth/#new-cloud-agnostic-authentication-recommended","title":"New Cloud-Agnostic Authentication (Recommended)","text":"<p>Kirin now supports cloud-agnostic authentication parameters that work across all cloud providers:</p>"},{"location":"cloud-storage-auth/#awss3-authentication","title":"AWS/S3 Authentication","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Using AWS profile\ncatalog = Catalog(\n    root_dir=\"s3://my-bucket/data\",\n    aws_profile=\"my-profile\"\n)\n\n# Using Dataset with AWS profile\ndataset = Dataset(\n    root_dir=\"s3://my-bucket/data\",\n    name=\"my-dataset\",\n    aws_profile=\"my-profile\"\n)\n</code></pre>"},{"location":"cloud-storage-auth/#gcpgcs-authentication","title":"GCP/GCS Authentication","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Using service account key file\ncatalog = Catalog(\n    root_dir=\"gs://my-bucket/data\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"my-project\"\n)\n\n# Using Dataset with GCS credentials\ndataset = Dataset(\n    root_dir=\"gs://my-bucket/data\",\n    name=\"my-dataset\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"my-project\"\n)\n</code></pre>"},{"location":"cloud-storage-auth/#azure-blob-storage-authentication","title":"Azure Blob Storage Authentication","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Using connection string\ncatalog = Catalog(\n    root_dir=\"az://my-container/data\",\n    azure_connection_string=os.getenv(\"AZURE_CONNECTION_STRING\")\n)\n\n# Using account name and key\ndataset = Dataset(\n    root_dir=\"az://my-container/data\",\n    name=\"my-dataset\",\n    azure_account_name=\"my-account\",\n    azure_account_key=\"my-key\"\n)\n</code></pre>"},{"location":"cloud-storage-auth/#web-ui-integration","title":"Web UI Integration","text":"<p>The web UI also supports cloud authentication through the <code>CatalogConfig.to_catalog()</code> method:</p> <pre><code>from kirin.web.config import CatalogConfig\n\n# Create catalog config with cloud auth\nconfig = CatalogConfig(\n    id=\"my-catalog\",\n    name=\"My Catalog\",\n    root_dir=\"s3://my-bucket/data\",\n    aws_profile=\"my-profile\"\n)\n\n# Convert to runtime catalog\ncatalog = config.to_catalog()\n</code></pre>"},{"location":"cloud-storage-auth/#legacy-authentication-methods","title":"Legacy Authentication Methods","text":"<p>The following sections show the legacy authentication methods that still work but are less convenient than the new cloud-agnostic approach.</p>"},{"location":"cloud-storage-auth/#google-cloud-storage-gcs","title":"Google Cloud Storage (GCS)","text":""},{"location":"cloud-storage-auth/#error-you-might-see","title":"Error You Might See","text":"<pre><code>gcsfs.retry.HttpError: Anonymous caller does not have storage.objects.list access\nto the Google Cloud Storage bucket. Permission 'storage.objects.list' denied on\nresource (or it may not exist)., 401\n</code></pre>"},{"location":"cloud-storage-auth/#solutions","title":"Solutions","text":""},{"location":"cloud-storage-auth/#option-1-application-default-credentials-recommended","title":"Option 1: Application Default Credentials (Recommended)","text":"<p>Use <code>gcloud</code> CLI to set up credentials:</p> <pre><code># Install gcloud CLI first, then:\ngcloud auth application-default login\n</code></pre> <p>Then use kirin normally:</p> <pre><code>from kirin.dataset import Dataset\n\n# Will automatically use your gcloud credentials\nds = Dataset(root_dir=\"gs://my-bucket/datasets\", dataset_name=\"my_data\")\n</code></pre>"},{"location":"cloud-storage-auth/#option-2-service-account-key-file","title":"Option 2: Service Account Key File","text":"<pre><code>from kirin.dataset import Dataset\nimport fsspec\n\n# Create filesystem with service account\nfs = fsspec.filesystem(\n    'gs',\n    token='/path/to/service-account-key.json'\n)\n\n# Pass it to Dataset\nds = Dataset(\n    root_dir=\"gs://my-bucket/datasets\",\n    dataset_name=\"my_data\",\n    fs=fs  # Use authenticated filesystem\n)\n</code></pre>"},{"location":"cloud-storage-auth/#option-3-environment-variable","title":"Option 3: Environment Variable","text":"<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/service-account-key.json\"\n</code></pre> <pre><code>from kirin.dataset import Dataset\n\n# Will automatically use the credentials from environment variable\nds = Dataset(root_dir=\"gs://my-bucket/datasets\", dataset_name=\"my_data\")\n</code></pre>"},{"location":"cloud-storage-auth/#option-4-pass-credentials-directly","title":"Option 4: Pass Credentials Directly","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    'gs',\n    project='my-project-id',\n    token='cloud'  # Uses gcloud credentials\n)\n\nds = Dataset(root_dir=\"gs://my-bucket/datasets\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#amazon-s3","title":"Amazon S3","text":""},{"location":"cloud-storage-auth/#option-1-aws-cli-credentials-recommended","title":"Option 1: AWS CLI Credentials (Recommended)","text":"<pre><code># Configure AWS credentials\naws configure\n</code></pre> <p>Then use normally:</p> <pre><code>from kirin.dataset import Dataset\n\nds = Dataset(root_dir=\"s3://my-bucket/datasets\", dataset_name=\"my_data\")\n</code></pre>"},{"location":"cloud-storage-auth/#option-2-environment-variables","title":"Option 2: Environment Variables","text":"<pre><code>export AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\nexport AWS_DEFAULT_REGION=\"us-east-1\"\n</code></pre>"},{"location":"cloud-storage-auth/#option-3-pass-credentials-explicitly","title":"Option 3: Pass Credentials Explicitly","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    's3',\n    key='your-access-key',\n    secret='your-secret-key',\n    client_kwargs={'region_name': 'us-east-1'}\n)\n\nds = Dataset(root_dir=\"s3://my-bucket/datasets\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#s3-compatible-services-minio-backblaze-b2-digitalocean-spaces","title":"S3-Compatible Services (Minio, Backblaze B2, DigitalOcean Spaces)","text":""},{"location":"cloud-storage-auth/#minio-example","title":"Minio Example","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    's3',\n    key='your-access-key',\n    secret='your-secret-key',\n    client_kwargs={\n        'endpoint_url': 'http://localhost:9000'  # Your Minio endpoint\n    }\n)\n\nds = Dataset(root_dir=\"s3://my-bucket/datasets\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#backblaze-b2-example","title":"Backblaze B2 Example","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    's3',\n    key='your-application-key-id',\n    secret='your-application-key',\n    client_kwargs={\n        'endpoint_url': 'https://s3.us-west-002.backblazeb2.com'\n    }\n)\n\nds = Dataset(root_dir=\"s3://my-bucket/datasets\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#azure-blob-storage","title":"Azure Blob Storage","text":""},{"location":"cloud-storage-auth/#option-1-azure-cli-credentials","title":"Option 1: Azure CLI Credentials","text":"<pre><code>az login\n</code></pre>"},{"location":"cloud-storage-auth/#option-2-connection-string","title":"Option 2: Connection String","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    'az',\n    connection_string='your-connection-string'\n)\n\nds = Dataset(root_dir=\"az://container/path\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#option-3-account-key","title":"Option 3: Account Key","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    'az',\n    account_name='your-account-name',\n    account_key='your-account-key'\n)\n\nds = Dataset(root_dir=\"az://container/path\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#general-pattern","title":"General Pattern","text":"<p>For any cloud provider, the recommended pattern is:</p> <ol> <li>New Cloud-Agnostic Approach (Recommended):</li> </ol> <pre><code>from kirin import Catalog, Dataset\n\n# For AWS/S3\ncatalog = Catalog(root_dir=\"s3://bucket/path\", aws_profile=\"my-profile\")\ndataset = Dataset(root_dir=\"s3://bucket/path\", name=\"my_data\",\n                 aws_profile=\"my-profile\")\n\n# For GCS\ncatalog = Catalog(root_dir=\"gs://bucket/path\", gcs_token=\"/path/to/key.json\",\n                 gcs_project=\"my-project\")\ndataset = Dataset(root_dir=\"gs://bucket/path\", name=\"my_data\",\n                 gcs_token=\"/path/to/key.json\", gcs_project=\"my-project\")\n\n# For Azure\ncatalog = Catalog(root_dir=\"az://container/path\", azure_connection_string=\"...\")\ndataset = Dataset(root_dir=\"az://container/path\", name=\"my_data\", azure_connection_string=\"...\")\n</code></pre> <ol> <li>Auto-detect (works if credentials are already configured):</li> </ol> <pre><code>ds = Dataset(root_dir=\"protocol://bucket/path\", name=\"my_data\")\n</code></pre> <ol> <li>Legacy explicit authentication (still supported):</li> </ol> <pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\n# Create authenticated filesystem\nfs = fsspec.filesystem('protocol', **auth_kwargs)\n\n# Pass to Dataset\nds = Dataset(root_dir=\"protocol://bucket/path\", name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#testing-without-credentials","title":"Testing Without Credentials","text":"<p>For local development/testing without cloud access, you can use:</p> <pre><code># In-memory filesystem (no cloud needed)\nfrom kirin.dataset import Dataset\n\nds = Dataset(root_dir=\"memory://test-data\", dataset_name=\"my_data\")\n</code></pre>"},{"location":"cloud-storage-auth/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cloud-storage-auth/#issue-anonymous-caller-or-access-denied","title":"Issue: \"Anonymous caller\" or \"Access Denied\"","text":"<ul> <li>Cause: No credentials provided</li> <li>Solution: Set up credentials using one of the methods above</li> </ul>"},{"location":"cloud-storage-auth/#issue-permission-denied","title":"Issue: \"Permission denied\"","text":"<ul> <li>Cause: Credentials don't have required permissions</li> <li>Solution: Ensure your IAM role/service account has read/write permissions   on the bucket</li> </ul>"},{"location":"cloud-storage-auth/#issue-bucket-does-not-exist","title":"Issue: \"Bucket does not exist\"","text":"<ul> <li>Cause: Bucket name is incorrect or doesn't exist</li> <li>Solution: Create the bucket first or check the bucket name</li> </ul>"},{"location":"cloud-storage-auth/#issue-import-errors-like-no-module-named-s3fs","title":"Issue: Import errors like \"No module named 's3fs'\"","text":"<ul> <li>Cause: Cloud storage package not installed</li> <li>Solution: Install required package:</li> </ul> <pre><code>pip install s3fs      # For S3\npip install gcsfs     # For GCS\npip install adlfs     # For Azure\n</code></pre>"},{"location":"cloud-storage-auth/#known-issues","title":"Known Issues","text":""},{"location":"cloud-storage-auth/#macos-python-313-ssl-certificate-verification","title":"macOS Python 3.13 SSL Certificate Verification","text":"<p>On macOS with Python 3.13, you may encounter SSL certificate verification errors when using cloud storage backends. This is a known Python/macOS issue.</p> <p>Workaround: The web UI skips connection testing during backend creation. Backends are validated when actually used. If you encounter SSL errors during actual usage, install certificates:</p> <pre><code>/Applications/Python\\ 3.13/Install\\ Certificates.command\n</code></pre> <p>Or use <code>certifi</code>:</p> <pre><code>pip install --upgrade certifi\n</code></pre>"},{"location":"cloud-storage-auth/#web-ui-auto-authentication","title":"Web UI Auto-Authentication","text":"<p>NEW: The Kirin web UI can automatically execute authentication commands when needed.</p>"},{"location":"cloud-storage-auth/#configuring-auto-authentication","title":"Configuring Auto-Authentication","text":"<p>When creating or editing a catalog in the web UI, you can provide an optional authentication command:</p> <p>AWS S3:</p> <pre><code>Auth Command: aws sso login --profile {{ aws_profile }}\n</code></pre> <p>GCP GCS:</p> <pre><code>Auth Command: gcloud auth login\n</code></pre> <p>Azure Blob Storage:</p> <pre><code>Auth Command: az login\n</code></pre>"},{"location":"cloud-storage-auth/#how-auto-authentication-works","title":"How Auto-Authentication Works","text":"<ol> <li>Catalog Access: When you click on a catalog, Kirin attempts to    connect</li> <li>Authentication Check: If authentication fails or times out, Kirin    checks for stored auth command</li> <li>Auto-Execution: If found, Kirin executes the command automatically    (30-second timeout)</li> <li>Retry: If authentication succeeds, Kirin retries loading the    datasets</li> <li>Feedback: Success/failure messages are shown in the UI</li> </ol>"},{"location":"cloud-storage-auth/#benefits","title":"Benefits","text":"<ul> <li>No manual CLI: Authentication happens automatically in the web UI</li> <li>Faster workflow: No need to switch to terminal and run commands</li> <li>Clear feedback: You see exactly what happened (success or failure)</li> <li>Manual fallback: You can still authenticate manually if needed</li> </ul>"},{"location":"cloud-storage-auth/#timeout-protection","title":"Timeout Protection","text":"<p>To prevent the UI from hanging on slow or failed connections:</p> <ul> <li>10-second timeout for catalog operations (listing datasets)</li> <li>5-second timeout for individual dataset loading</li> <li>30-second timeout for authentication commands</li> <li>Clear error messages when timeouts occur</li> </ul>"},{"location":"cloud-storage-auth/#security-considerations","title":"Security Considerations","text":"<ul> <li>Commands are stored locally in your catalog configuration file</li> <li>No passwords stored: Auth commands only contain profile names or   trigger browser-based OAuth</li> <li>Subprocess isolation: Commands run in isolated subprocesses with   timeout protection</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>High-level system architecture and design principles for Kirin.</p>"},{"location":"architecture/overview/#system-architecture","title":"System Architecture","text":"<p>Kirin implements a simplified content-addressed storage system with the following key components:</p> <pre><code>graph TB\n    subgraph \"Kirin Core Components\"\n        DA[\"Dataset API&lt;br/&gt;\u2022 File operations&lt;br/&gt;\u2022 Commit management&lt;br/&gt;\u2022 Versioning\"]\n        CS[\"Commit Store&lt;br/&gt;\u2022 Linear history&lt;br/&gt;\u2022 Metadata&lt;br/&gt;\u2022 References\"]\n        COS[\"Content Store&lt;br/&gt;\u2022 Content hash&lt;br/&gt;\u2022 Deduplication&lt;br/&gt;\u2022 Backend agnostic\"]\n    end\n\n    subgraph \"Storage Layer\"\n        FS[\"FSSpec Layer&lt;br/&gt;\u2022 Local FS&lt;br/&gt;\u2022 S3&lt;br/&gt;\u2022 GCS&lt;br/&gt;\u2022 Azure&lt;br/&gt;\u2022 Other backends\"]\n    end\n\n    DA --&gt; CS\n    DA --&gt; COS\n    CS --&gt; COS\n    COS --&gt; FS</code></pre>"},{"location":"architecture/overview/#core-design-principles","title":"Core Design Principles","text":""},{"location":"architecture/overview/#1-simplified-data-versioning","title":"1. Simplified Data Versioning","text":"<p>Kirin is simplified \"git\" for data - follows git conventions but with linear-only history:</p> <ul> <li>Linear Commits: Simple, linear commit history without branching complexity</li> <li>Content-Addressed Storage: Files stored by content hash for integrity and deduplication</li> <li>Ergonomic Python API: Focus on ease of use and developer experience</li> <li>Backend-Agnostic: Works with any storage backend via fsspec</li> <li>No Branching: Linear-only commit history to avoid complexity</li> </ul>"},{"location":"architecture/overview/#2-content-addressed-storage-design","title":"2. Content-Addressed Storage Design","text":"<p>CRITICAL: Files are stored without file extensions in the content-addressed storage system:</p> <ul> <li>Storage Path: <code>root_dir/data/{hash[:2]}/{hash[2:]}</code> (e.g.,   <code>data/ab/cdef1234...</code>)</li> <li>No Extensions: Original <code>.csv</code>, <code>.txt</code>, <code>.json</code> extensions are not   preserved in storage</li> <li>Metadata Storage: File extensions are stored as metadata in the <code>File</code>   entity's <code>name</code> attribute</li> <li>Extension Restoration: When files are downloaded or accessed, they get   their original names back</li> <li>Content Integrity: Files are identified purely by content hash, ensuring   data integrity</li> <li>Deduplication: Identical content (regardless of original filename) is   stored only once</li> </ul>"},{"location":"architecture/overview/#3-file-access-patterns","title":"3. File Access Patterns","text":"<p>Kirin provides simple file access through standard Python file operations:</p> <ul> <li>Temporary file downloads: Files are downloaded to temporary locations   when accessed</li> <li>Standard file handles: Files are accessed through normal Python file   objects</li> <li>Automatic cleanup: Temporary files are automatically cleaned up when   file handles are closed</li> <li>Streaming support: Large files can be streamed through fsspec backends   for efficient transfer</li> </ul>"},{"location":"architecture/overview/#key-benefits","title":"Key Benefits","text":""},{"location":"architecture/overview/#for-data-scientists","title":"For Data Scientists","text":"<ul> <li>Linear Data Versioning: Track changes to datasets with simple, linear commits</li> <li>Content-Addressed Storage: Ensure data integrity and enable deduplication</li> <li>Multi-Backend Support: Work with S3, GCS, Azure, local filesystem, and more</li> <li>Serverless Architecture: No dedicated servers required</li> <li>Ergonomic Python API: Focus on ease of use and developer experience</li> <li>File Versioning: Track changes to individual files over time</li> </ul>"},{"location":"architecture/overview/#for-data-engineers","title":"For Data Engineers","text":"<ul> <li>Backend-agnostic: Works with any storage backend via fsspec</li> <li>Automatic deduplication: Identical files stored once, saving space</li> <li>Content integrity: Files stored by content hash for data integrity</li> <li>Performance optimized: Chunked processing for large files</li> <li>Extensible: Easy to add new backends and features</li> </ul>"},{"location":"architecture/overview/#user-personas-and-jobs-to-be-done","title":"User Personas and Jobs to be Done","text":""},{"location":"architecture/overview/#data-scientist-ml-engineer","title":"Data Scientist / ML Engineer","text":"<p>Jobs to be Done:</p> <ol> <li>Track Experiment Data: \"I need to keep track of which datasets were    used in which experiments so I can reproduce my results.\"</li> <li>Find and Use the Right Data Version: \"I need to identify and access    specific versions of datasets for training models.\"</li> <li>Collaborate with Team Members: \"I need to share datasets with    colleagues in a way that ensures we're all using the same exact data.\"</li> <li>Document Data Transformations: \"I need to track how raw data is    transformed into model-ready data.\"</li> </ol>"},{"location":"architecture/overview/#data-engineer","title":"Data Engineer","text":"<p>Jobs to be Done:</p> <ol> <li>Manage Data Pipelines: \"I need to ensure data pipelines produce    consistent, traceable outputs.\"</li> <li>Optimize Storage Usage: \"I need to handle large datasets efficiently    without wasting storage.\"</li> <li>Support Multiple Storage Solutions: \"I need to work with data across    various storage systems our organization uses.\"</li> <li>Ensure Data Governance: \"I need to track who accesses what data and    how it's used.\"</li> </ol>"},{"location":"architecture/overview/#data-team-manager-lead","title":"Data Team Manager / Lead","text":"<p>Jobs to be Done:</p> <ol> <li>Ensure Reproducibility: \"I need to guarantee that our team's work is    reproducible for scientific integrity and audit purposes.\"</li> <li>Manage Technical Debt: \"I need to understand data dependencies to    prevent cascading failures when data changes.\"</li> <li>Accelerate Onboarding: \"I need new team members to quickly understand    our data ecosystem.\"</li> <li>Support Regulatory Compliance: \"I need to demonstrate data provenance    for regulatory compliance.\"</li> </ol>"},{"location":"architecture/overview/#mlops-engineer","title":"MLOps Engineer","text":"<p>Jobs to be Done:</p> <ol> <li>Deploy Models with Data Dependencies: \"I need to package models with    their exact data dependencies.\"</li> <li>Monitor Data Drift: \"I need to compare production data against training    data to detect drift.\"</li> <li>Implement Data-Centric CI/CD: \"I need automated tests that verify data    quality across pipeline stages.\"</li> <li>Roll Back Data When Needed: \"I need to quickly revert to previous data    versions if issues arise.\"</li> </ol>"},{"location":"architecture/overview/#feature-to-job-mapping","title":"Feature-to-Job Mapping","text":""},{"location":"architecture/overview/#content-addressed-storage","title":"Content-Addressed Storage","text":"<ul> <li>Jobs: Track Experiment Data, Find and Use the Right Data Version,   Collaborate with Team Members, Ensure Reproducibility</li> <li>Users: Data Scientist, ML Engineer, Team Lead, Laboratory Scientist</li> </ul>"},{"location":"architecture/overview/#automatic-lineage-tracking","title":"Automatic Lineage Tracking","text":"<ul> <li>Jobs: Document Data Transformations, Manage Data Pipelines,   Track Sample Lineage, Manage Technical Debt</li> <li>Users: Data Scientist, Data Engineer, Laboratory Scientist</li> </ul>"},{"location":"architecture/overview/#backend-agnostic-storage","title":"Backend-Agnostic Storage","text":"<ul> <li>Jobs: Support Multiple Storage Solutions, Optimize Storage Usage,   Manage Collaborative Research</li> <li>Users: Data Engineer, MLOps Engineer, Laboratory Scientist</li> </ul>"},{"location":"architecture/overview/#dataset-versioning","title":"Dataset Versioning","text":"<ul> <li>Jobs: Deploy Models with Data Dependencies, Roll Back Data When Needed,   Monitor Data Drift, Ensure Experimental Reproducibility</li> <li>Users: MLOps Engineer, Data Engineer, Laboratory Scientist</li> </ul>"},{"location":"architecture/overview/#usage-tracking","title":"Usage Tracking","text":"<ul> <li>Jobs: Document Data Usage, Ensure Data Governance,   Support Regulatory Compliance, Document Methods and Parameters</li> <li>Users: Team Lead, Laboratory Scientist</li> </ul>"},{"location":"architecture/overview/#streaming-file-access","title":"Streaming File Access","text":"<ul> <li>Jobs: Optimize Storage Usage, Handle Large Datasets</li> <li>Users: Data Engineer, MLOps Engineer</li> </ul>"},{"location":"architecture/overview/#data-catalog","title":"Data Catalog","text":"<ul> <li>Jobs: Accelerate Onboarding, Find the Right Data Version,   Manage Collaborative Research</li> <li>Users: Team Lead, Data Scientist, Laboratory Scientist</li> </ul>"},{"location":"architecture/overview/#path-based-api","title":"Path-Based API","text":"<ul> <li>Jobs: Implement Data-Centric CI/CD, Manage Data Pipelines</li> <li>Users: MLOps Engineer, Data Engineer</li> </ul>"},{"location":"architecture/overview/#system-flow","title":"System Flow","text":""},{"location":"architecture/overview/#1-data-ingestion-flow","title":"1. Data Ingestion Flow","text":"<ol> <li>User provides files to be tracked</li> <li>Files are hashed and stored in content store</li> <li>A commit is created with references to file versions</li> <li>The commit is recorded in the linear history</li> </ol>"},{"location":"architecture/overview/#2-data-access-flow","title":"2. Data Access Flow","text":"<ol> <li>User requests a specific version of a file or dataset</li> <li>System resolves the logical path to a content hash</li> <li>Content is retrieved from the storage backend</li> <li>Content is provided to the user</li> </ol>"},{"location":"architecture/overview/#3-data-processing-flow","title":"3. Data Processing Flow","text":"<ol> <li>User accesses input data files</li> <li>Processing is performed on the data</li> <li>Output files are stored in Kirin</li> <li>New commit is created with updated files</li> </ol>"},{"location":"architecture/overview/#linear-vs-branching","title":"Linear vs. Branching","text":"<p>Kirin uses linear commit history instead of Git's branching model:</p> <p>Linear History (Kirin):</p> <pre><code>graph LR\n    A[Commit A] --&gt; B[Commit B]\n    B --&gt; C[Commit C]\n    C --&gt; D[Commit D]</code></pre> <p>Branching History (Git):</p> <pre><code>graph TD\n    A[Commit A] --&gt; B[Commit B]\n    B --&gt; C[Commit C]\n    B --&gt; D[Commit D]\n    D --&gt; E[Commit E]</code></pre> <p>Benefits of Linear History:</p> <ul> <li>Simpler: No merge conflicts or complex branching</li> <li>Clearer: Easy to understand data evolution</li> <li>Safer: No risk of losing data through complex merges</li> <li>Faster: No need to resolve merge conflicts</li> </ul>"},{"location":"architecture/overview/#backend-agnostic-design","title":"Backend-Agnostic Design","text":"<p>Kirin works with any storage backend through the fsspec library:</p> <p>Supported Backends:</p> <ul> <li>Local filesystem: <code>/path/to/data</code></li> <li>AWS S3: <code>s3://bucket/path</code></li> <li>Google Cloud Storage: <code>gs://bucket/path</code></li> <li>Azure Blob Storage: <code>az://container/path</code></li> <li>S3-compatible services: Minio, Backblaze B2, DigitalOcean Spaces, Wasabi</li> <li>And many more: Dropbox, Google Drive, etc. (sync/auth handled by backend)</li> </ul> <p>Benefits:</p> <ul> <li>Flexibility: Use any storage backend</li> <li>Scalability: Scale from local to cloud</li> <li>Portability: Move between backends easily</li> <li>Cost optimization: Choose the right storage for your needs</li> </ul>"},{"location":"architecture/overview/#system-flow-diagrams","title":"System Flow Diagrams","text":""},{"location":"architecture/overview/#data-ingestion-flow","title":"Data Ingestion Flow","text":"<pre><code>sequenceDiagram\n    participant U as User\n    participant D as Dataset API\n    participant CS as Content Store\n    participant CM as Commit Store\n\n    U-&gt;&gt;D: Provide files to track\n    D-&gt;&gt;CS: Hash and store files\n    CS--&gt;&gt;D: Return content hashes\n    D-&gt;&gt;CM: Create commit with file references\n    CM--&gt;&gt;D: Save commit to linear history\n    D--&gt;&gt;U: Return commit hash</code></pre>"},{"location":"architecture/overview/#data-access-flow","title":"Data Access Flow","text":"<pre><code>sequenceDiagram\n    participant U as User\n    participant D as Dataset API\n    participant CS as Content Store\n    participant FS as FSSpec Layer\n\n    U-&gt;&gt;D: Request specific file version\n    D-&gt;&gt;D: Resolve logical path to content hash\n    D-&gt;&gt;CS: Retrieve content by hash\n    CS-&gt;&gt;FS: Read from storage backend\n    FS--&gt;&gt;CS: Return file content\n    CS--&gt;&gt;D: Return content bytes\n    D--&gt;&gt;U: Provide file to user</code></pre>"},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Storage Format - Technical storage details</li> </ul>"},{"location":"getting-started/core-concepts/","title":"Core Concepts","text":"<p>Understanding the fundamental concepts behind Kirin's data versioning system.</p>"},{"location":"getting-started/core-concepts/#what-is-kirin","title":"What is Kirin?","text":"<p>Kirin is a simplified tool for version-controlling data using content-addressed storage. It provides linear commit history for datasets without the complexity of branching and merging.</p>"},{"location":"getting-started/core-concepts/#key-concepts","title":"Key Concepts","text":""},{"location":"getting-started/core-concepts/#datasets","title":"Datasets","text":"<p>A dataset is a logical collection of files that you want to version together. Think of it as a folder that tracks changes over time.</p> <pre><code>from kirin import Dataset\n\n# Create a dataset\ndataset = Dataset(root_dir=\"/path/to/data\", name=\"my_dataset\")\n</code></pre> <p>Characteristics:</p> <ul> <li>Contains multiple files</li> <li>Has a linear commit history</li> <li>Can be shared and collaborated on</li> <li>Maintains data integrity through content-addressing</li> </ul>"},{"location":"getting-started/core-concepts/#files","title":"Files","text":"<p>A file in Kirin represents a versioned file with content-addressed storage. Files are immutable once created and identified by their content hash.</p> <pre><code># Get a file from the current commit\nfile_obj = dataset.get_file(\"data.csv\")\nprint(f\"File hash: {file_obj.hash}\")\nprint(f\"File size: {file_obj.size} bytes\")\nprint(f\"Content type: {file_obj.content_type}\")\nprint(f\"Short hash: {file_obj.short_hash}\")\n</code></pre> <p>Key Properties:</p> <ul> <li>Content-addressed: Identified by content hash, not filename</li> <li>Immutable: Cannot be changed once created</li> <li>Deduplicated: Identical content stored only once</li> <li>Backend-agnostic: Works with any storage backend</li> </ul>"},{"location":"getting-started/core-concepts/#commits","title":"Commits","text":"<p>A commit represents an immutable snapshot of files at a point in time. Commits form a linear history with single parent relationships.</p> <pre><code># Create a commit\ncommit_hash = dataset.commit(\n    message=\"Add new data\",\n    add_files=[\"data.csv\", \"config.json\"]\n)\n\n# Get commit information\ncommit = dataset.get_commit(commit_hash)\nprint(f\"Commit: {commit.hash}\")\nprint(f\"Message: {commit.message}\")\nprint(f\"Timestamp: {commit.timestamp}\")\nprint(f\"Files: {commit.list_files()}\")\nprint(f\"Short hash: {commit.short_hash}\")\n</code></pre> <p>Characteristics:</p> <ul> <li>Linear history: No branching, simple parent-child relationships</li> <li>Immutable: Cannot be changed once created</li> <li>Atomic: All files in a commit are added/removed together</li> <li>Traceable: Full history of changes over time</li> </ul>"},{"location":"getting-started/core-concepts/#content-addressed-storage","title":"Content-Addressed Storage","text":"<p>Content-addressed storage means files are stored and identified by their content hash, not their filename or location.</p> <p>Benefits:</p> <ul> <li>Data integrity: Files cannot be corrupted without detection</li> <li>Deduplication: Identical content stored only once</li> <li>Efficient storage: Saves space by avoiding duplicate files</li> <li>Tamper-proof: Any change to content changes the hash</li> </ul> <p>Storage Layout:</p> <pre><code>data/\n\u251c\u2500\u2500 ab/                  # First two characters of hash\n\u2502   \u2514\u2500\u2500 cdef1234...      # Rest of hash (no file extensions)\n\u2514\u2500\u2500 ...\n</code></pre> <p>Important: Files are stored without extensions in the content store. Original extensions are preserved as metadata in the File entity's <code>name</code> attribute and restored when files are accessed.</p>"},{"location":"getting-started/core-concepts/#catalogs","title":"Catalogs","text":"<p>A catalog is a collection of datasets that you want to manage together. It's like a workspace for multiple related datasets.</p> <pre><code>from kirin import Catalog\n\n# Create a catalog\ncatalog = Catalog(root_dir=\"/path/to/data\")\n\n# List all datasets\ndatasets = catalog.datasets()\nprint(f\"Available datasets: {datasets}\")\n\n# Get a specific dataset\ndataset = catalog.get_dataset(\"my_dataset\")\n</code></pre> <p>Use Cases:</p> <ul> <li>Project organization: Group related datasets</li> <li>Team collaboration: Share multiple datasets</li> <li>Workflow management: Organize data processing pipelines</li> </ul>"},{"location":"getting-started/core-concepts/#how-it-works","title":"How It Works","text":""},{"location":"getting-started/core-concepts/#1-file-storage","title":"1. File Storage","text":"<p>When you add a file to Kirin:</p> <ol> <li>Hash calculation: File content is hashed (SHA256)</li> <li>Content storage: File stored at <code>data/{hash[:2]}/{hash[2:]}</code></li> <li>Metadata tracking: Original filename stored as metadata</li> <li>Deduplication: If file already exists, no duplicate storage</li> </ol>"},{"location":"getting-started/core-concepts/#2-commit-process","title":"2. Commit Process","text":"<p>When you create a commit:</p> <ol> <li>File staging: Files to be added/removed are identified</li> <li>Hash resolution: Content hashes are calculated/resolved</li> <li>Commit creation: New commit object created with file references</li> <li>History update: Commit added to linear history</li> </ol>"},{"location":"getting-started/core-concepts/#3-data-access","title":"3. Data Access","text":"<p>When you access files:</p> <ol> <li>Commit resolution: Current commit or specific commit identified</li> <li>File lookup: File references resolved to content hashes</li> <li>Content retrieval: Files retrieved from content store</li> <li>Extension restoration: Original filenames restored</li> </ol>"},{"location":"getting-started/core-concepts/#linear-vs-branching","title":"Linear vs. Branching","text":"<p>Kirin uses linear commit history instead of Git's branching model:</p> <p>Linear History (Kirin):</p> <pre><code>Commit A \u2192 Commit B \u2192 Commit C \u2192 Commit D\n</code></pre> <p>Branching History (Git):</p> <pre><code>Commit A \u2192 Commit B \u2192 Commit C\n         \u2198\n           Commit D \u2192 Commit E\n</code></pre> <p>Benefits of Linear History:</p> <ul> <li>Simpler: No merge conflicts or complex branching</li> <li>Clearer: Easy to understand data evolution</li> <li>Safer: No risk of losing data through complex merges</li> <li>Faster: No need to resolve merge conflicts</li> </ul>"},{"location":"getting-started/core-concepts/#creating-branches-with-new-datasets","title":"Creating \"Branches\" with New Datasets","text":"<p>If you need branching-like functionality, create a new dataset using existing files:</p> <pre><code># Original dataset\noriginal_dataset = catalog.get_dataset(\"experiment_v1\")\n\n# Create a \"branch\" by starting a new dataset with existing files\nbranch_dataset = catalog.create_dataset(\"experiment_v2\")\n\n# Copy files from the original dataset to the new one\nwith original_dataset.local_files() as local_files:\n    for filename, local_path in local_files.items():\n        # Copy file to new dataset\n        import shutil\n        shutil.copy2(local_path, filename)\n\n# Commit the copied files to the new dataset\nbranch_dataset.commit(\n    message=\"Branch from experiment_v1\",\n    add_files=[\"data.csv\", \"config.json\"]\n)\n\n# Now you can develop independently in each dataset\noriginal_dataset.commit(\"Continue original work\", add_files=[\"new_data.csv\"])\nbranch_dataset.commit(\"Try different approach\", add_files=[\"alternative_data.csv\"])\n</code></pre> <p>Benefits of Dataset-based \"Branching\":</p> <ul> <li>Clear separation: Each dataset is independent</li> <li>Easy comparison: Compare datasets side by side</li> <li>No conflicts: No merge conflicts between datasets</li> <li>Flexible: Can share files between datasets as needed</li> </ul>"},{"location":"getting-started/core-concepts/#backend-agnostic-design","title":"Backend-Agnostic Design","text":"<p>Kirin works with any storage backend through the fsspec library:</p> <p>Supported Backends:</p> <ul> <li>Local filesystem: <code>/path/to/data</code></li> <li>AWS S3: <code>s3://bucket/path</code></li> <li>Google Cloud Storage: <code>gs://bucket/path</code></li> <li>Azure Blob Storage: <code>az://container/path</code></li> <li>And many more: Dropbox, Google Drive, etc. (sync/auth handled by backend)</li> </ul> <p>Benefits:</p> <ul> <li>Flexibility: Use any storage backend</li> <li>Scalability: Scale from local to cloud</li> <li>Portability: Move between backends easily</li> <li>Cost optimization: Choose the right storage for your needs</li> </ul>"},{"location":"getting-started/core-concepts/#zero-copy-operations","title":"Zero-Copy Operations","text":"<p>Kirin is designed with zero-copy philosophy for efficient large file handling:</p> <p>Zero-Copy Features:</p> <ul> <li>Memory-mapped files: Avoid loading entire files into memory</li> <li>Chunked processing: Process data incrementally using libraries like pandas</li> <li>Direct transfers: Stream between storage backends</li> <li>Reference-based operations: Use references instead of copying</li> </ul> <p>Benefits:</p> <ul> <li>Memory efficient: Handle files larger than RAM</li> <li>Fast operations: No unnecessary data copying</li> <li>Scalable: Work with datasets of any size</li> </ul>"},{"location":"getting-started/core-concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart - Try Kirin with a simple example</li> <li>Basic Usage Guide - Learn common workflows</li> <li>Working with Files - File operations and   patterns</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Choose the installation method that best fits your use case.</p>"},{"location":"getting-started/installation/#option-1-pixi-recommended-for-development","title":"Option 1: Pixi (Recommended for Development)","text":"<p>Best for contributors and developers who want the full development environment.</p> <pre><code># Clone and install\ngit clone git@github.com:nll-ai/kirin\ncd kirin\npixi install\n\n# Set up SSL certificates for cloud storage (one-time setup)\npixi run setup-ssl\n\n# Start the web UI\npixi run kirin ui\n</code></pre> <p>Benefits:</p> <ul> <li>Full development environment with all dependencies</li> <li>Easy to contribute to the project</li> <li>Includes testing and development tools</li> </ul>"},{"location":"getting-started/installation/#option-2-uv-tool-recommended-for-production","title":"Option 2: UV Tool (Recommended for Production)","text":"<p>Best for users who want a clean, isolated installation.</p> <pre><code># Install with uv\nuv tool install kirin\n\n# Set up SSL certificates (one-time setup)\nuv run python -m kirin.setup_ssl\n\n# Start the web UI\nuv run kirin ui\n</code></pre> <p>Benefits:</p> <ul> <li>Clean, isolated installation</li> <li>Easy to update and manage</li> <li>No system Python conflicts</li> </ul>"},{"location":"getting-started/installation/#option-3-uvx-one-time-use","title":"Option 3: UVX (One-time Use)","text":"<p>Best for trying out Kirin without permanent installation.</p> <pre><code># Run directly with uvx\nuvx kirin ui\n\n# If SSL issues occur, set up certificates\nuvx python -m kirin.setup_ssl\n</code></pre> <p>Benefits:</p> <ul> <li>No permanent installation</li> <li>Always uses latest version</li> <li>Good for experimentation</li> </ul>"},{"location":"getting-started/installation/#ssl-certificate-setup","title":"SSL Certificate Setup","text":"<p>When using isolated Python environments (pixi, uv, conda), SSL certificates are not automatically available. This affects HTTPS connections to cloud storage providers.</p>"},{"location":"getting-started/installation/#automatic-setup-recommended","title":"Automatic Setup (Recommended)","text":"<pre><code># Works with any Python environment - detects automatically\npython -m kirin.setup_ssl\n</code></pre>"},{"location":"getting-started/installation/#manual-setup-if-automatic-setup-fails","title":"Manual Setup (if automatic setup fails)","text":"<pre><code># The automatic setup script handles this automatically\n# But if you need to do it manually, use the Python executable path:\npython -c \"import sys; print('Python path:', sys.executable)\"\n# Then create ssl directory next to that path and copy certificates\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code># Check if Kirin is installed\npython -c \"import kirin; print('Kirin version:', kirin.__version__)\"\n\n# Test HTTPS connection\npython -c \"import requests; r = requests.get('https://storage.googleapis.com'); \\\nprint('HTTPS works:', r.status_code)\"\n\n# Or use the automatic setup\npython -m kirin.setup_ssl\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#ssl-certificate-issues","title":"SSL Certificate Issues","text":"<p>If you get SSL errors when connecting to cloud storage:</p> <ol> <li>Run the SSL setup: <code>python -m kirin.setup_ssl</code></li> <li>Check your Python environment: Make sure you're using the right Python</li> <li>Verify cloud credentials: Ensure your cloud authentication is set up correctly</li> </ol>"},{"location":"getting-started/installation/#import-errors","title":"Import Errors","text":"<p>If you get import errors:</p> <ol> <li>Check your Python environment: Make sure you're using the right Python</li> <li>Verify installation: Run <code>python -c \"import kirin\"</code></li> <li>Check dependencies: Ensure all required packages are installed</li> </ol>"},{"location":"getting-started/installation/#cloud-authentication-issues","title":"Cloud Authentication Issues","text":"<p>If you have trouble with cloud storage:</p> <ol> <li>See the Cloud Storage Guide for detailed setup</li> <li>Check your credentials: Verify AWS profiles, GCS tokens, etc.</li> <li>Test connectivity: Try a simple cloud operation first</li> </ol>"},{"location":"getting-started/installation/#development-setup","title":"Development Setup","text":"<p>If you want to contribute to Kirin:</p> <pre><code># Clone the repository\ngit clone git@github.com:nll-ai/kirin\ncd kirin\n\n# Install with pixi\npixi install\n\n# Set up SSL certificates\npixi run setup-ssl\n\n# Run tests\npixi run -e tests pytest\n\n# Start development server\npixi run kirin ui\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Core Concepts - Understanding how Kirin works</li> <li>Quickstart - Get started with your first dataset</li> <li>Cloud Storage Guide - Set up cloud storage</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>Get up and running with Kirin in 5 minutes!</p>"},{"location":"getting-started/quickstart/#what-is-kirin","title":"What is Kirin?","text":"<p>Kirin is simplified \"git\" for data - it provides linear versioning for datasets with content-addressed storage. Think of it as Git, but designed specifically for data scientists working with large datasets.</p>"},{"location":"getting-started/quickstart/#5-minute-quickstart","title":"5-Minute Quickstart","text":""},{"location":"getting-started/quickstart/#1-install-kirin","title":"1. Install Kirin","text":"<pre><code># Option 1: Using pixi (recommended for development)\ngit clone git@github.com:nll-ai/kirin\ncd kirin\npixi install\n\n# Option 2: Using uv tool (recommended for production)\nuv tool install kirin\n\n# Option 3: Using pip\npip install kirin\n</code></pre>"},{"location":"getting-started/quickstart/#2-create-your-first-dataset","title":"2. Create Your First Dataset","text":"<pre><code>from kirin import Catalog, Dataset\nfrom pathlib import Path\n\n# Create a catalog (works with local and cloud storage)\ncatalog = Catalog(root_dir=\"/path/to/data\")  # Local storage\n\n# Get or create a dataset\nds = catalog.get_dataset(\"my_first_dataset\")\n\n# Add some files to your dataset\ncommit_hash = ds.commit(\n    message=\"Initial commit\",\n    add_files=[\"data.csv\", \"config.json\"]\n)\n\nprint(f\"Created commit: {commit_hash}\")\n</code></pre>"},{"location":"getting-started/quickstart/#3-work-with-your-data","title":"3. Work with Your Data","text":"<pre><code># Checkout the latest commit\nds.checkout()\n\n# Access files from current commit\nfiles = ds.files\nprint(f\"Files in current commit: {list(files.keys())}\")\n\n# Work with files locally (recommended approach)\nwith ds.local_files() as local_files:\n    # Access files as local paths\n    csv_path = local_files[\"data.csv\"]\n\n    # Read file content\n    content = Path(csv_path).read_text()\n    print(f\"File content: {content[:100]}...\")\n</code></pre> <p>Tip: When you display a dataset, commit, or catalog in a notebook cell (e.g., <code>ds</code>), Kirin shows an interactive HTML view. Click \"Copy Code to Access\" on any file to copy code snippets to your clipboard. You can customize the variable name used in snippets by setting <code>dataset._repr_variable_name = \"my_name\"</code>.</p> <p>Note: The \"Copy Code to Access\" button requires browser clipboard access. It works in Jupyter/Marimo notebooks viewed in a web browser, but may not work in VSCode's embedded notebook viewer (as of December 2025).</p>"},{"location":"getting-started/quickstart/#4-view-your-commit-history","title":"4. View Your Commit History","text":"<pre><code># Get commit history\nhistory = ds.history(limit=10)\nfor commit in history:\n    print(f\"{commit.short_hash}: {commit.message}\")\n\n# Checkout a specific commit\nds.checkout(commit_hash)\n</code></pre>"},{"location":"getting-started/quickstart/#5-start-the-web-ui-optional","title":"5. Start the Web UI (Optional)","text":"<pre><code># Development (with pixi)\npixi run kirin ui\n\n# Production (with uv)\nuv run kirin ui\n\n# One-time use (with uvx)\nuvx kirin ui\n</code></pre> <p>The web UI provides a graphical interface for browsing datasets, viewing commit history, and managing your data catalogs.</p>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Detailed installation options</li> <li>Core Concepts - Understanding datasets, commits, and content-addressing</li> <li>Basic Usage Guide - Common workflows and patterns</li> <li>Cloud Storage Guide - Working with S3, GCS, Azure</li> </ul>"},{"location":"getting-started/quickstart/#key-benefits","title":"Key Benefits","text":"<ul> <li>Linear versioning: Simple, Git-like commits without branching complexity</li> <li>Content-addressed storage: Files stored by content hash for integrity and deduplication</li> <li>Cloud support: Works with S3, GCS, Azure, and more</li> <li>Ergonomic API: Designed for data science workflows</li> <li>Zero-copy operations: Efficient handling of large files</li> </ul>"},{"location":"getting-started/quickstart/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Experiment tracking: Version your training data and model inputs</li> <li>Data pipeline versioning: Track changes in ETL processes</li> <li>Collaborative research: Share datasets with exact version control</li> <li>Reproducible analysis: Ensure you can recreate your results</li> </ul>"},{"location":"guides/basic-usage/","title":"Basic Usage","text":"<p>Learn the essential workflows for working with Kirin datasets.</p>"},{"location":"guides/basic-usage/#creating-and-managing-datasets","title":"Creating and Managing Datasets","text":""},{"location":"guides/basic-usage/#create-a-new-dataset","title":"Create a New Dataset","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Create a catalog\ncatalog = Catalog(root_dir=\"/path/to/data\")\n\n# Create a new dataset\ndataset = catalog.create_dataset(\n    name=\"my_experiment\",\n    description=\"ML experiment data for Q1 2024\"\n)\n\nprint(f\"Created dataset: {dataset.name}\")\n</code></pre>"},{"location":"guides/basic-usage/#get-an-existing-dataset","title":"Get an Existing Dataset","text":"<pre><code># Get an existing dataset\ndataset = catalog.get_dataset(\"my_experiment\")\n\n# Check if dataset exists\nif dataset:\n    print(f\"Dataset has {len(dataset.history())} commits\")\nelse:\n    print(\"Dataset not found\")\n</code></pre>"},{"location":"guides/basic-usage/#adding-files-to-a-dataset","title":"Adding Files to a Dataset","text":"<pre><code># Add single file\ncommit_hash = dataset.commit(\n    message=\"Add initial data\",\n    add_files=[\"data.csv\"]\n)\n\n# Add multiple files\ncommit_hash = dataset.commit(\n    message=\"Add processed data\",\n    add_files=[\"data.csv\", \"config.json\", \"results.txt\"]\n)\n\nprint(f\"Created commit: {commit_hash}\")\n</code></pre>"},{"location":"guides/basic-usage/#removing-files-from-a-dataset","title":"Removing Files from a Dataset","text":"<pre><code># Remove single file\ndataset.commit(\n    message=\"Remove old data\",\n    remove_files=[\"old_data.csv\"]\n)\n\n# Remove multiple files\ndataset.commit(\n    message=\"Clean up dataset\",\n    remove_files=[\"temp1.csv\", \"temp2.json\"]\n)\n</code></pre>"},{"location":"guides/basic-usage/#combined-operations","title":"Combined Operations","text":"<pre><code># Add and remove files in same commit\ndataset.commit(\n    message=\"Update dataset\",\n    add_files=[\"new_data.csv\"],\n    remove_files=[\"old_data.csv\"]\n)\n</code></pre>"},{"location":"guides/basic-usage/#working-with-commits","title":"Working with Commits","text":""},{"location":"guides/basic-usage/#viewing-commit-history","title":"Viewing Commit History","text":"<pre><code># Get all commits\nhistory = dataset.history()\nfor commit in history:\n    print(f\"{commit.short_hash}: {commit.message}\")\n    print(f\"  Files: {commit.list_files()}\")\n    print(f\"  Date: {commit.timestamp}\")\n\n# Get limited history\nrecent_commits = dataset.history(limit=5)\n</code></pre>"},{"location":"guides/basic-usage/#checking-out-commits","title":"Checking Out Commits","text":"<pre><code># Checkout latest commit (default)\ndataset.checkout()\n\n# Checkout specific commit\ndataset.checkout(commit_hash)\n\n# Get current commit info\ncurrent_commit = dataset.current_commit\nif current_commit:\n    print(f\"Current commit: {current_commit.short_hash}\")\n    print(f\"Message: {current_commit.message}\")\n</code></pre>"},{"location":"guides/basic-usage/#comparing-commits","title":"Comparing Commits","text":"<pre><code># Get two commits\ncommit1 = dataset.get_commit(hash1)\ncommit2 = dataset.get_commit(hash2)\n\nif commit1 and commit2:\n    # Compare file lists\n    files1 = set(commit1.list_files())\n    files2 = set(commit2.list_files())\n\n    added = files2 - files1\n    removed = files1 - files2\n    common = files1 &amp; files2\n\n    print(f\"Added files: {added}\")\n    print(f\"Removed files: {removed}\")\n    print(f\"Common files: {common}\")\n</code></pre>"},{"location":"guides/basic-usage/#working-with-files","title":"Working with Files","text":""},{"location":"guides/basic-usage/#file-information","title":"File Information","text":"<pre><code># List files in current commit\nfiles = dataset.files\nprint(f\"Files: {list(files.keys())}\")\n\n# Check if file exists\nif dataset.has_file(\"data.csv\"):\n    print(\"File exists\")\n\n# Get file object\nfile_obj = dataset.get_file(\"data.csv\")\nif file_obj:\n    print(f\"File size: {file_obj.size} bytes\")\n    print(f\"Content hash: {file_obj.hash}\")\n</code></pre>"},{"location":"guides/basic-usage/#local-file-access-recommended-pattern","title":"Local File Access (Recommended Pattern)","text":"<p>The <code>local_files()</code> context manager is the recommended way to work with files:</p> <pre><code>from pathlib import Path\nimport pandas as pd\n\n# Work with files locally\nwith dataset.local_files() as local_files:\n    for filename, local_path in local_files.items():\n        print(f\"{filename} -&gt; {local_path}\")\n\n        # Process files with standard Python libraries\n        if filename.endswith('.csv'):\n            df = pd.read_csv(local_path)\n            print(f\"CSV shape: {df.shape}\")\n        elif filename.endswith('.json'):\n            import json\n            data = json.loads(Path(local_path).read_text())\n            print(f\"JSON keys: {list(data.keys())}\")\n</code></pre> <p>Benefits of local_files():</p> <ul> <li>Library compatibility: Works with pandas, polars, etc.</li> <li>Automatic cleanup: Files cleaned up when done</li> <li>Standard paths: Use normal Python file operations</li> <li>Memory efficient: No need to load entire files into memory</li> </ul>"},{"location":"guides/basic-usage/#dataset-information","title":"Dataset Information","text":""},{"location":"guides/basic-usage/#get-dataset-info","title":"Get Dataset Info","text":"<pre><code># Check if dataset is empty\nif dataset.is_empty():\n    print(\"Dataset has no commits\")\nelse:\n    print(f\"Dataset has {len(dataset.history())} commits\")\n\n# Get dataset information\ninfo = dataset.get_info()\nprint(f\"Dataset info: {info}\")\n\n# Convert to dictionary\ndataset_dict = dataset.to_dict()\nprint(f\"Serialized dataset: {dataset_dict}\")\n</code></pre>"},{"location":"guides/basic-usage/#cleanup-operations","title":"Cleanup Operations","text":"<pre><code># Clean up orphaned files (files not referenced by any commit)\nremoved_count = dataset.cleanup_orphaned_files()\nprint(f\"Removed {removed_count} orphaned files\")\n</code></pre>"},{"location":"guides/basic-usage/#common-workflows","title":"Common Workflows","text":""},{"location":"guides/basic-usage/#experiment-tracking","title":"Experiment Tracking","text":"<pre><code># Track ML experiment\ndataset = catalog.get_dataset(\"ml_experiment\")\n\n# Add training data\ndataset.commit(\n    message=\"Add training data\",\n    add_files=[\"train.csv\", \"train_labels.csv\"]\n)\n\n# Add model\ndataset.commit(\n    message=\"Add trained model\",\n    add_files=[\"model.pkl\", \"model_metrics.json\"]\n)\n\n# Add results\ndataset.commit(\n    message=\"Add experiment results\",\n    add_files=[\"results.csv\", \"plots.png\"]\n)\n</code></pre>"},{"location":"guides/basic-usage/#data-pipeline-versioning","title":"Data Pipeline Versioning","text":"<pre><code># Track ETL pipeline\ndataset = catalog.get_dataset(\"etl_pipeline\")\n\n# Raw data\ndataset.commit(\n    message=\"Add raw data\",\n    add_files=[\"raw_data.csv\"]\n)\n\n# Processed data\ndataset.commit(\n    message=\"Add processed data\",\n    add_files=[\"processed_data.csv\", \"processing_log.txt\"]\n)\n\n# Final output\ndataset.commit(\n    message=\"Add final output\",\n    add_files=[\"final_data.csv\", \"summary.json\"]\n)\n</code></pre>"},{"location":"guides/basic-usage/#collaborative-research","title":"Collaborative Research","text":"<pre><code># Share dataset with team\ndataset = catalog.get_dataset(\"research_data\")\n\n# Add initial data\ndataset.commit(\n    message=\"Initial dataset\",\n    add_files=[\"raw_data.csv\", \"metadata.json\"]\n)\n\n# Team member adds analysis\ndataset.commit(\n    message=\"Add analysis results\",\n    add_files=[\"analysis.py\", \"results.csv\", \"plots.png\"]\n)\n</code></pre>"},{"location":"guides/basic-usage/#best-practices","title":"Best Practices","text":""},{"location":"guides/basic-usage/#commit-messages","title":"Commit Messages","text":"<p>Use descriptive commit messages:</p> <pre><code># Good commit messages\ndataset.commit(\"Add Q1 sales data\", add_files=[\"sales_q1.csv\"])\ndataset.commit(\"Fix data quality issues\", add_files=[\"sales_q1.csv\"])\ndataset.commit(\"Add customer segmentation\", add_files=[\"segments.csv\"])\n\n# Avoid vague messages\ndataset.commit(\"Update\", add_files=[\"data.csv\"])  # Too vague\ndataset.commit(\"Fix\", add_files=[\"data.csv\"])     # Too vague\n</code></pre>"},{"location":"guides/basic-usage/#file-organization","title":"File Organization","text":"<p>Organize files logically:</p> <pre><code># Good organization\ndataset.commit(\"Add raw data\", add_files=[\"raw/sales.csv\", \"raw/customers.csv\"])\ndataset.commit(\"Add processed data\", add_files=[\"processed/sales_clean.csv\"])\ndataset.commit(\"Add analysis\", add_files=[\"analysis/results.csv\", \"analysis/plots.png\"])\n\n# Avoid flat structure\ndataset.commit(\"Add files\", add_files=[\"sales.csv\", \"customers.csv\",\n                                        \"results.csv\", \"plots.png\"])\n</code></pre>"},{"location":"guides/basic-usage/#regular-commits","title":"Regular Commits","text":"<p>Commit changes regularly:</p> <pre><code># Commit after each logical step\ndataset.commit(\"Add initial data\", add_files=[\"data.csv\"])\n# ... process data ...\ndataset.commit(\"Add processed data\", add_files=[\"processed.csv\"])\n# ... analyze data ...\ndataset.commit(\"Add analysis\", add_files=[\"results.csv\"])\n</code></pre>"},{"location":"guides/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Working with Files - Advanced file operations</li> <li>Commit Management - Understanding commit history</li> <li>Cloud Storage - Working with cloud backends</li> </ul>"},{"location":"guides/cloud-storage/","title":"Cloud Storage","text":"<p>Set up and use Kirin with cloud storage backends like S3, GCS, and Azure.</p>"},{"location":"guides/cloud-storage/#overview","title":"Overview","text":"<p>Kirin supports multiple cloud storage backends through the fsspec library. You can use the same API whether you're working with local files or cloud storage.</p> <p>Supported Backends:</p> <ul> <li>AWS S3: <code>s3://bucket/path</code></li> <li>Google Cloud Storage: <code>gs://bucket/path</code></li> <li>Azure Blob Storage: <code>az://container/path</code></li> <li>And many more: Dropbox, Google Drive, etc.</li> </ul>"},{"location":"guides/cloud-storage/#authentication-methods","title":"Authentication Methods","text":""},{"location":"guides/cloud-storage/#awss3-authentication","title":"AWS/S3 Authentication","text":""},{"location":"guides/cloud-storage/#using-aws-profile-recommended","title":"Using AWS Profile (Recommended)","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Using AWS profile\ncatalog = Catalog(\n    root_dir=\"s3://{{ bucket_name }}/data\",\n    aws_profile=\"{{ aws_profile }}\"\n)\n\n# Using Dataset with AWS profile\ndataset = Dataset(\n    root_dir=\"s3://my-bucket/data\",\n    name=\"{{ dataset_name }}\",\n    aws_profile=\"my-profile\"\n)\n</code></pre>"},{"location":"guides/cloud-storage/#using-environment-variables","title":"Using Environment Variables","text":"<p>Set environment variables in your shell or system:</p> <pre><code># Set AWS credentials\nexport AWS_ACCESS_KEY_ID={{ access_key_id }}\nexport AWS_SECRET_ACCESS_KEY={{ secret_access_key }}\nexport AWS_DEFAULT_REGION={{ region }}\n\n# Set Azure credentials\nexport AZURE_CONNECTION_STRING={{ azure_connection_string }}\n</code></pre> <p>Then use without explicit credentials:</p> <pre><code># Environment variables are automatically detected\ncatalog = Catalog(root_dir=\"s3://{{ bucket_name }}/data\")\n</code></pre>"},{"location":"guides/cloud-storage/#using-iam-roles-ec2ecslambda","title":"Using IAM Roles (EC2/ECS/Lambda)","text":"<pre><code># No explicit credentials needed - uses IAM role automatically\ncatalog = Catalog(root_dir=\"s3://my-bucket/data\")\n</code></pre>"},{"location":"guides/cloud-storage/#using-aws-sso","title":"Using AWS SSO","text":"<pre><code># After running: aws sso login\ncatalog = Catalog(\n    root_dir=\"s3://{{ bucket_name }}/data\",\n    aws_profile=\"{{ sso_profile_name }}\"\n)\n</code></pre>"},{"location":"guides/cloud-storage/#gcpgcs-authentication","title":"GCP/GCS Authentication","text":""},{"location":"guides/cloud-storage/#using-service-account-key-file","title":"Using Service Account Key File","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Using service account key file\ncatalog = Catalog(\n    root_dir=\"gs://{{ bucket_name }}/data\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"{{ project_id }}\"\n)\n\n# Using Dataset with GCS credentials\ndataset = Dataset(\n    root_dir=\"gs://{{ bucket_name }}/data\",\n    name=\"{{ dataset_name }}\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"{{ project_id }}\"\n)\n</code></pre>"},{"location":"guides/cloud-storage/#using-application-default-credentials","title":"Using Application Default Credentials","text":"<pre><code># Set up ADC (one-time setup)\n# gcloud auth application-default login\n\n# Use without explicit credentials (automatically detects ADC)\ncatalog = Catalog(root_dir=\"gs://my-bucket/data\")\n</code></pre>"},{"location":"guides/cloud-storage/#using-environment-variables-gcs","title":"Using Environment Variables (GCS)","text":"<pre><code>import os\n\n# Set environment variable\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/path/to/service-account.json\"\n\n# Use without explicit credentials (automatically detects environment)\ncatalog = Catalog(root_dir=\"gs://my-bucket/data\")\n</code></pre>"},{"location":"guides/cloud-storage/#using-workload-identity-gkekubernetes","title":"Using Workload Identity (GKE/Kubernetes)","text":"<pre><code># No explicit credentials needed - uses workload identity\ncatalog = Catalog(root_dir=\"gs://my-bucket/data\")\n</code></pre>"},{"location":"guides/cloud-storage/#azure-blob-storage-authentication","title":"Azure Blob Storage Authentication","text":""},{"location":"guides/cloud-storage/#using-connection-string","title":"Using Connection String","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Using connection string\ncatalog = Catalog(\n    root_dir=\"az://{{ container_name }}/data\",\n    azure_connection_string=os.getenv(\"AZURE_CONNECTION_STRING\")\n)\n\n# Using Dataset with connection string\ndataset = Dataset(\n    root_dir=\"az://{{ container_name }}/data\",\n    name=\"{{ dataset_name }}\",\n    azure_connection_string=os.getenv(\"AZURE_CONNECTION_STRING\")\n)\n</code></pre>"},{"location":"guides/cloud-storage/#using-account-name-and-key","title":"Using Account Name and Key","text":"<pre><code>catalog = Catalog(\n    root_dir=\"az://{{ container_name }}/data\",\n    azure_account_name=\"myaccount\",\n    azure_account_key=\"mykey\"\n)\n</code></pre>"},{"location":"guides/cloud-storage/#using-environment-variables-azure","title":"Using Environment Variables (Azure)","text":"<pre><code>import os\n\n# Set environment variables\nos.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"] = \"myaccount\"\nos.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"] = \"mykey\"\n\n# Use without explicit credentials (automatically detects environment)\ncatalog = Catalog(root_dir=\"az://my-container/data\")\n</code></pre>"},{"location":"guides/cloud-storage/#using-azure-cli-authentication","title":"Using Azure CLI Authentication","text":"<pre><code># After running: az login\n# No explicit credentials needed - uses Azure CLI authentication\ncatalog = Catalog(root_dir=\"az://my-container/data\")\n</code></pre>"},{"location":"guides/cloud-storage/#using-managed-identity-azure-vmsapp-service","title":"Using Managed Identity (Azure VMs/App Service)","text":"<pre><code># No explicit credentials needed - uses managed identity\ncatalog = Catalog(root_dir=\"az://my-container/data\")\n</code></pre>"},{"location":"guides/cloud-storage/#working-with-cloud-storage","title":"Working with Cloud Storage","text":""},{"location":"guides/cloud-storage/#basic-operations","title":"Basic Operations","text":"<pre><code># Create catalog with cloud storage\ncatalog = Catalog(root_dir=\"s3://my-bucket/data\")\n\n# Create dataset\ndataset = catalog.create_dataset(\"cloud_dataset\")\n\n# Add files (same API as local storage)\ncommit_hash = dataset.commit(\n    message=\"Add data to cloud\",\n    add_files=[\"data.csv\", \"config.json\"]\n)\n\n# Work with files (same API as local storage)\nwith dataset.local_files() as local_files:\n    for filename, local_path in local_files.items():\n        print(f\"{filename} -&gt; {local_path}\")\n        # Process files normally\n</code></pre>"},{"location":"guides/cloud-storage/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/cloud-storage/#processing-large-files","title":"Processing Large Files","text":"<pre><code># For large files, use chunked processing\nwith dataset.local_files() as local_files:\n    if \"large_data.csv\" in local_files:\n        local_path = local_files[\"large_data.csv\"]\n        # Use pandas chunking for large files\n        for chunk in pd.read_csv(local_path, chunksize=10000):\n            print(f\"Processing chunk with {len(chunk)} rows\")\n            process_chunk(chunk)\n</code></pre>"},{"location":"guides/cloud-storage/#batch-operations","title":"Batch Operations","text":"<pre><code># Batch multiple operations for better performance\nfiles_to_add = [\"file1.csv\", \"file2.csv\", \"file3.csv\"]\ndataset.commit(\n    message=\"Add multiple files\",\n    add_files=files_to_add\n)\n</code></pre>"},{"location":"guides/cloud-storage/#error-handling","title":"Error Handling","text":"<pre><code>import boto3\nfrom botocore.exceptions import ClientError\n\ntry:\n    catalog = Catalog(root_dir=\"s3://my-bucket/data\")\n    dataset = catalog.get_dataset(\"my-dataset\")\nexcept ClientError as e:\n    if e.response['Error']['Code'] == 'NoSuchBucket':\n        print(\"Bucket does not exist\")\n    elif e.response['Error']['Code'] == 'AccessDenied':\n        print(\"Access denied - check your credentials\")\n    else:\n        print(f\"AWS error: {e}\")\nexcept Exception as e:\n    print(f\"General error: {e}\")\n</code></pre>"},{"location":"guides/cloud-storage/#web-ui-cloud-integration","title":"Web UI Cloud Integration","text":""},{"location":"guides/cloud-storage/#setting-up-cloud-catalogs","title":"Setting Up Cloud Catalogs","text":"<p>The web UI supports cloud storage through a simple interface:</p> <ol> <li>Authenticate with your cloud provider using their CLI tools:</li> </ol> <pre><code># AWS\naws configure\n\n# GCP\ngcloud auth login\n\n# Azure\naz login\n</code></pre> <ol> <li>Create catalog in web UI:</li> <li>Click \"Add Catalog\" in the web interface</li> <li>Enter catalog details (ID, name, root directory)</li> <li>For S3: Select AWS profile from dropdown</li> <li> <p>For GCS/Azure: Ensure credentials are configured via environment variables      or CLI</p> </li> <li> <p>Authentication handling:</p> </li> <li>S3: Web UI provides profile selection</li> <li>GCS/Azure: Requires pre-configured credentials (environment variables,      CLI auth, etc.)</li> </ol>"},{"location":"guides/cloud-storage/#cloud-authentication-in-web-ui","title":"Cloud Authentication in Web UI","text":"<ol> <li>Create catalog with cloud URL: Use <code>s3://</code>, <code>gs://</code>, or <code>az://</code> URLs</li> <li>AWS Profile Selection: Web UI provides AWS profile dropdown for S3    authentication</li> <li>Other Cloud Providers: For GCS and Azure, authentication must be    configured programmatically or via environment variables</li> <li>Credentials stored securely: AWS profiles saved in catalog configuration</li> <li>Automatic authentication: Subsequent uses authenticate automatically</li> </ol>"},{"location":"guides/cloud-storage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/cloud-storage/#common-issues","title":"Common Issues","text":""},{"location":"guides/cloud-storage/#ssl-certificate-errors","title":"SSL Certificate Errors","text":"<pre><code># Set up SSL certificates for isolated Python environments\npython -m kirin.setup_ssl\n</code></pre>"},{"location":"guides/cloud-storage/#authentication-failures","title":"Authentication Failures","text":"<pre><code># Check your credentials\nimport boto3\n\n# Test AWS credentials\nsession = boto3.Session(profile_name=\"my-profile\")\ns3 = session.client('s3')\ns3.list_buckets()  # Should work without errors\n</code></pre>"},{"location":"guides/cloud-storage/#permission-issues","title":"Permission Issues","text":"<pre><code># Check bucket permissions\nimport boto3\n\ns3 = boto3.client('s3')\ntry:\n    s3.head_bucket(Bucket='my-bucket')\n    print(\"Bucket accessible\")\nexcept ClientError as e:\n    print(f\"Bucket not accessible: {e}\")\n</code></pre>"},{"location":"guides/cloud-storage/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/cloud-storage/#use-appropriate-regions","title":"Use Appropriate Regions","text":"<pre><code># Use same region as your compute resources\ncatalog = Catalog(\n    root_dir=\"s3://{{ bucket_name }}/data\",\n    aws_profile=\"{{ aws_profile }}\"\n)\n# Ensure bucket is in same region as your compute\n</code></pre>"},{"location":"guides/cloud-storage/#optimize-file-sizes","title":"Optimize File Sizes","text":"<pre><code># For very large files, consider chunking\n# Split large files into smaller chunks\ndataset.commit(\n    message=\"Add chunked data\",\n    add_files=[\"chunk_001.csv\", \"chunk_002.csv\", \"chunk_003.csv\"]\n)\n</code></pre>"},{"location":"guides/cloud-storage/#use-compression","title":"Use Compression","text":"<pre><code># Compress files before adding to reduce storage costs\nimport gzip\nimport shutil\n\n# Compress file\nwith open(\"data.csv\", \"rb\") as f_in:\n    with gzip.open(\"data.csv.gz\", \"wb\") as f_out:\n        shutil.copyfileobj(f_in, f_out)\n\n# Add compressed file\ndataset.commit(\n    message=\"Add compressed data\",\n    add_files=[\"data.csv.gz\"]\n)\n</code></pre>"},{"location":"guides/cloud-storage/#best-practices","title":"Best Practices","text":""},{"location":"guides/cloud-storage/#security","title":"Security","text":"<ul> <li>Use IAM roles when possible instead of access keys</li> <li>Rotate credentials regularly</li> <li>Use least privilege - only grant necessary permissions</li> <li>Monitor access through cloud provider audit logs</li> </ul>"},{"location":"guides/cloud-storage/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Use appropriate storage classes (S3 Standard, IA, Glacier)</li> <li>Enable lifecycle policies for automatic archival</li> <li>Monitor usage through cloud provider dashboards</li> <li>Use compression for text files</li> </ul>"},{"location":"guides/cloud-storage/#performance","title":"Performance","text":"<ul> <li>Use same region as your compute resources</li> <li>Batch operations when possible</li> <li>Use chunked processing for large files</li> <li>Consider CDN for frequently accessed data</li> </ul>"},{"location":"guides/cloud-storage/#next-steps","title":"Next Steps","text":"<ul> <li>Working with Files - Advanced file operations</li> <li>Web UI Overview - Using the web interface</li> <li>Basic Usage - Core dataset operations</li> </ul>"},{"location":"guides/commit-management/","title":"Commit Management","text":"<p>Understanding and working with Kirin's linear commit history.</p>"},{"location":"guides/commit-management/#understanding-commits","title":"Understanding Commits","text":""},{"location":"guides/commit-management/#what-is-a-commit","title":"What is a Commit?","text":"<p>A commit in Kirin represents an immutable snapshot of files at a point in time. Unlike Git, Kirin uses a linear commit history - each commit has exactly one parent commit, creating a simple chain of changes.</p> <pre><code>Commit A \u2192 Commit B \u2192 Commit C \u2192 Commit D\n</code></pre>"},{"location":"guides/commit-management/#commit-properties","title":"Commit Properties","text":"<p>Every commit has these key properties:</p> <ul> <li>Hash: Unique identifier (SHA256)</li> <li>Message: Human-readable description</li> <li>Timestamp: When the commit was created</li> <li>Parent: Reference to previous commit (linear history)</li> <li>Files: Dictionary of files in this commit</li> </ul> <pre><code># Get commit information\ncommit = dataset.get_commit(commit_hash)\nif commit:\n    print(f\"Commit hash: {commit.hash}\")\n    print(f\"Short hash: {commit.short_hash}\")\n    print(f\"Message: {commit.message}\")\n    print(f\"Timestamp: {commit.timestamp}\")\n    print(f\"Parent: {commit.parent_hash}\")\n    print(f\"Files: {commit.list_files()}\")\n    print(f\"Total size: {commit.get_total_size()} bytes\")\n</code></pre>"},{"location":"guides/commit-management/#creating-commits","title":"Creating Commits","text":""},{"location":"guides/commit-management/#commit-method-parameters","title":"Commit Method Parameters","text":"<p>The <code>commit()</code> method accepts the following parameters:</p> <ul> <li>message (required): Human-readable description of the changes</li> <li>add_files (optional): List of file paths to add or update</li> <li>remove_files (optional): List of filenames to remove from the dataset</li> </ul> <pre><code># Add new files\ndataset.commit(message=\"Add new data\", add_files=[\"data.csv\", \"metadata.json\"])\n\n# Remove files\ndataset.commit(message=\"Remove old files\", remove_files=[\"old_data.csv\"])\n\n# Add and remove files in the same commit\ndataset.commit(\n    message=\"Update dataset\",\n    add_files=[\"new_data.csv\"],\n    remove_files=[\"old_data.csv\"]\n)\n</code></pre>"},{"location":"guides/commit-management/#error-handling","title":"Error Handling","text":"<p>The <code>commit()</code> method can raise the following exceptions:</p> <ul> <li>ValueError: If no changes are specified (both <code>add_files</code> and   <code>remove_files</code> are empty)</li> <li>FileNotFoundError: If a file in <code>add_files</code> doesn't exist</li> </ul> <pre><code>try:\n    # This will raise ValueError\n    dataset.commit(message=\"No changes\")\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n\ntry:\n    # This will raise FileNotFoundError if file doesn't exist\n    dataset.commit(message=\"Add file\", add_files=[\"nonexistent.csv\"])\nexcept FileNotFoundError as e:\n    print(f\"File not found: {e}\")\n</code></pre>"},{"location":"guides/commit-management/#working-with-commit-history","title":"Working with Commit History","text":""},{"location":"guides/commit-management/#viewing-history","title":"Viewing History","text":"<pre><code># Get all commits\nhistory = dataset.history()\nfor commit in history:\n    print(f\"{commit.short_hash}: {commit.message}\")\n    print(f\"  Date: {commit.timestamp}\")\n    print(f\"  Files: {commit.list_files()}\")\n    print()\n\n# Get limited history\nrecent_commits = dataset.history(limit=5)\nfor commit in recent_commits:\n    print(f\"{commit.short_hash}: {commit.message}\")\n</code></pre>"},{"location":"guides/commit-management/#navigating-history","title":"Navigating History","text":"<pre><code># Checkout latest commit (default)\ndataset.checkout()\n\n# Checkout specific commit\ndataset.checkout(commit_hash)\n\n# Get current commit\ncurrent_commit = dataset.current_commit\nif current_commit:\n    print(f\"Current commit: {current_commit.short_hash}\")\n    print(f\"Message: {current_commit.message}\")\n</code></pre>"},{"location":"guides/commit-management/#comparing-commits","title":"Comparing Commits","text":"<pre><code>def compare_commits(dataset, commit1_hash, commit2_hash):\n    \"\"\"Compare two commits to see what changed.\"\"\"\n    commit1 = dataset.get_commit(commit1_hash)\n    commit2 = dataset.get_commit(commit2_hash)\n\n    if not commit1 or not commit2:\n        print(\"One or both commits not found\")\n        return\n\n    files1 = set(commit1.list_files())\n    files2 = set(commit2.list_files())\n\n    added = files2 - files1\n    removed = files1 - files2\n    common = files1 &amp; files2\n\n    print(f\"Added files: {added}\")\n    print(f\"Removed files: {removed}\")\n    print(f\"Common files: {common}\")\n\n    # Check if common files changed\n    for filename in common:\n        file1 = commit1.get_file(filename)\n        file2 = commit2.get_file(filename)\n        if file1.hash != file2.hash:\n            print(f\"Changed: {filename}\")\n\n# Use the comparison function\ncompare_commits(dataset, \"abc123\", \"def456\")\n</code></pre>"},{"location":"guides/commit-management/#commit-workflows","title":"Commit Workflows","text":""},{"location":"guides/commit-management/#linear-development","title":"Linear Development","text":"<p>Kirin's linear history is perfect for data science workflows:</p> <pre><code># Initial data\ndataset.commit(message=\"Add raw data\", add_files=[\"raw_data.csv\"])\n\n# Data cleaning\ndataset.commit(message=\"Clean data\", add_files=[\"cleaned_data.csv\"])\n\n# Feature engineering\ndataset.commit(message=\"Add features\", add_files=[\"features.csv\"])\n\n# Model training\ndataset.commit(message=\"Add trained model\", add_files=[\"model.pkl\"])\n\n# Results\ndataset.commit(message=\"Add results\", add_files=[\"results.csv\", \"plots.png\"])\n</code></pre>"},{"location":"guides/commit-management/#experiment-tracking","title":"Experiment Tracking","text":"<p>Track different experiments as separate commits:</p> <pre><code># Experiment 1: Random Forest\ndataset.commit(message=\"RF experiment\", add_files=[\"rf_model.pkl\", \"rf_results.csv\"])\n\n# Experiment 2: Gradient Boosting\ndataset.commit(message=\"GB experiment\", add_files=[\"gb_model.pkl\", \"gb_results.csv\"])\n\n# Experiment 3: Neural Network\ndataset.commit(message=\"NN experiment\", add_files=[\"nn_model.pkl\", \"nn_results.csv\"])\n</code></pre>"},{"location":"guides/commit-management/#data-pipeline-versioning","title":"Data Pipeline Versioning","text":"<p>Version your data processing pipeline outputs:</p> <pre><code># Raw data ingestion\ndataset.commit(message=\"Ingest raw data\", add_files=[\"raw/sales.csv\", \"raw/customers.csv\"])\n\n# Data validation\ndataset.commit(message=\"Validate data\", add_files=[\"validated/sales.csv\", \"validated/customers.csv\"])\n\n# Data transformation\ndataset.commit(message=\"Transform data\", add_files=[\"transformed/sales_clean.csv\"])\n\n# Feature engineering\ndataset.commit(message=\"Create features\", add_files=[\"features/engineered_features.csv\"])\n\n# Final output\ndataset.commit(message=\"Final dataset\", add_files=[\"final/dataset.csv\"])\n</code></pre>"},{"location":"guides/commit-management/#advanced-commit-operations","title":"Advanced Commit Operations","text":""},{"location":"guides/commit-management/#commit-information","title":"Commit Information","text":"<pre><code>def analyze_commit(commit):\n    \"\"\"Analyze a commit for detailed information.\"\"\"\n    print(f\"Commit: {commit.short_hash}\")\n    print(f\"Message: {commit.message}\")\n    print(f\"Date: {commit.timestamp}\")\n    print(f\"Parent: {commit.parent_hash}\")\n    print(f\"Files: {len(commit.files)}\")\n    print(f\"Total size: {commit.get_total_size()} bytes\")\n\n    # File details\n    for filename, file_obj in commit.files.items():\n        print(f\"  {filename}: {file_obj.size} bytes ({file_obj.short_hash})\")\n\n# Analyze current commit\ncurrent_commit = dataset.current_commit\nif current_commit:\n    analyze_commit(current_commit)\n</code></pre>"},{"location":"guides/commit-management/#commit-statistics","title":"Commit Statistics","text":"<pre><code>def commit_statistics(dataset):\n    \"\"\"Get statistics about the commit history.\"\"\"\n    history = dataset.history()\n\n    if not history:\n        print(\"No commits found\")\n        return\n\n    total_commits = len(history)\n    total_size = sum(commit.get_total_size() for commit in history)\n    avg_size = total_size / total_commits\n\n    print(f\"Total commits: {total_commits}\")\n    print(f\"Total size: {total_size / (1024*1024):.1f} MB\")\n    print(f\"Average commit size: {avg_size / (1024*1024):.1f} MB\")\n\n    # File frequency\n    file_counts = {}\n    for commit in history:\n        for filename in commit.list_files():\n            file_counts[filename] = file_counts.get(filename, 0) + 1\n\n    print(f\"Most frequently changed files:\")\n    for filename, count in sorted(file_counts.items(), key=lambda x: x[1], reverse=True)[:5]:\n        print(f\"  {filename}: {count} commits\")\n\n# Get statistics\ncommit_statistics(dataset)\n</code></pre>"},{"location":"guides/commit-management/#commit-best-practices","title":"Commit Best Practices","text":""},{"location":"guides/commit-management/#commit-messages","title":"Commit Messages","text":"<p>Write clear, descriptive commit messages:</p> <pre><code># Good commit messages\ndataset.commit(message=\"Add Q1 2024 sales data\", add_files=[\"sales_q1_2024.csv\"])\ndataset.commit(message=\"Fix data quality issues in customer records\", add_files=[\"customers_cleaned.csv\"])\ndataset.commit(message=\"Add feature engineering for ML model\", add_files=[\"features.csv\"])\n\n# Avoid vague messages\ndataset.commit(message=\"Update\", add_files=[\"data.csv\"])\ndataset.commit(message=\"Fix\", add_files=[\"file.csv\"])\ndataset.commit(message=\"Add stuff\", add_files=[\"file1.csv\", \"file2.csv\"])\n</code></pre>"},{"location":"guides/commit-management/#commit-frequency","title":"Commit Frequency","text":"<p>Commit changes regularly:</p> <pre><code># Commit after each logical step\ndataset.commit(message=\"Add raw data\", add_files=[\"raw_data.csv\"])\n# ... process data ...\ndataset.commit(message=\"Add cleaned data\", add_files=[\"cleaned_data.csv\"])\n# ... analyze data ...\ndataset.commit(message=\"Add analysis results\", add_files=[\"results.csv\"])\n</code></pre>"},{"location":"guides/commit-management/#atomic-commits","title":"Atomic Commits","text":"<p>Make commits atomic (single logical change):</p> <pre><code># Good: Single logical change\ndataset.commit(message=\"Add customer data\", add_files=[\"customers.csv\"])\n\n# Good: Related changes together\ndataset.commit(message=\"Update customer data and add validation\",\n               add_files=[\"customers_updated.csv\", \"validation_rules.json\"])\n\n# Avoid: Unrelated changes\ndataset.commit(message=\"Add customer data and fix bug\",\n               add_files=[\"customers.csv\", \"bug_fix.py\"])\n</code></pre>"},{"location":"guides/commit-management/#working-with-specific-commits","title":"Working with Specific Commits","text":""},{"location":"guides/commit-management/#accessing-files-from-specific-commits","title":"Accessing Files from Specific Commits","text":"<pre><code>def get_files_from_commit(dataset, commit_hash):\n    \"\"\"Get files from a specific commit.\"\"\"\n    commit = dataset.get_commit(commit_hash)\n    if not commit:\n        print(\"Commit not found\")\n        return\n\n    # Checkout the commit\n    dataset.checkout(commit_hash)\n\n    # Access files\n    files = dataset.files\n    print(f\"Files in commit {commit_hash}:\")\n    for filename, file_obj in files.items():\n        print(f\"  {filename}: {file_obj.size} bytes\")\n\n    return files\n\n# Get files from specific commit\nfiles = get_files_from_commit(dataset, \"abc123\")\n</code></pre>"},{"location":"guides/commit-management/#commit-history-visualization","title":"Commit History Visualization","text":""},{"location":"guides/commit-management/#commit-timeline","title":"Commit Timeline","text":"<pre><code>def create_timeline(dataset):\n    \"\"\"Create a timeline of commits.\"\"\"\n    history = dataset.history()\n\n    print(\"Commit Timeline:\")\n    print(\"=\" * 30)\n\n    for commit in history:\n        date_str = commit.timestamp.strftime(\"%Y-%m-%d %H:%M\")\n        print(f\"{date_str} | {commit.short_hash} | {commit.message}\")\n\n# Create timeline\ncreate_timeline(dataset)\n</code></pre>"},{"location":"guides/commit-management/#troubleshooting-commits","title":"Troubleshooting Commits","text":""},{"location":"guides/commit-management/#finding-lost-commits","title":"Finding Lost Commits","text":"<pre><code>def find_commit_by_message(dataset, search_term):\n    \"\"\"Find commits by message content.\"\"\"\n    history = dataset.history()\n\n    for commit in history:\n        if search_term.lower() in commit.message.lower():\n            print(f\"Found: {commit.short_hash} - {commit.message}\")\n            return commit\n\n    print(f\"No commits found matching '{search_term}'\")\n    return None\n\n# Find commit\ncommit = find_commit_by_message(dataset, \"model\")\n</code></pre>"},{"location":"guides/commit-management/#recovering-from-mistakes","title":"Recovering from Mistakes","text":"<pre><code>def recover_from_mistake(dataset, good_commit_hash):\n    \"\"\"Recover from a mistake by checking out a good commit.\"\"\"\n    # Checkout the good commit\n    dataset.checkout(good_commit_hash)\n\n    # Verify we're on the right commit\n    current_commit = dataset.current_commit\n    if current_commit and current_commit.hash == good_commit_hash:\n        print(f\"Successfully recovered to commit {good_commit_hash}\")\n        print(f\"Current commit: {current_commit.message}\")\n    else:\n        print(\"Failed to recover to specified commit\")\n\n# Recover from mistake\nrecover_from_mistake(dataset, \"abc123\")\n</code></pre>"},{"location":"guides/commit-management/#next-steps","title":"Next Steps","text":"<ul> <li>Working with Files - File operations and patterns</li> <li>Basic Usage - Core dataset operations</li> <li>Cloud Storage - Working with cloud backends</li> </ul>"},{"location":"guides/model-versioning/","title":"Model Versioning with Kirin","text":"<p>Kirin provides powerful model versioning capabilities that make it easy to track, compare, and manage machine learning models throughout their lifecycle. This guide shows you how to use Kirin for model versioning workflows.</p>"},{"location":"guides/model-versioning/#overview","title":"Overview","text":"<p>Model versioning with Kirin enables you to:</p> <ul> <li>Track model artifacts alongside rich metadata (hyperparameters, metrics,   training info)</li> <li>Tag models for different stages (dev, staging, production) or versions</li> <li>Query and discover models by performance metrics, tags, or custom criteria</li> <li>Compare model versions to understand what changed between iterations</li> <li>Maintain linear history with simple, git-like semantics</li> <li>Store models anywhere - local filesystem, S3, GCS, Azure, etc.</li> </ul>"},{"location":"guides/model-versioning/#basic-model-versioning-workflow","title":"Basic Model Versioning Workflow","text":""},{"location":"guides/model-versioning/#1-initialize-a-model-registry","title":"1. Initialize a Model Registry","text":"<pre><code>from kirin import Dataset\n\n# Create a model registry (works with any storage backend)\nmodel_registry = Dataset(\n    root_dir=\"s3://my-bucket/models\",  # or local path, GCS, Azure, etc.\n    name=\"sentiment_classifier\"\n)\n</code></pre>"},{"location":"guides/model-versioning/#2-save-your-first-model","title":"2. Save Your First Model","text":"<pre><code>import torch\n\n# Save your model files\ntorch.save(model.state_dict(), \"model_weights.pt\")\n\n# Commit with metadata and tags\ncommit_hash = model_registry.commit(\n    message=\"Initial baseline model\",\n    add_files=[\"model_weights.pt\", \"config.json\"],\n    metadata={\n        \"framework\": \"pytorch\",\n        \"accuracy\": 0.87,\n        \"f1_score\": 0.85,\n        \"hyperparameters\": {\n            \"learning_rate\": 0.001,\n            \"epochs\": 10,\n            \"batch_size\": 32\n        },\n        \"training_data\": \"sentiment_v1\",\n        \"model_size_mb\": 0.5\n    },\n    tags=[\"baseline\", \"v1.0\"]\n)\n</code></pre>"},{"location":"guides/model-versioning/#3-save-improved-models","title":"3. Save Improved Models","text":"<pre><code># Train an improved model\n# ... training code ...\n\n# Save improved model\ntorch.save(improved_model.state_dict(), \"model_weights_v2.pt\")\n\n# Commit with updated metadata\ncommit_hash = model_registry.commit(\n    message=\"Improved model with better regularization\",\n    add_files=[\"model_weights_v2.pt\"],\n    metadata={\n        \"framework\": \"pytorch\",\n        \"accuracy\": 0.92,  # Improved!\n        \"f1_score\": 0.90,\n        \"hyperparameters\": {\n            \"learning_rate\": 0.0005,\n            \"epochs\": 15,\n            \"batch_size\": 32,\n            \"weight_decay\": 0.01  # Added regularization\n        },\n        \"training_data\": \"sentiment_v2\",\n        \"improvements\": [\"Better regularization\", \"More epochs\"]\n    },\n    tags=[\"improved\", \"v2.0\", \"production\"]\n)\n</code></pre>"},{"location":"guides/model-versioning/#querying-and-discovery","title":"Querying and Discovery","text":""},{"location":"guides/model-versioning/#find-models-by-tags","title":"Find Models by Tags","text":"<pre><code># Find all production models\nproduction_models = model_registry.find_commits(tags=[\"production\"])\n\n# Find models with multiple tags\nv2_models = model_registry.find_commits(tags=[\"v2.0\", \"production\"])\n</code></pre>"},{"location":"guides/model-versioning/#find-models-by-performance-metrics","title":"Find Models by Performance Metrics","text":"<pre><code># Find high-accuracy models\nhigh_accuracy = model_registry.find_commits(\n    metadata_filter=lambda m: m.get(\"accuracy\", 0) &gt; 0.9\n)\n\n# Find models by framework\npytorch_models = model_registry.find_commits(\n    metadata_filter=lambda m: m.get(\"framework\") == \"pytorch\"\n)\n\n# Complex queries\nbest_models = model_registry.find_commits(\n    tags=[\"production\"],\n    metadata_filter=lambda m: (\n        m.get(\"accuracy\", 0) &gt; 0.9 and\n        m.get(\"f1_score\", 0) &gt; 0.85\n    )\n)\n</code></pre>"},{"location":"guides/model-versioning/#compare-model-versions","title":"Compare Model Versions","text":"<pre><code># Compare two model versions\ncomparison = model_registry.compare_commits(\n    \"abc123def\",  # First model hash\n    \"xyz789ghi\"   # Second model hash\n)\n\nprint(\"Metadata changes:\")\nprint(comparison[\"metadata_diff\"][\"changed\"])\nprint(\"Tag changes:\")\nprint(comparison[\"tags_diff\"])\n</code></pre>"},{"location":"guides/model-versioning/#loading-and-using-models","title":"Loading and Using Models","text":""},{"location":"guides/model-versioning/#checkout-specific-model-version","title":"Checkout Specific Model Version","text":"<pre><code># Checkout a specific model version\nmodel_registry.checkout(\"abc123def\")\n\n# Access files from that version\nwith model_registry.local_files() as files:\n    # Files are lazily downloaded when accessed\n    model_path = files[\"model_weights.pt\"]\n    config_path = files[\"config.json\"]\n\n    # Load your model\n    model = torch.load(model_path)\n</code></pre>"},{"location":"guides/model-versioning/#get-model-information","title":"Get Model Information","text":"<pre><code># Get current model info\ncurrent_commit = model_registry.current_commit\nprint(f\"Model: {current_commit.message}\")\nprint(f\"Accuracy: {current_commit.metadata['accuracy']}\")\nprint(f\"Tags: {current_commit.tags}\")\n\n# List all files in current version\nfor filename in model_registry.list_files():\n    file_obj = model_registry.get_file(filename)\n    print(f\"{filename}: {file_obj.size} bytes\")\n</code></pre>"},{"location":"guides/model-versioning/#metadata-schema-conventions","title":"Metadata Schema Conventions","text":"<p>While Kirin doesn't enforce a specific schema, here are recommended conventions:</p>"},{"location":"guides/model-versioning/#core-model-information","title":"Core Model Information","text":"<pre><code>metadata = {\n    # Required\n    \"framework\": \"pytorch\",  # or \"tensorflow\", \"sklearn\", etc.\n    \"version\": \"2.1.0\",      # Semantic versioning\n\n    # Performance metrics\n    \"accuracy\": 0.92,\n    \"f1_score\": 0.90,\n    \"precision\": 0.91,\n    \"recall\": 0.89,\n\n    # Model configuration\n    \"hyperparameters\": {\n        \"learning_rate\": 0.001,\n        \"epochs\": 10,\n        \"batch_size\": 32,\n        \"optimizer\": \"adam\"\n    },\n\n    # Training information\n    \"training_data\": \"dataset_v2\",\n    \"training_time_seconds\": 1200,\n    \"model_size_mb\": 0.5,\n\n    # Optional: domain-specific info\n    \"domain\": \"medical\",\n    \"use_cases\": [\"patient_feedback\", \"clinical_notes\"]\n}\n</code></pre>"},{"location":"guides/model-versioning/#tag-conventions","title":"Tag Conventions","text":"<pre><code># Staging tags\ntags = [\"dev\", \"staging\", \"production\"]\n\n# Version tags\ntags = [\"v1.0\", \"v2.0\", \"v2.1\"]\n\n# Domain tags\ntags = [\"medical\", \"financial\", \"general\"]\n\n# Status tags\ntags = [\"baseline\", \"improved\", \"experimental\"]\n</code></pre>"},{"location":"guides/model-versioning/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guides/model-versioning/#model-staging-pipeline","title":"Model Staging Pipeline","text":"<pre><code># Development model\ndev_commit = model_registry.commit(\n    message=\"Experimental model with new architecture\",\n    add_files=[\"model.pt\"],\n    metadata={\"accuracy\": 0.88, \"framework\": \"pytorch\"},\n    tags=[\"dev\", \"experimental\"]\n)\n\n# Promote to staging\nstaging_commit = model_registry.commit(\n    message=\"Promote experimental model to staging\",\n    add_files=[\"model.pt\"],  # Same model, different tags\n    metadata={\"accuracy\": 0.88, \"framework\": \"pytorch\"},\n    tags=[\"staging\", \"v2.1-beta\"]\n)\n\n# Promote to production\nprod_commit = model_registry.commit(\n    message=\"Release v2.1 to production\",\n    add_files=[\"model.pt\"],\n    metadata={\"accuracy\": 0.88, \"framework\": \"pytorch\"},\n    tags=[\"production\", \"v2.1\"]\n)\n</code></pre>"},{"location":"guides/model-versioning/#ab-testing-models","title":"A/B Testing Models","text":"<pre><code># Model A\nmodel_a = model_registry.commit(\n    message=\"Model A - Original architecture\",\n    add_files=[\"model_a.pt\"],\n    metadata={\"accuracy\": 0.89, \"architecture\": \"original\"},\n    tags=[\"ab-test\", \"model-a\"]\n)\n\n# Model B\nmodel_b = model_registry.commit(\n    message=\"Model B - Improved architecture\",\n    add_files=[\"model_b.pt\"],\n    metadata={\"accuracy\": 0.92, \"architecture\": \"improved\"},\n    tags=[\"ab-test\", \"model-b\"]\n)\n\n# Find A/B test models\nab_models = model_registry.find_commits(tags=[\"ab-test\"])\n</code></pre>"},{"location":"guides/model-versioning/#domain-specific-models","title":"Domain-Specific Models","text":"<pre><code># General model\ngeneral_model = model_registry.commit(\n    message=\"General sentiment classifier\",\n    add_files=[\"general_model.pt\"],\n    metadata={\n        \"accuracy\": 0.90,\n        \"domain\": \"general\",\n        \"training_data\": \"general_reviews\"\n    },\n    tags=[\"general\", \"v1.0\"]\n)\n\n# Medical domain model\nmedical_model = model_registry.commit(\n    message=\"Medical sentiment classifier\",\n    add_files=[\"medical_model.pt\"],\n    metadata={\n        \"accuracy\": 0.85,  # Lower on general data\n        \"domain_accuracy\": 0.94,  # Higher on medical data\n        \"domain\": \"medical\",\n        \"training_data\": \"medical_reviews\"\n    },\n    tags=[\"medical\", \"domain-specific\", \"v1.1\"]\n)\n</code></pre>"},{"location":"guides/model-versioning/#best-practices","title":"Best Practices","text":""},{"location":"guides/model-versioning/#1-consistent-metadata-schema","title":"1. Consistent Metadata Schema","text":"<p>Define a standard metadata schema for your team:</p> <pre><code>def create_model_metadata(accuracy, f1_score, hyperparams, training_info):\n    \"\"\"Create standardized metadata for model commits.\"\"\"\n    return {\n        \"framework\": \"pytorch\",\n        \"accuracy\": accuracy,\n        \"f1_score\": f1_score,\n        \"hyperparameters\": hyperparams,\n        \"training_data\": training_info[\"dataset\"],\n        \"training_time_seconds\": training_info[\"duration\"],\n        \"model_size_mb\": training_info[\"size_mb\"],\n        \"timestamp\": datetime.now().isoformat()\n    }\n</code></pre>"},{"location":"guides/model-versioning/#2-meaningful-commit-messages","title":"2. Meaningful Commit Messages","text":"<pre><code># Good commit messages\n\"Initial baseline model - BERT fine-tuned on customer reviews\"\n\"Improved model v2.0 - Added regularization and more training data\"\n\"Hotfix v2.0.1 - Fixed tokenization bug in production model\"\n\"Domain adaptation - Medical sentiment classifier\"\n\n# Avoid vague messages\n\"Updated model\"\n\"New version\"\n\"Changes\"\n</code></pre>"},{"location":"guides/model-versioning/#3-tag-management","title":"3. Tag Management","text":"<p>Use consistent tagging strategies:</p> <pre><code># Semantic versioning\ntags = [\"v1.0.0\", \"v1.1.0\", \"v2.0.0\"]\n\n# Staging pipeline\ntags = [\"dev\", \"staging\", \"production\"]\n\n# Feature flags\ntags = [\"feature-xyz\", \"experimental\", \"deprecated\"]\n</code></pre>"},{"location":"guides/model-versioning/#4-model-validation","title":"4. Model Validation","text":"<p>Always validate model performance before committing:</p> <pre><code>def validate_model(model, test_data):\n    \"\"\"Validate model before committing.\"\"\"\n    accuracy = evaluate_model(model, test_data)\n    if accuracy &lt; 0.8:\n        raise ValueError(f\"Model accuracy {accuracy} below threshold\")\n    return accuracy\n\n# Use in commit workflow\naccuracy = validate_model(model, test_data)\nmodel_registry.commit(\n    message=\"Validated model v2.0\",\n    add_files=[\"model.pt\"],\n    metadata={\"accuracy\": accuracy, \"validated\": True},\n    tags=[\"validated\", \"v2.0\"]\n)\n</code></pre>"},{"location":"guides/model-versioning/#integration-with-ml-workflows","title":"Integration with ML Workflows","text":""},{"location":"guides/model-versioning/#with-experiment-tracking","title":"With Experiment Tracking","text":"<pre><code># Log to both Kirin and your experiment tracker\nimport wandb\n\n# Start experiment\nwandb.init(project=\"sentiment-classifier\")\n\n# Train model\nmodel, metrics = train_model()\n\n# Log to experiment tracker\nwandb.log(metrics)\n\n# Commit to Kirin\ncommit_hash = model_registry.commit(\n    message=f\"Experiment {wandb.run.name}\",\n    add_files=[\"model.pt\"],\n    metadata={\n        **metrics,\n        \"experiment_id\": wandb.run.id,\n        \"run_name\": wandb.run.name\n    },\n    tags=[\"experiment\", wandb.run.name]\n)\n</code></pre>"},{"location":"guides/model-versioning/#with-model-serving","title":"With Model Serving","text":"<pre><code># Deploy model from Kirin\ndef deploy_model(commit_hash):\n    model_registry.checkout(commit_hash)\n\n    with model_registry.local_files() as files:\n        model = torch.load(files[\"model.pt\"])\n        config = json.load(open(files[\"config.json\"]))\n\n    # Deploy to your serving infrastructure\n    deploy_to_production(model, config)\n    print(f\"Deployed model {commit_hash[:8]}\")\n</code></pre>"},{"location":"guides/model-versioning/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/model-versioning/#common-issues","title":"Common Issues","text":""},{"location":"guides/model-versioning/#my-metadata-isnt-being-saved","title":"My metadata isn't being saved","text":"<p>Make sure you're passing the <code>metadata</code> parameter to <code>commit()</code>. Check that your metadata is JSON-serializable.</p>"},{"location":"guides/model-versioning/#cant-find-my-models-with-find_commits","title":"Can't find my models with <code>find_commits()</code>","text":"<p>Verify your filter function returns a boolean. Use <code>lambda m: m.get(\"key\", default) &gt; value</code> for safe access.</p>"},{"location":"guides/model-versioning/#files-arent-downloading-with-local_files","title":"Files aren't downloading with <code>local_files()</code>","text":"<p>Files are lazily loaded. Access them through the dictionary: <code>local_files[\"filename\"]</code> to trigger download.</p>"},{"location":"guides/model-versioning/#how-do-i-migrate-from-other-model-versioning-tools","title":"How do I migrate from other model versioning tools?","text":"<p>Kirin can work alongside other tools. You can import models from MLflow, DVC, etc., by saving their artifacts and committing them to Kirin.</p>"},{"location":"guides/model-versioning/#performance-tips","title":"Performance Tips","text":"<ul> <li>Use <code>limit</code> parameter in <code>find_commits()</code> for large datasets</li> <li>Store large models in cloud storage (S3, GCS) for better performance</li> <li>Use <code>local_files()</code> context manager to ensure cleanup of temporary files</li> <li>Consider using tags for frequently queried model categories</li> </ul>"},{"location":"guides/model-versioning/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the API Reference for detailed method documentation</li> <li>Check out the Model Versioning   Demo for a complete example</li> <li>Learn about Cloud Storage Integration for production deployments</li> </ul>"},{"location":"guides/working-with-files/","title":"Working with Files","text":"<p>Advanced file operations and integration patterns for data science workflows.</p> <p>Notebook Tip: When working in Jupyter or Marimo notebooks, displaying a dataset (e.g., <code>dataset</code>) shows an interactive HTML view. Click \"Copy Code to Access\" on any file to get code snippets that automatically use your variable name!</p>"},{"location":"guides/working-with-files/#file-access-patterns","title":"File Access Patterns","text":""},{"location":"guides/working-with-files/#local-files-context-manager-recommended","title":"Local Files Context Manager (Recommended)","text":"<p>The <code>local_files()</code> context manager is the recommended way to work with files:</p> <pre><code>from pathlib import Path\nimport pandas as pd\nimport polars as pl\n\n# Work with files locally\nwith dataset.local_files() as local_files:\n    for filename, local_path in local_files.items():\n        print(f\"{filename} -&gt; {local_path}\")\n\n        # Process files with standard Python libraries\n        if filename.endswith('.csv'):\n            # Use pandas\n            df = pd.read_csv(local_path)\n            print(f\"CSV shape: {df.shape}\")\n\n            # Or use polars\n            df_polars = pl.read_csv(local_path)\n            print(f\"Polars shape: {df_polars.shape}\")\n\n        elif filename.endswith('.parquet'):\n            # Read parquet files\n            df = pd.read_parquet(local_path)\n            print(f\"Parquet shape: {df.shape}\")\n\n        elif filename.endswith('.json'):\n            import json\n            data = json.loads(Path(local_path).read_text())\n            print(f\"JSON keys: {list(data.keys())}\")\n</code></pre> <p>Benefits:</p> <ul> <li>Library compatibility: Works with pandas, polars, numpy, etc.</li> <li>Automatic cleanup: Files cleaned up when done</li> <li>Standard paths: Use normal Python file operations</li> <li>Memory efficient: No need to load entire files into memory</li> </ul>"},{"location":"guides/working-with-files/#working-with-data-science-libraries","title":"Working with Data Science Libraries","text":""},{"location":"guides/working-with-files/#pandas-examples","title":"Pandas Examples","text":"<pre><code>import pandas as pd\n\nwith dataset.local_files() as local_files:\n    # Read CSV files\n    if \"data.csv\" in local_files:\n        df = pd.read_csv(local_files[\"data.csv\"])\n        print(f\"DataFrame shape: {df.shape}\")\n\n        # Process data\n        df_processed = df.dropna().reset_index(drop=True)\n\n        # Save processed data\n        df_processed.to_csv(\"processed_data.csv\")\n\n        # Commit processed data\n        dataset.commit(\n            message=\"Add processed data\",\n            add_files=[\"processed_data.csv\"]\n        )\n</code></pre>"},{"location":"guides/working-with-files/#polars-examples","title":"Polars Examples","text":"<pre><code>import polars as pl\n\nwith dataset.local_files() as local_files:\n    # Read CSV files with Polars\n    if \"data.csv\" in local_files:\n        df = pl.read_csv(local_files[\"data.csv\"])\n        print(f\"Polars DataFrame shape: {df.shape}\")\n\n        # Process data with Polars\n        df_processed = df.filter(pl.col(\"value\").is_not_null())\n\n        # Save processed data\n        df_processed.write_csv(\"processed_data.csv\")\n\n        # Commit processed data\n        dataset.commit(\n            message=\"Add processed data\",\n            add_files=[\"processed_data.csv\"]\n        )\n</code></pre>"},{"location":"guides/working-with-files/#numpy-examples","title":"NumPy Examples","text":"<pre><code>import numpy as np\n\nwith dataset.local_files() as local_files:\n    # Read CSV files as NumPy arrays\n    if \"data.csv\" in local_files:\n        df = pd.read_csv(local_files[\"data.csv\"])\n        array = df.values\n        print(f\"NumPy array shape: {array.shape}\")\n\n        # Process with NumPy\n        processed_array = np.nan_to_num(array)\n\n        # Save processed array\n        np.savetxt(\"processed_data.csv\", processed_array, delimiter=\",\")\n\n        # Commit processed data\n        dataset.commit(\n            message=\"Add processed array\",\n            add_files=[\"processed_data.csv\"]\n        )\n</code></pre>"},{"location":"guides/working-with-files/#scikit-learn-examples","title":"Scikit-learn Examples","text":"<pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport joblib\n\nwith dataset.local_files() as local_files:\n    # Load training data\n    if \"train.csv\" in local_files:\n        df = pd.read_csv(local_files[\"train.csv\"])\n        X = df.drop(\"target\", axis=1)\n        y = df[\"target\"]\n\n        # Split data\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n        # Train model\n        model = RandomForestClassifier(n_estimators=100)\n        model.fit(X_train, y_train)\n\n        # Save model\n        joblib.dump(model, \"model.pkl\")\n\n        # Save test data\n        X_test.to_csv(\"X_test.csv\", index=False)\n        y_test.to_csv(\"y_test.csv\", index=False)\n\n        # Commit model and test data\n        dataset.commit(\n            message=\"Add trained model and test data\",\n            add_files=[\"model.pkl\", \"X_test.csv\", \"y_test.csv\"]\n        )\n</code></pre>"},{"location":"guides/working-with-files/#best-practices","title":"Best Practices","text":""},{"location":"guides/working-with-files/#file-naming","title":"File Naming","text":"<p>Use descriptive, consistent file names:</p> <pre><code># Good naming\ndataset.commit(\"Add Q1 sales data\", add_files=[\"sales_q1_2024.csv\"])\ndataset.commit(\"Add processed data\", add_files=[\"sales_q1_2024_processed.csv\"])\ndataset.commit(\"Add model results\", add_files=[\"model_results_q1_2024.json\"])\n\n# Avoid vague names\ndataset.commit(\"Add data\", add_files=[\"data.csv\"])\ndataset.commit(\"Update\", add_files=[\"file1.csv\", \"file2.csv\"])\n</code></pre>"},{"location":"guides/working-with-files/#file-organization","title":"File Organization","text":"<p>Organize files in logical directories:</p> <pre><code># Good organization\ndataset.commit(\"Add raw data\", add_files=[\"raw/sales.csv\", \"raw/customers.csv\"])\ndataset.commit(\"Add processed data\", add_files=[\"processed/sales_clean.csv\"])\ndataset.commit(\"Add analysis\", add_files=[\"analysis/results.csv\", \"analysis/plots.png\"])\n\n# Avoid flat structure\ndataset.commit(\"Add files\", add_files=[\"sales.csv\", \"customers.csv\",\n                                        \"results.csv\", \"plots.png\"])\n</code></pre>"},{"location":"guides/working-with-files/#next-steps","title":"Next Steps","text":"<ul> <li>Commit Management - Understanding commit   history</li> <li>Cloud Storage - Working with cloud backends</li> <li>Basic Usage - Core dataset operations</li> </ul>"},{"location":"reference/api/","title":"Kirin API Reference","text":""},{"location":"reference/api/#core-classes","title":"Core Classes","text":""},{"location":"reference/api/#dataset","title":"Dataset","text":"<p>The main class for working with Kirin datasets.</p> <pre><code>from kirin import Dataset\n\n# Create or load a dataset\ndataset = Dataset(root_dir=\"/path/to/data\", name=\"my-dataset\")\n</code></pre>"},{"location":"reference/api/#constructor-parameters","title":"Constructor Parameters","text":"<pre><code>Dataset(\n    root_dir: Union[str, Path],           # Root directory for the dataset\n    name: str,                           # Name of the dataset\n    description: str = \"\",               # Description of the dataset\n    fs: Optional[fsspec.AbstractFileSystem] = None,  # Filesystem to use\n    # Cloud authentication parameters\n    aws_profile: Optional[str] = None,   # AWS profile for S3 authentication\n    gcs_token: Optional[Union[str, Path]] = None,  # GCS service account token\n    gcs_project: Optional[str] = None,   # GCS project ID\n    azure_account_name: Optional[str] = None,  # Azure account name\n    azure_account_key: Optional[str] = None,    # Azure account key\n    azure_connection_string: Optional[str] = None,  # Azure connection string\n)\n</code></pre>"},{"location":"reference/api/#basic-operations","title":"Basic Operations","text":"<ul> <li><code>commit(message, add_files=None, remove_files=None, metadata=None,   tags=None)</code> - Commit changes to the dataset</li> <li><code>checkout(commit_hash=None)</code> - Switch to a specific commit (latest if None)</li> <li><code>files</code> - Dictionary of files in the current commit</li> <li><code>local_files()</code> - Context manager for accessing files as local paths</li> <li><code>history(limit=None)</code> - Get commit history</li> <li><code>get_file(filename)</code> - Get a file from the current commit</li> <li><code>read_file(filename)</code> - Read file content as text</li> <li><code>download_file(filename, target_path)</code> - Download file to local path</li> <li><code>save_plot(plot_object, filename, auto_commit=False, message=None, ...)</code> -   Save plots with automatic format detection</li> </ul>"},{"location":"reference/api/#plot-versioning-operations","title":"Plot Versioning Operations","text":"<ul> <li><code>save_plot(plot_object, filename, auto_commit=False, message=None,   metadata=None, tags=None, format=None)</code> - Save plots from matplotlib,   plotly, etc. with automatic format detection (SVG for vectors, WebP for   bitmaps)</li> </ul>"},{"location":"reference/api/#model-versioning-operations","title":"Model Versioning Operations","text":"<ul> <li><code>find_commits(tags=None, metadata_filter=None, limit=None)</code> - Find commits   matching criteria</li> <li><code>compare_commits(hash1, hash2)</code> - Compare metadata between two commits</li> </ul>"},{"location":"reference/api/#notebook-integration","title":"Notebook Integration","text":"<p>Kirin provides rich HTML representations for datasets, commits, and catalogs that display beautifully in Jupyter and Marimo notebooks.</p>"},{"location":"reference/api/#html-representation","title":"HTML Representation","text":"<p>When you display a <code>Dataset</code>, <code>Commit</code>, or <code>Catalog</code> object in a notebook cell, Kirin automatically generates an interactive HTML view with:</p> <ul> <li>File Lists: Click on any file to reveal/hide code snippets showing how to   access it</li> <li>Copy Code to Access Button: Each file has a button that copies Python code   to your clipboard with the correct variable name</li> <li>Commit History: Visual display of commit history with metadata</li> <li>File Metadata: File sizes, content types, and icons</li> </ul> <p>Example:</p> <pre><code>from kirin import Dataset\n\ndataset = Dataset(root_dir=\"/path/to/data\", name=\"my_dataset\")\n\n# Display in notebook - shows interactive HTML\ndataset\n</code></pre>"},{"location":"reference/api/#variable-names-in-code-snippets","title":"Variable Names in Code Snippets","text":"<p>By default, code snippets use generic variable names (\"dataset\", \"commit\", or \"catalog\") based on the class type. You can customize the variable name used in code snippets by setting the <code>_repr_variable_name</code> attribute.</p> <p>Default Behavior:</p> <pre><code>dataset = Dataset(root_dir=\"/path/to/data\", name=\"my_dataset\")\n\n# Display in notebook - code snippets use \"dataset\" by default\ndataset\n</code></pre> <p>When you click \"Copy Code to Access\" on a file, the copied code will use the default variable name:</p> <pre><code># Get path to local clone of file\nwith dataset.local_files() as files:\n    file_path = files[\"data.csv\"]\n</code></pre> <p>Custom Variable Names:</p> <p>If you want code snippets to use a different variable name, set the <code>_repr_variable_name</code> attribute:</p> <pre><code>my_dataset = Dataset(root_dir=\"/path/to/data\", name=\"my_dataset\")\nmy_dataset._repr_variable_name = \"my_dataset\"\n\n# Now code snippets will use \"my_dataset\" instead of \"dataset\"\nmy_dataset  # Display in notebook\n</code></pre> <p>When you click \"Copy Code to Access\", the copied code will use your custom variable name:</p> <pre><code># Get path to local clone of file\nwith my_dataset.local_files() as files:\n    file_path = files[\"data.csv\"]\n</code></pre> <p>Note: The <code>_repr_variable_name</code> attribute is only used for HTML representation and doesn't affect the actual dataset object.</p> <p>Known Limitation (as of December 2025): The \"Copy Code to Access\" button does not work within Marimo notebooks running inside VSCode due to clipboard API restrictions. The button works correctly when viewing notebooks in a web browser.</p>"},{"location":"reference/api/#method-details","title":"Method Details","text":""},{"location":"reference/api/#commitmessage-add_filesnone-remove_filesnone-metadatanone-tagsnone","title":"<code>commit(message, add_files=None, remove_files=None, metadata=None, tags=None)</code>","text":"<p>Create a new commit with changes to the dataset.</p> <p>Parameters:</p> <ul> <li><code>message</code> (str): Commit message describing the changes</li> <li><code>add_files</code> (List[Union[str, Path]], optional): List of files to add or update</li> <li><code>remove_files</code> (List[str], optional): List of filenames to remove</li> <li><code>metadata</code> (Dict[str, Any], optional): Metadata dictionary for model versioning</li> <li><code>tags</code> (List[str], optional): List of tags for staging/versioning</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Hash of the new commit</li> </ul> <p>Examples:</p> <pre><code># Basic commit\ndataset.commit(\"Add new data\", add_files=[\"data.csv\"])\n\n# Model versioning commit\ndataset.commit(\n    message=\"Improved model v2.0\",\n    add_files=[\"model.pt\", \"config.json\"],\n    metadata={\n        \"framework\": \"pytorch\",\n        \"accuracy\": 0.92,\n        \"hyperparameters\": {\"lr\": 0.001, \"epochs\": 10}\n    },\n    tags=[\"production\", \"v2.0\"]\n)\n</code></pre>"},{"location":"reference/api/#find_commitstagsnone-metadata_filternone-limitnone","title":"<code>find_commits(tags=None, metadata_filter=None, limit=None)</code>","text":"<p>Find commits matching specified criteria.</p> <p>Parameters:</p> <ul> <li><code>tags</code> (List[str], optional): Filter by tags (commits must have ALL   specified tags)</li> <li><code>metadata_filter</code> (Callable[[Dict], bool], optional): Function that takes   metadata dict and returns bool</li> <li><code>limit</code> (int, optional): Maximum number of commits to return</li> </ul> <p>Returns:</p> <ul> <li><code>List[Commit]</code>: List of matching commits (newest first)</li> </ul> <p>Examples:</p> <pre><code># Find production models\nproduction_models = dataset.find_commits(tags=[\"production\"])\n\n# Find high-accuracy models\nhigh_accuracy = dataset.find_commits(\n    metadata_filter=lambda m: m.get(\"accuracy\", 0) &gt; 0.9\n)\n\n# Find PyTorch production models\npytorch_prod = dataset.find_commits(\n    tags=[\"production\"],\n    metadata_filter=lambda m: m.get(\"framework\") == \"pytorch\"\n)\n</code></pre>"},{"location":"reference/api/#compare_commitshash1-hash2","title":"<code>compare_commits(hash1, hash2)</code>","text":"<p>Compare metadata between two commits.</p> <p>Parameters:</p> <ul> <li><code>hash1</code> (str): First commit hash</li> <li><code>hash2</code> (str): Second commit hash</li> </ul> <p>Returns:</p> <ul> <li><code>dict</code>: Dictionary with comparison results including metadata and tag   differences</li> </ul> <p>Example:</p> <pre><code>comparison = dataset.compare_commits(\"abc123\", \"def456\")\nprint(\"Metadata changes:\", comparison[\"metadata_diff\"][\"changed\"])\nprint(\"Tag changes:\", comparison[\"tags_diff\"])\n</code></pre>"},{"location":"reference/api/#save_plot","title":"save_plot","text":"<p>Save a plot to the dataset with automatic format detection.</p> <p>Signature:</p> <pre><code>save_plot(\n    plot_object,\n    filename,\n    auto_commit=False,\n    message=None,\n    metadata=None,\n    tags=None,\n    format=None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>plot_object</code>: The plot object to save (matplotlib Figure, plotly Figure,   etc.)</li> <li><code>filename</code> (str): Desired filename for the plot (extension may be adjusted)</li> <li><code>auto_commit</code> (bool): If True, automatically commits the plot. If False,   returns file path for explicit commit</li> <li><code>message</code> (str, optional): Commit message (required if auto_commit=True)</li> <li><code>metadata</code> (Dict[str, Any], optional): Metadata dictionary for the commit</li> <li><code>tags</code> (List[str], optional): List of tags for the commit</li> <li><code>format</code> (str, optional): Format override ('svg' or 'webp'). If None, auto-detects</li> </ul> <p>Returns:</p> <ul> <li>If <code>auto_commit=False</code>: File path (str) that can be used in <code>commit()</code></li> <li>If <code>auto_commit=True</code>: Commit hash (str) of the created commit</li> </ul>"},{"location":"reference/api/#strict-content-hashing-for-svg-plots","title":"Strict Content Hashing for SVG Plots","text":"<p>Kirin uses strict content-addressed hashing based on exact file content. This means:</p> <ul> <li> <p>SVG plots will produce different hashes even when the plot content is   identical, because matplotlib embeds creation timestamps in the SVG metadata   (<code>&lt;dc:date&gt;</code> elements). This is the strictest form of hashing and ensures   complete content integrity.</p> </li> <li> <p>Identical plots = different commits: If you save the same plot twice (same   data, same code), you'll get different hashes and thus different commits,   because the SVG files differ by their timestamps.</p> </li> <li> <p>This is by design: Content-addressed storage requires exact byte matching.   The timestamp metadata is part of the file content, so it affects the hash.</p> </li> <li> <p>For deterministic hashing: If you need identical plots to produce identical   hashes, consider using WebP format (raster) instead of SVG, or strip metadata   before saving (not currently implemented).</p> </li> </ul>"},{"location":"reference/api/#automatic-source-file-linking","title":"Automatic Source File Linking","text":"<p>When saving a plot, Kirin automatically detects and stores the source notebook or script that generated the plot, linking them together via metadata:</p> <ul> <li> <p>Automatic Detection: Uses <code>inspect.stack()</code> to detect the calling script or   notebook. For Jupyter notebooks, uses the <code>ipynbname</code> package if available.</p> </li> <li> <p>Source File Storage: The source file is automatically stored in the same   commit as the plot, using content-addressed storage (deduplication handled   automatically).</p> </li> <li> <p>Metadata Linking: The plot file's metadata contains:</p> </li> <li><code>source_file</code>: Filename of the source notebook/script</li> <li> <p><code>source_hash</code>: Content hash of the source file</p> </li> <li> <p>Web UI Display: The web UI displays source file links in both the file list   and file preview pages, making it easy to navigate from plots to their source   code.</p> </li> <li> <p>Edge Cases: If source detection fails (e.g., running in interactive shell),   the plot is still saved successfully, but no source linking occurs.</p> </li> </ul> <p>Example:</p> <pre><code># In a Jupyter notebook or Python script\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.plot([1, 2, 3], [1, 4, 9])\n\n# Save plot - source file is automatically detected and linked\ncommit_hash = dataset.save_plot(\n    fig, \"plot.png\", auto_commit=True, message=\"Add visualization\"\n)\n\n# Access source file metadata\nplot_file = dataset.get_file(\"plot.png\")\nif plot_file.metadata.get(\"source_file\"):\n    print(f\"Plot generated from: {plot_file.metadata['source_file']}\")\n    source_file = dataset.get_file(plot_file.metadata[\"source_file\"])\n    print(f\"Source file hash: {source_file.hash}\")\n</code></pre> <p>Examples:</p> <pre><code>import matplotlib.pyplot as plt\n\n# Create a plot\nfig, ax = plt.subplots()\nax.plot([1, 2, 3], [1, 4, 9])\n\n# Auto-commit mode (convenient for single plots)\ncommit_hash = dataset.save_plot(\n    fig, \"plot.png\", auto_commit=True, message=\"Add visualization\"\n)\n\n# Default mode (allows batching multiple plots)\nplot_path = dataset.save_plot(fig, \"plot1.png\")\nplot_path2 = dataset.save_plot(fig2, \"plot2.png\")\ndataset.commit(message=\"Add multiple plots\", add_files=[plot_path, plot_path2])\n</code></pre>"},{"location":"reference/api/#examples","title":"Examples","text":"<pre><code># Basic usage\ndataset = Dataset(root_dir=\"/data\", name=\"project\")\ndataset.commit(\"Initial commit\", add_files=[\"data.csv\"])\n\n# Cloud storage with authentication\ndataset = Dataset(\n    root_dir=\"s3://my-bucket/data\",\n    name=\"project\",\n    aws_profile=\"my-profile\"\n)\n\n# GCS with service account\ndataset = Dataset(\n    root_dir=\"gs://my-bucket/data\",\n    name=\"project\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"my-project\"\n)\n\n# Azure with connection string\ndataset = Dataset(\n    root_dir=\"az://my-container/data\",\n    name=\"project\",\n    azure_connection_string=os.getenv(\"AZURE_CONNECTION_STRING\")\n)\n</code></pre>"},{"location":"reference/api/#save_plot-standalone-function","title":"save_plot (standalone function)","text":"<p>Standalone function for saving plots to content-addressed storage.</p> <pre><code>from kirin import save_plot\nfrom kirin.storage import ContentStore\n\nstorage = ContentStore(\"/path/to/data\")\nhash, filename = save_plot(fig, \"plot.png\", storage)\n</code></pre>"},{"location":"reference/api/#strict-content-hashing-for-svg-plots-save_plot-function","title":"Strict Content Hashing for SVG Plots (save_plot function)","text":"<p>Kirin uses strict content-addressed hashing - files are identified by their exact byte content. SVG plots from matplotlib will produce different hashes even when the plot content is identical, because matplotlib embeds creation timestamps in the SVG metadata. This is the strictest form of hashing and ensures complete content integrity. See <code>Dataset.save_plot()</code> documentation for more details.</p> <p>Parameters:</p> <ul> <li><code>plot_object</code>: The plot object to save (matplotlib Figure, plotly Figure, etc.)</li> <li><code>filename</code> (str): Desired filename for the plot (extension may be adjusted)</li> <li><code>storage</code> (ContentStore): ContentStore instance to use for storage</li> <li><code>format</code> (str, optional): Format override ('svg' or 'webp'). If None, auto-detects</li> </ul> <p>Returns:</p> <ul> <li><code>tuple[str, str, Optional[str], Optional[str]]</code>: Tuple of   (content_hash, actual_filename, source_file_path, source_file_hash) where:</li> <li><code>content_hash</code>: Hash of the stored plot file</li> <li><code>actual_filename</code>: Filename used (may have extension adjusted based on format)</li> <li><code>source_file_path</code>: Path to source notebook/script, or None if not detected</li> <li><code>source_file_hash</code>: Hash of stored source file, or None if not detected</li> </ul> <p>Example:</p> <pre><code>import matplotlib.pyplot as plt\nfrom kirin import save_plot\nfrom kirin.storage import ContentStore\n\nstorage = ContentStore(\"/path/to/data\")\n\nfig, ax = plt.subplots()\nax.plot([1, 2, 3], [1, 4, 9])\n\n# Save plot (returns hash, filename, and source info)\ncontent_hash, actual_filename, source_path, source_hash = save_plot(\n    fig, \"plot.png\", storage\n)\nprint(f\"Saved plot with hash: {content_hash[:8]}\")\nprint(f\"Actual filename: {actual_filename}\")  # May be \"plot.svg\"\nif source_path:\n    print(f\"Source file: {source_path} (hash: {source_hash[:8]})\")\n</code></pre>"},{"location":"reference/api/#catalog","title":"Catalog","text":"<p>The main class for managing collections of datasets.</p> <pre><code>from kirin import Catalog\n\n# Create or load a catalog\ncatalog = Catalog(root_dir=\"/path/to/data\")\n</code></pre>"},{"location":"reference/api/#catalog-constructor-parameters","title":"Catalog Constructor Parameters","text":"<pre><code>Catalog(\n    root_dir: Union[str, fsspec.AbstractFileSystem],  # Root directory for the catalog\n    fs: Optional[fsspec.AbstractFileSystem] = None,  # Filesystem to use\n    # Cloud authentication parameters\n    aws_profile: Optional[str] = None,   # AWS profile for S3 authentication\n    gcs_token: Optional[Union[str, Path]] = None,  # GCS service account token\n    gcs_project: Optional[str] = None,   # GCS project ID\n    azure_account_name: Optional[str] = None,  # Azure account name\n    azure_account_key: Optional[str] = None,    # Azure account key\n    azure_connection_string: Optional[str] = None,  # Azure connection string\n)\n</code></pre>"},{"location":"reference/api/#catalog-basic-operations","title":"Catalog Basic Operations","text":"<ul> <li><code>datasets()</code> - List all datasets in the catalog</li> <li><code>get_dataset(name)</code> - Get a specific dataset</li> <li><code>create_dataset(name, description=\"\")</code> - Create a new dataset</li> <li><code>__len__()</code> - Number of datasets in the catalog</li> </ul>"},{"location":"reference/api/#catalog-examples","title":"Catalog Examples","text":"<pre><code># Basic usage\ncatalog = Catalog(root_dir=\"/data\")\ndatasets = catalog.datasets()\ndataset = catalog.get_dataset(\"my-dataset\")\n\n# Cloud storage with authentication\ncatalog = Catalog(\n    root_dir=\"s3://my-bucket/data\",\n    aws_profile=\"my-profile\"\n)\n\n# GCS with service account\ncatalog = Catalog(\n    root_dir=\"gs://my-bucket/data\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"my-project\"\n)\n</code></pre>"},{"location":"reference/api/#web-ui","title":"Web UI","text":"<p>The web UI provides a graphical interface for Kirin operations.</p>"},{"location":"reference/api/#routes","title":"Routes","text":"<ul> <li><code>/</code> - Home page for catalog management</li> <li><code>/catalogs/add</code> - Add new catalog</li> <li><code>/catalog/{catalog_id}</code> - View catalog and datasets</li> <li><code>/catalog/{catalog_id}/{dataset_name}</code> - View specific dataset</li> <li><code>/catalog/{catalog_id}/{dataset_name}/commit</code> - Commit interface</li> </ul>"},{"location":"reference/api/#catalog-management","title":"Catalog Management","text":"<p>The web UI supports cloud authentication through CatalogConfig:</p> <pre><code>from kirin.web.config import CatalogConfig\n\n# Create catalog config with cloud auth\nconfig = CatalogConfig(\n    id=\"my-catalog\",\n    name=\"My Catalog\",\n    root_dir=\"s3://my-bucket/data\",\n    aws_profile=\"my-profile\"\n)\n\n# Convert to runtime catalog\ncatalog = config.to_catalog()\n</code></pre>"},{"location":"reference/api/#cloud-authentication-in-web-ui","title":"Cloud Authentication in Web UI","text":"<p>The web UI automatically handles cloud authentication when you:</p> <ol> <li>Create a catalog with cloud storage URL (s3://, gs://, az://)</li> <li>The system will prompt for authentication parameters</li> <li>Credentials are stored securely in the catalog configuration</li> </ol>"},{"location":"reference/api/#storage-format","title":"Storage Format","text":"<p>Kirin uses a simplified Git-like storage format:</p> <pre><code>data/\n\u251c\u2500\u2500 data/                 # Content-addressed file storage\n\u2502   \u2514\u2500\u2500 {hash[:2]}/{hash[2:]}\n\u251c\u2500\u2500 datasets/\n\u2502   \u2514\u2500\u2500 my-dataset/\n\u2502       \u2514\u2500\u2500 commits.json  # Linear commit history\n</code></pre>"},{"location":"reference/api/#error-handling","title":"Error Handling","text":""},{"location":"reference/api/#common-exceptions","title":"Common Exceptions","text":"<ul> <li><code>ValueError</code> - Invalid operations (file not found, invalid commit hash, etc.)</li> <li><code>FileNotFoundError</code> - File not found in dataset</li> <li><code>HTTPException</code> - Web UI errors (catalog not found, validation errors)</li> </ul>"},{"location":"reference/api/#example-error-handling","title":"Example Error Handling","text":"<pre><code>try:\n    dataset.checkout(\"nonexistent-commit\")\nexcept ValueError as e:\n    print(f\"Checkout failed: {e}\")\n\ntry:\n    content = dataset.read_file(\"nonexistent.txt\")\nexcept FileNotFoundError as e:\n    print(f\"File not found: {e}\")\n</code></pre>"},{"location":"reference/api/#best-practices","title":"Best Practices","text":""},{"location":"reference/api/#dataset-naming","title":"Dataset Naming","text":"<ul> <li>Use descriptive names: <code>user-data</code>, <code>ml-experiments</code>, <code>production-models</code></li> <li>Avoid generic names: <code>test</code>, <code>data</code>, <code>temp</code></li> </ul>"},{"location":"reference/api/#workflow-patterns","title":"Workflow Patterns","text":"<ul> <li>Commit changes regularly with descriptive messages</li> <li>Use linear commit history for simplicity</li> <li>Keep datasets focused on specific use cases</li> <li>Use catalogs to organize related datasets</li> </ul>"},{"location":"reference/api/#file-management","title":"File Management","text":"<ul> <li>Use <code>local_files()</code> context manager for library compatibility</li> <li>Commit changes after adding/removing files</li> <li>Use descriptive commit messages</li> </ul>"},{"location":"reference/api/#advanced-features","title":"Advanced Features","text":""},{"location":"reference/api/#context-managers","title":"Context Managers","text":"<pre><code># Access files as local paths\nwith dataset.local_files() as local_files:\n    df = pd.read_csv(local_files[\"data.csv\"])\n    # Files automatically cleaned up\n</code></pre>"},{"location":"reference/api/#commit","title":"Commit","text":"<p>Represents an immutable snapshot of files at a point in time with optional metadata and tags.</p>"},{"location":"reference/api/#commit-notebook-integration","title":"Commit Notebook Integration","text":"<p>Commits also support rich HTML representation in notebooks. When you display a <code>Commit</code> object, you'll see:</p> <ul> <li>Commit Metadata: Hash, message, timestamp, and parent commit</li> <li>File List: All files in the commit with interactive access</li> <li>Copy Code to Access: Each file has a button that copies code including a   checkout step</li> </ul> <p>Example:</p> <pre><code>from kirin import Dataset\n\ndataset = Dataset(root_dir=\"/path/to/data\", name=\"my_dataset\")\ncommit = dataset.get_commit(commit_hash)\n\n# Display in notebook - shows interactive HTML\ncommit\n</code></pre> <p>Commit Code Snippets:</p> <p>When you click \"Copy Code to Access\" on a file in a commit, the code includes a checkout step:</p> <pre><code># Checkout this commit first\ndataset.checkout(\"commit_hash\")\n# Get path to local clone of file\nwith dataset.local_files() as files:\n    file_path = files[\"data.csv\"]\n</code></pre> <p>Note: Commits are frozen dataclasses, so you cannot set <code>_repr_variable_name</code> on them. Code snippets will use <code>\"dataset\"</code> as the default variable name.</p>"},{"location":"reference/api/#properties","title":"Properties","text":"<ul> <li><code>hash</code> (str): Unique commit identifier</li> <li><code>message</code> (str): Commit message</li> <li><code>timestamp</code> (datetime): When the commit was created</li> <li><code>parent_hash</code> (Optional[str]): Hash of the parent commit (None for initial commit)</li> <li><code>files</code> (Dict[str, File]): Dictionary of files in this commit</li> <li><code>metadata</code> (Dict[str, Any]): Metadata dictionary for model versioning</li> <li><code>tags</code> (List[str]): List of tags for staging/versioning</li> </ul>"},{"location":"reference/api/#methods","title":"Methods","text":"<ul> <li><code>get_file(name)</code> - Get a file by name</li> <li><code>list_files()</code> - List all file names</li> <li><code>has_file(name)</code> - Check if file exists</li> <li><code>get_file_count()</code> - Get number of files</li> <li><code>get_total_size()</code> - Get total size of all files</li> <li><code>to_dict()</code> - Convert to dictionary representation</li> <li><code>from_dict(data, storage)</code> - Create from dictionary</li> </ul>"},{"location":"reference/api/#commit-examples","title":"Commit Examples","text":"<pre><code># Access commit properties\ncommit = dataset.current_commit\nprint(f\"Commit: {commit.short_hash}\")\nprint(f\"Message: {commit.message}\")\nprint(f\"Files: {len(commit.files)}\")\nprint(f\"Metadata: {commit.metadata}\")\nprint(f\"Tags: {commit.tags}\")\n\n# Check if commit has specific metadata\nif commit.metadata.get(\"accuracy\", 0) &gt; 0.9:\n    print(\"High accuracy model!\")\n\n# Check if commit has specific tags\nif \"production\" in commit.tags:\n    print(\"Production model\")\n</code></pre>"},{"location":"reference/api/#commit-history","title":"Commit History","text":"<pre><code># Get commit history\nhistory = dataset.history(limit=10)\nfor commit in history:\n    print(f\"{commit.hash}: {commit.message}\")\n</code></pre>"},{"location":"reference/api/#file-operations","title":"File Operations","text":"<pre><code># Add files to commit\ndataset.commit(\"Add new data\", add_files=[\"new_data.csv\"])\n\n# Remove files from commit\ndataset.commit(\"Remove old data\", remove_files=[\"old_data.csv\"])\n\n# Combined operations\ndataset.commit(\"Update dataset\",\n              add_files=[\"new_data.csv\"],\n              remove_files=[\"old_data.csv\"])\n</code></pre>"},{"location":"reference/api/#cloud-storage-integration","title":"Cloud Storage Integration","text":"<pre><code># AWS S3 with profile\ndataset = Dataset(\n    root_dir=\"s3://my-bucket/data\",\n    name=\"my-dataset\",\n    aws_profile=\"production\"\n)\n\n# GCS with service account\ndataset = Dataset(\n    root_dir=\"gs://my-bucket/data\",\n    name=\"my-dataset\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"my-project\"\n)\n\n# Azure with connection string\ndataset = Dataset(\n    root_dir=\"az://my-container/data\",\n    name=\"my-dataset\",\n    azure_connection_string=os.getenv(\"AZURE_CONNECTION_STRING\")\n)\n</code></pre> <p>For detailed examples and cloud storage setup, see the Cloud Storage Authentication Guide.</p>"},{"location":"reference/storage-format/","title":"Storage Format","text":"<p>Technical details about Kirin's storage format and data structures.</p>"},{"location":"reference/storage-format/#overview","title":"Overview","text":"<p>Kirin uses a simplified Git-like storage format optimized for data versioning. The storage is organized into two main areas:</p> <ul> <li>Content Store: Content-addressed file storage</li> <li>Dataset Store: Commit history and metadata</li> </ul>"},{"location":"reference/storage-format/#storage-layout","title":"Storage Layout","text":"<pre><code>&lt;root&gt;/\n\u251c\u2500\u2500 data/                     # Content-addressed storage\n\u2502   \u251c\u2500\u2500 ab/                  # First two characters of hash\n\u2502   \u2502   \u2514\u2500\u2500 cdef1234...      # Rest of hash (no file extensions)\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 datasets/                 # Dataset storage\n    \u251c\u2500\u2500 dataset1/             # Dataset directory\n    \u2502   \u2514\u2500\u2500 commits.json       # Linear commit history\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"reference/storage-format/#content-store","title":"Content Store","text":""},{"location":"reference/storage-format/#file-storage","title":"File Storage","text":"<p>Files are stored in the content store using their content hash:</p> <p>Storage Path: <code>data/{hash[:2]}/{hash[2:]}</code></p> <p>Example:</p> <ul> <li>File hash: <code>abc123def456...</code></li> <li>Storage path: <code>data/ab/c123def456...</code></li> </ul>"},{"location":"reference/storage-format/#critical-design-extension-less-storage","title":"Critical Design: Extension-less Storage","text":"<p>Files are stored WITHOUT file extensions in the content store:</p> <ul> <li>Storage Path: <code>data/ab/cdef1234...</code> (no <code>.csv</code>, <code>.txt</code>, etc.)</li> <li>Original Extensions: Stored as metadata in the <code>File</code> entity's <code>name</code> attribute</li> <li>Extension Restoration: Original filenames restored when files are accessed</li> <li>Content Integrity: Files identified purely by content hash</li> <li>Deduplication: Identical content stored only once, regardless of original filename</li> </ul>"},{"location":"reference/storage-format/#benefits-of-extension-less-storage","title":"Benefits of Extension-less Storage","text":"<ol> <li>Content Integrity: Files identified by content, not filename</li> <li>Deduplication: Identical content stored once regardless of original name</li> <li>Tamper-proof: Any change to content changes the hash</li> <li>Efficient Storage: No duplicate storage for identical content</li> </ol>"},{"location":"reference/storage-format/#dataset-store","title":"Dataset Store","text":""},{"location":"reference/storage-format/#commit-history-format","title":"Commit History Format","text":"<p>Each dataset maintains a single JSON file with linear commit history:</p> <p>File: <code>datasets/{dataset_name}/commits.json</code></p> <pre><code>{\n  \"dataset_name\": \"my_dataset\",\n  \"commits\": [\n    {\n      \"hash\": \"abc123...\",\n      \"message\": \"Initial commit\",\n      \"timestamp\": \"2024-01-01T12:00:00\",\n      \"parent_hash\": null,\n      \"files\": {\n        \"data.csv\": {\n          \"hash\": \"def456...\",\n          \"name\": \"data.csv\",\n          \"size\": 1024,\n          \"content_type\": \"text/csv\"\n        }\n      }\n    },\n    {\n      \"hash\": \"ghi789...\",\n      \"message\": \"Add processed data\",\n      \"timestamp\": \"2024-01-01T13:00:00\",\n      \"parent_hash\": \"abc123...\",\n      \"files\": {\n        \"data.csv\": {\n          \"hash\": \"def456...\",\n          \"name\": \"data.csv\",\n          \"size\": 1024,\n          \"content_type\": \"text/csv\"\n        },\n        \"processed.csv\": {\n          \"hash\": \"jkl012...\",\n          \"name\": \"processed.csv\",\n          \"size\": 2048,\n          \"content_type\": \"text/csv\"\n        }\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"reference/storage-format/#commit-structure","title":"Commit Structure","text":"<p>Each commit contains:</p> <ul> <li>hash: SHA256 hash of the commit</li> <li>message: Human-readable commit message</li> <li>timestamp: ISO 8601 timestamp</li> <li>parent_hash: Hash of parent commit (null for first commit)</li> <li>files: Dictionary mapping filename to file metadata</li> </ul>"},{"location":"reference/storage-format/#file-metadata","title":"File Metadata","text":"<p>Each file entry contains:</p> <ul> <li>hash: Content hash of the file</li> <li>name: Original filename (including extension)</li> <li>size: File size in bytes</li> <li>content_type: MIME type of the file</li> </ul>"},{"location":"reference/storage-format/#data-structures","title":"Data Structures","text":""},{"location":"reference/storage-format/#file-entity","title":"File Entity","text":"<pre><code>@dataclass(frozen=True)\nclass File:\n    \"\"\"Represents a versioned file with content-addressed storage.\"\"\"\n\n    hash: str                    # Content hash (SHA256)\n    name: str                   # Original filename\n    size: int                   # File size in bytes\n    content_type: Optional[str] = None  # MIME type\n\n    def read_bytes(self) -&gt; bytes: ...\n    def open(self, mode: str = \"rb\") -&gt; Union[BinaryIO, TextIO]: ...\n    def download_to(self, path: Union[str, Path]) -&gt; str: ...\n    def exists(self) -&gt; bool: ...\n    def to_dict(self) -&gt; dict: ...\n</code></pre>"},{"location":"reference/storage-format/#commit-entity","title":"Commit Entity","text":"<pre><code>@dataclass(frozen=True)\nclass Commit:\n    \"\"\"Represents an immutable snapshot of files at a point in time.\"\"\"\n\n    hash: str                           # Commit hash\n    message: str                        # Commit message\n    timestamp: datetime                  # Creation timestamp\n    parent_hash: Optional[str]          # Parent commit hash\n    files: Dict[str, File]             # filename -&gt; File mapping\n\n    def get_file(self, name: str) -&gt; Optional[File]: ...\n    def list_files(self) -&gt; List[str]: ...\n    def has_file(self, name: str) -&gt; bool: ...\n    def get_total_size(self) -&gt; int: ...\n</code></pre>"},{"location":"reference/storage-format/#dataset-entity","title":"Dataset Entity","text":"<pre><code>class Dataset:\n    \"\"\"Represents a logical collection of files with linear history.\"\"\"\n\n    def __init__(self, root_dir: Union[str, Path], name: str,\n                 description: str = \"\",\n                 fs: Optional[fsspec.AbstractFileSystem] = None,\n                 # AWS/S3 authentication\n                 aws_profile: Optional[str] = None,\n                 # GCP/GCS authentication\n                 gcs_token: Optional[Union[str, Path]] = None,\n                 gcs_project: Optional[str] = None,\n                 # Azure authentication\n                 azure_account_name: Optional[str] = None,\n                 azure_account_key: Optional[str] = None,\n                 azure_connection_string: Optional[str] = None): ...\n\n    def commit(self, message: str, add_files: List[Union[str, Path]] = None,\n               remove_files: List[str] = None) -&gt; str: ...\n    def checkout(self, commit_hash: Optional[str] = None) -&gt; None: ...\n    def get_file(self, name: str) -&gt; Optional[File]: ...\n    def list_files(self) -&gt; List[str]: ...\n    def has_file(self, name: str) -&gt; bool: ...\n    def read_file(self, name: str, mode: str = \"r\") -&gt; Union[str, bytes]: ...\n    def download_file(self, name: str, target_path: Union[str, Path]) -&gt; str: ...\n    def open_file(self, name: str, mode: str = \"rb\") -&gt; Union[BinaryIO,\n                                                               TextIO]: ...\n    def local_files(self): ...  # Context manager for local file access\n    def history(self, limit: Optional[int] = None) -&gt; List[Commit]: ...\n    def get_commit(self, commit_hash: str) -&gt; Optional[Commit]: ...\n    def get_commits(self) -&gt; List[Commit]: ...\n    def is_empty(self) -&gt; bool: ...\n    def cleanup_orphaned_files(self) -&gt; int: ...\n    def get_info(self) -&gt; dict: ...\n    def to_dict(self) -&gt; dict: ...\n\n    # Properties\n    @property\n    def current_commit(self) -&gt; Optional[Commit]: ...\n    @property\n    def head(self) -&gt; Optional[Commit]: ...  # Alias for current_commit\n    @property\n    def files(self) -&gt; Dict[str, File]: ...  # Files from current commit\n</code></pre>"},{"location":"reference/storage-format/#catalog-entity","title":"Catalog Entity","text":"<pre><code>@dataclass\nclass Catalog:\n    \"\"\"Represents a collection of datasets.\"\"\"\n\n    root_dir: Union[str, fsspec.AbstractFileSystem]\n    fs: Optional[fsspec.AbstractFileSystem] = None\n    # AWS/S3 authentication\n    aws_profile: Optional[str] = None\n    # GCP/GCS authentication\n    gcs_token: Optional[Union[str, Path]] = None\n    gcs_project: Optional[str] = None\n    # Azure authentication\n    azure_account_name: Optional[str] = None\n    azure_account_key: Optional[str] = None\n    azure_connection_string: Optional[str] = None\n\n    def datasets(self) -&gt; List[str]: ...  # List dataset names\n    def get_dataset(self, dataset_name: str) -&gt; Dataset: ...  # Get existing dataset\n    def create_dataset(self, dataset_name: str,\n                       description: str = \"\") -&gt; Dataset: ...  # Create new dataset\n    def __len__(self) -&gt; int: ...  # Number of datasets\n</code></pre>"},{"location":"reference/storage-format/#content-addressing","title":"Content Addressing","text":""},{"location":"reference/storage-format/#hash-calculation","title":"Hash Calculation","text":"<p>Files are hashed using SHA256 directly on content bytes:</p> <pre><code>import hashlib\n\ndef calculate_hash(content: bytes) -&gt; str:\n    \"\"\"Calculate SHA256 hash of content bytes.\"\"\"\n    return hashlib.sha256(content).hexdigest()\n\n# Example usage in storage\ndef store_file(file_path: Path) -&gt; str:\n    with open(file_path, \"rb\") as f:\n        content = f.read()\n    return hashlib.sha256(content).hexdigest()\n</code></pre>"},{"location":"reference/storage-format/#deduplication","title":"Deduplication","text":"<p>Identical content is stored only once:</p> <pre><code># Two files with identical content\nfile1_content = b\"Hello, World!\"\nfile2_content = b\"Hello, World!\"\n\n# Both files get the same hash\nhash1 = hashlib.sha256(file1_content).hexdigest()\nhash2 = hashlib.sha256(file2_content).hexdigest()\n\nassert hash1 == hash2  # Same hash = same storage location\n</code></pre>"},{"location":"reference/storage-format/#content-integrity","title":"Content Integrity","text":"<p>Any change to file content changes the hash:</p> <pre><code># Original content\ncontent1 = b\"Hello, World!\"\nhash1 = hashlib.sha256(content1).hexdigest()\n\n# Modified content\ncontent2 = b\"Hello, World!\"  # Even a single character change\nhash2 = hashlib.sha256(content2).hexdigest()\n\nassert hash1 != hash2  # Different hash = different storage location\n</code></pre>"},{"location":"reference/storage-format/#commit-hash-generation","title":"Commit Hash Generation","text":"<p>Commit hashes are generated using file hashes, message, and timestamp:</p> <pre><code>def generate_commit_hash(files: Dict[str, File], message: str,\n                        parent_hash: Optional[str],\n                        timestamp: datetime) -&gt; str:\n    \"\"\"Generate commit hash from file hashes, message, and timestamp.\"\"\"\n    import hashlib\n\n    # Sort file hashes for consistency\n    file_hashes = sorted(file.hash for file in files.values())\n    parent_hash = parent_hash or \"\"\n\n    # Combine all components\n    content = (\n        \"\\n\".join(file_hashes) + \"\\n\" +\n        message + \"\\n\" +\n        parent_hash + \"\\n\" +\n        str(timestamp)\n    )\n\n    # Generate hash\n    hasher = hashlib.sha256()\n    hasher.update(content.encode(\"utf-8\"))\n    return hasher.hexdigest()\n</code></pre>"},{"location":"reference/storage-format/#backend-integration","title":"Backend Integration","text":""},{"location":"reference/storage-format/#fsspec-backends","title":"FSSpec Backends","text":"<p>Kirin supports any fsspec backend:</p> <pre><code># Local filesystem\nfs = fsspec.filesystem(\"file\")\n\n# S3\nfs = fsspec.filesystem(\"s3\", profile=\"my-profile\")\n\n# GCS\nfs = fsspec.filesystem(\"gcs\", token=\"/path/to/key.json\")\n\n# Azure\nfs = fsspec.filesystem(\"az\", connection_string=\"...\")\n</code></pre>"},{"location":"reference/storage-format/#storage-operations","title":"Storage Operations","text":"<pre><code># Store file content\ndef store_file(fs, file_path: Path) -&gt; str:\n    \"\"\"Store file and return content hash.\"\"\"\n    with open(file_path, \"rb\") as f:\n        content = f.read()\n\n    hash_value = hashlib.sha256(content).hexdigest()\n    storage_path = f\"data/{hash_value[:2]}/{hash_value[2:]}\"\n\n    fs.write_bytes(storage_path, content)\n    return hash_value\n\n# Retrieve file content\ndef retrieve_file(fs, hash_value: str) -&gt; bytes:\n    \"\"\"Retrieve file content by hash.\"\"\"\n    storage_path = f\"data/{hash_value[:2]}/{hash_value[2:]}\"\n    return fs.read_bytes(storage_path)\n</code></pre>"},{"location":"reference/storage-format/#performance-considerations","title":"Performance Considerations","text":""},{"location":"reference/storage-format/#zero-copy-operations","title":"Zero-Copy Operations","text":"<p>Kirin is designed with zero-copy philosophy:</p> <ul> <li>Reference-based operations: Use File objects as references instead of   copying content</li> <li>Lazy loading: File content is only downloaded when accessed</li> <li>Deduplication: Identical content is stored only once regardless of filename</li> </ul>"},{"location":"reference/storage-format/#caching","title":"Caching","text":"<p>Kirin implements commit-level caching for improved performance:</p> <pre><code># Commit objects are cached in memory\nclass CommitStore:\n    def __init__(self):\n        self._commits_cache: Dict[str, Commit] = {}\n\n    def get_commit(self, commit_hash: str) -&gt; Commit:\n        # Try cache first\n        if commit_hash in self._commits_cache:\n            return self._commits_cache[commit_hash]\n        # Load from storage if not cached\n        # ...\n</code></pre>"},{"location":"reference/storage-format/#lazy-loading","title":"Lazy Loading","text":"<p>Content loaded only when needed:</p> <pre><code># Lazy file loading\nclass LazyFile:\n    def __init__(self, fs, hash_value: str, name: str):\n        self.fs = fs\n        self.hash_value = hash_value\n        self.name = name\n        self._content = None\n\n    def read_bytes(self) -&gt; bytes:\n        if self._content is None:\n            self._content = retrieve_file(self.fs, self.hash_value)\n        return self._content\n</code></pre>"},{"location":"reference/storage-format/#migration-and-backup","title":"Migration and Backup","text":""},{"location":"reference/storage-format/#backup-strategies","title":"Backup Strategies","text":"<pre><code># Backup content store\nrsync -av data/ backup/data/\n\n# Backup dataset metadata\nrsync -av datasets/ backup/datasets/\n</code></pre>"},{"location":"reference/storage-format/#migration-between-backends","title":"Migration Between Backends","text":"<pre><code># Migrate from local to S3\nlocal_fs = fsspec.filesystem(\"file\")\ns3_fs = fsspec.filesystem(\"s3\", profile=\"my-profile\")\n\n# Copy content store\nfor root, dirs, files in os.walk(\"data\"):\n    for file in files:\n        local_path = os.path.join(root, file)\n        s3_path = f\"s3://my-bucket/{local_path}\"\n        s3_fs.put(local_path, s3_path)\n</code></pre>"},{"location":"reference/storage-format/#security-considerations","title":"Security Considerations","text":""},{"location":"reference/storage-format/#access-control","title":"Access Control","text":"<ul> <li>File permissions: Respect filesystem permissions</li> <li>Cloud IAM: Use appropriate cloud permissions</li> <li>Encryption: Support for encrypted storage backends</li> </ul>"},{"location":"reference/storage-format/#data-integrity","title":"Data Integrity","text":"<ul> <li>Hash verification: Verify content hashes on retrieval</li> <li>Tamper detection: Detect any content changes</li> <li>Audit trails: Track all storage operations</li> </ul>"},{"location":"reference/storage-format/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Architecture Overview - System architecture</li> </ul>"},{"location":"releases/v0.0.2/","title":"V0.0.2","text":""},{"location":"releases/v0.0.2/#version-002","title":"Version 0.0.2","text":"<p>This release introduces major new features for cloud storage support, a FastAPI-based web UI, improved authentication, and significant enhancements to dataset management and usability. The project now supports content-addressed storage with visible filenames, lazy file loading, and a more robust, cloud-agnostic architecture. Numerous improvements to documentation, testing, and CI/CD workflows are also included.</p>"},{"location":"releases/v0.0.2/#new-features","title":"New Features","text":"<ul> <li>Add FastAPI-based web UI for dataset, file, and commit management, with shadcn/ui-inspired design and HTMX partials. Includes CRUD operations, commit history visualization, and file preview. (ee5e7f, Eric Ma)</li> <li>Add cloud authentication helpers for S3, GCS, Azure, and S3-compatible services, enabling seamless local and cloud storage via fsspec. (ee5e7f, Eric Ma)</li> <li>Refactor Dataset and DatasetCommit to support fsspec filesystems, allowing all file operations to work with local and cloud storage. (ee5e7f, Eric Ma)</li> <li>Implement content-addressed storage with visible filenames, supporting deduplication and automatic migration from old formats. (89c920, Eric Ma)</li> <li>Add cloud-agnostic authentication parameters to Catalog, Dataset, and web UI, supporting S3, GCS, and Azure authentication. (9a2afb, Eric Ma)</li> <li>Add AWS profile selection and lazy directory creation for S3 compatibility, with improved error handling and unified catalog forms in the web UI. (b948ec, Eric Ma)</li> <li>Add secure cloud auth detection, keyring credential storage, and improved backend authentication UX, including CLI and web UI enhancements. (4dd1d2, Eric Ma)</li> <li>Implement lazy loading for local_files context manager in Dataset, introducing LazyLocalFiles for efficient file access and caching. (365b7d, Eric Ma)</li> <li>Add initial implementation of Data Catalog for managing collections of datasets, with methods for listing, retrieving, and creating datasets. (c1fc76, Eric Ma)</li> <li>Add dataset length functionality to Catalog and shared testing utilities for improved test coverage. (2bdbe3, Eric Ma)</li> <li>Add local file access context manager, partial hash resolution for Dataset.checkout, and improved file utilities for easier file operations. (637ac7, Eric Ma)</li> <li>Add search functionality to filter datasets by name or description in the web UI. (9acdeb, Eric Ma)</li> <li>Add tabbed interface for datasets and dataset creation, with improved responsive grid and utility classes in the web UI. (7c2750, Eric Ma)</li> <li>Add Python code snippets to web UI, including cloud authentication parameters and copy-to-clipboard functionality. (16857a, Eric Ma)</li> <li>Add rebase merge strategy for linear commit history, with updated visualization and Marimo notebook demo. (852f14, Eric Ma)</li> <li>Add interactive commit tree view with Mermaid.js and view toggle to dataset UI. (d1d845, Eric Ma)</li> <li>Add comprehensive regression tests for Git semantics and rebase merge workflow. (c5bce5, Eric Ma)</li> <li>Add debugging and testing scripts for commit structure and git semantics. (81e710, Eric Ma)</li> <li>Add detailed documentation for design, architecture, and best practices, including AGENTS.md, guides, and expanded README. (9ba556, Eric Ma; 8f2a62, Eric Ma)</li> <li>Add mkdocstrings and related dependencies for improved documentation generation. (4d5e32, Eric Ma)</li> </ul>"},{"location":"releases/v0.0.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fix file download endpoint in web UI and update integration tests to match new API routes. (c2083c, Eric Ma)</li> <li>Show correct commit counts for datasets on catalog landing page, with tests and documentation. (95082f, Eric Ma)</li> <li>Fix markdownlint compliance across all documentation files. (07b606, Eric Ma)</li> </ul>"},{"location":"releases/v0.0.2/#deprecations","title":"Deprecations","text":"<ul> <li>BREAKING CHANGE: Dataset and DatasetCommit now require fsspec filesystem compatibility; all file operations are routed through fsspec. Users must install appropriate cloud dependencies (s3fs, gcsfs, adlfs, etc.) for cloud storage support. (ee5e7f, Eric Ma)</li> <li>BREAKING CHANGE: The prototype notebook is replaced by new local and GCP demo notebooks. The new web UI is now the primary interface for dataset management. Old dataset creation patterns may need to be updated. (e80d04, Eric Ma)</li> <li>BREAKING CHANGE: Branching, merging, and web UI are no longer supported. All history is now linear-only. (5cdf91, Eric Ma)</li> <li>BREAKING CHANGE: All previous 'backend' routes, forms, and configuration are now replaced by 'catalog' equivalents in the web UI and API. (60c5cc, Eric Ma)</li> </ul> <p>This release marks a significant step forward in making Kirin a robust, cloud-ready, and user-friendly data versioning platform. Please review the breaking changes and update your usage patterns and dependencies as needed.</p>"},{"location":"releases/v0.0.3/","title":"V0.0.3","text":""},{"location":"releases/v0.0.3/#version-003","title":"Version 0.0.3","text":"<p>This release introduces a new CLI upload command, streamlines pre-commit hook configurations, and removes redundant tests for improved maintainability.</p>"},{"location":"releases/v0.0.3/#new-features","title":"New Features","text":"<ul> <li>Added a new upload command to the CLI, allowing users to upload files to a dataset in a catalog with a commit message. This includes argument parsing, validation, and comprehensive tests. (b73f84) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.3/#bug-fixes","title":"Bug Fixes","text":"<p>No bug fixes in this release.</p>"},{"location":"releases/v0.0.3/#deprecations","title":"Deprecations","text":"<p>No deprecations in this release.</p>"},{"location":"releases/v0.0.3/#other-changes","title":"Other Changes","text":"<ul> <li>Excluded release notes markdown files from pre-commit hooks to prevent unnecessary checks. (f7ffab) (Eric Ma)</li> <li>Updated and streamlined pre-commit hook exclude patterns for better maintainability. (016aa6) (Eric Ma)</li> <li>Removed a redundant test for the upload command help output. (ef0320) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.4/","title":"V0.0.4","text":""},{"location":"releases/v0.0.4/#version-004","title":"Version 0.0.4","text":"<p>This release includes a bug fix to the CLI entry point, ensuring the correct function is used when launching the application.</p>"},{"location":"releases/v0.0.4/#new-features","title":"New Features","text":"<p>No new features were added in this release.</p>"},{"location":"releases/v0.0.4/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated the CLI entry point in pyproject.toml to use kirin.cli:app instead of kirin.cli:main, ensuring the CLI launches correctly. (3ebdf7) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.4/#deprecations","title":"Deprecations","text":"<p>No deprecations were introduced in this release.</p>"},{"location":"releases/v0.0.5/","title":"V0.0.5","text":""},{"location":"releases/v0.0.5/#version-005","title":"Version 0.0.5","text":"<p>This release introduces automatic authentication and timeout protection for cloud catalog access in the web UI, along with improved documentation and testing for these new features. Users can now configure authentication commands for cloud providers, and the UI will automatically attempt to authenticate and retry operations if needed, providing a smoother and more secure experience.</p>"},{"location":"releases/v0.0.5/#new-features","title":"New Features","text":"<ul> <li>Added automatic authentication and timeout protection for cloud catalog access. If a catalog operation fails due to authentication or timeout, the system will attempt to authenticate using a configurable CLI command and retry the operation. This includes improved error handling and user feedback in the web UI, as well as timeout protection for dataset operations to prevent UI hangs. (68fd97) (Eric Ma)</li> <li>Documented the new web UI auto-authentication and timeout features, including configuration instructions for AWS, GCP, and Azure, security considerations, and troubleshooting guidance. Also added fast unit tests for authentication timeout and auto-execution behaviors. (bc4a92) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.5/#bug-fixes","title":"Bug Fixes","text":"<p>No bug fixes in this release.</p>"},{"location":"releases/v0.0.5/#deprecations","title":"Deprecations","text":"<p>No deprecations in this release.</p>"},{"location":"releases/v0.0.6/","title":"V0.0.6","text":""},{"location":"releases/v0.0.6/#version-006","title":"Version 0.0.6","text":"<p>This release introduces major new features for model and plot versioning, significant improvements to dependency management, and several bug fixes to enhance reliability and developer experience. The update also includes UI enhancements for authentication and code quality improvements throughout the codebase.</p>"},{"location":"releases/v0.0.6/#new-features","title":"New Features","text":"<ul> <li>Model Versioning Support: Added support for model versioning by extending commits and datasets to include metadata and tags. New methods for querying and comparing model versions are available, along with updated documentation and a demo notebook. All references from \"gitdata\" to \"kirin\" have been updated. (04b970) (Eric Ma)</li> <li>Plot Versioning with SVG/WebP and Thumbnails: Introduced plot saving functionality with automatic format detection (SVG for vector, WebP for raster), support for matplotlib and plotly, thumbnail generation and storage, and integration with the web UI for image previews. Comprehensive tests and documentation updates included. (29fbb4) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.6/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>/tmp/ File Filtering in CI: Fixed an issue where test files in <code>/tmp/</code> were incorrectly filtered out during CI runs, causing test failures. Now, only IPython-related temporary files are skipped. (a186a6) (Eric Ma)</li> <li>Pytest and Coverage in Pixi Environments: Added <code>pytest</code> and <code>pytest-cov</code> to the pixi test dependencies to fix failing CI jobs due to missing packages. (e08cda) (Eric Ma)</li> <li>Catalog Auto-Authentication: Improved catalog authentication by adding a dedicated endpoint, proactive authentication before listing datasets, and better error detection for expired tokens. The UI now allows users to manually trigger authentication. (a388ac) (Eric Ma)</li> <li>Erroneous Notebook Cell Removed: Deleted an unintended code cell from a prototype notebook to clean up the codebase. (2a698c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.6/#improvements-refactoring","title":"Improvements &amp; Refactoring","text":"<ul> <li>Pixi-First Dependency Management: Reorganized dependencies to follow a pixi-first approach, unpinning versions for flexibility, removing duplication, and improving documentation for dependency management. (1bd03f) (Eric Ma)</li> <li>Plot Source Linking Code Quality: Refactored plot source linking logic for better organization, type hints, error handling, and removed dead code. Tests and documentation were updated accordingly. (5c1245) (Eric Ma)</li> <li>Code Formatting: Cleaned up code formatting by removing unnecessary blank lines and condensing function calls for improved readability. (16740b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.6/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul> <p>Contributors: Eric Ma, dependabot[bot], github-actions</p>"},{"location":"releases/v0.0.7/","title":"V0.0.7","text":""},{"location":"releases/v0.0.7/#version-007","title":"Version 0.0.7","text":"<p>This release introduces major improvements to the file preview experience in the Web UI, including commit navigation for all file types, enhanced error handling, and comprehensive documentation updates. Several bugs affecting file previews and documentation formatting have also been addressed.</p>"},{"location":"releases/v0.0.7/#new-features","title":"New Features","text":"<ul> <li>Added commit navigation for all files in the preview modal, including Previous/Next buttons, contextual labels (Latest, Oldest, Commit X of Y), and support for all file types (not just images). This includes a new backend endpoint for file commit history, commit history caching, race condition protection, and comprehensive test coverage. (5275cd) (Eric Ma)</li> <li>Added comprehensive Web UI documentation, including new \"Getting Started\" and \"How-To Guide\" pages, expanded overview, and improved navigation and quickstart instructions. (5d5977) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.7/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed a bug where text file previews would incorrectly show an empty image; text files now preview correctly. (5275cd) (Eric Ma)</li> <li>Fixed commit navigation so it works for all files, not just images, and improved error handling for invalid checkout hashes. (5275cd) (Eric Ma)</li> <li>Fixed trailing whitespace in the contributors section of the v0.0.6 release notes. (c6621e) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.7/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"web-ui/catalog-management/","title":"Catalog Management","text":"<p>Configuration and management of data catalogs in the web UI.</p>"},{"location":"web-ui/catalog-management/#catalog-configuration","title":"Catalog Configuration","text":""},{"location":"web-ui/catalog-management/#local-storage-catalogs","title":"Local Storage Catalogs","text":"<p>For local filesystem storage:</p> <p>Benefits:</p> <ul> <li>Simple setup: No authentication required</li> <li>Fast access: Direct filesystem access</li> <li>Easy backup: Standard file operations</li> <li>No costs: No cloud storage fees</li> </ul>"},{"location":"web-ui/catalog-management/#cloud-storage-catalogs","title":"Cloud Storage Catalogs","text":""},{"location":"web-ui/catalog-management/#aws-s3-configuration","title":"AWS S3 Configuration","text":"<p>Setup Steps:</p> <ol> <li>Configure AWS credentials in your environment</li> <li>Create S3 bucket with appropriate permissions</li> <li>Set up IAM policies for bucket access</li> <li>Test connection in the web UI</li> </ol> <p>Web UI Configuration:</p> <ul> <li>Catalog Name: Friendly name for the catalog</li> <li>Root Directory: S3 URL (e.g., <code>s3://my-bucket/data</code>)</li> <li>AWS Profile: Optional AWS profile name (auto-detected from environment)</li> <li>Authentication Command: Optional CLI command for automatic authentication</li> </ul>"},{"location":"web-ui/catalog-management/#google-cloud-storage-configuration","title":"Google Cloud Storage Configuration","text":"<p>Setup Steps:</p> <ol> <li>Authenticate with GCP using <code>gcloud auth application-default login</code></li> <li>Create bucket in GCP console</li> <li>Set up bucket permissions</li> <li>Test connection in the web UI</li> </ol> <p>Web UI Configuration:</p> <ul> <li>Catalog Name: Friendly name for the catalog</li> <li>Root Directory: GCS URL (e.g., <code>gs://my-bucket/data</code>)</li> <li>Authentication Command: Optional CLI command for automatic authentication</li> </ul>"},{"location":"web-ui/catalog-management/#azure-blob-storage-configuration","title":"Azure Blob Storage Configuration","text":"<p>Setup Steps:</p> <ol> <li>Authenticate with Azure using <code>az login</code></li> <li>Create storage account in Azure portal</li> <li>Create container for data storage</li> <li>Test connection in the web UI</li> </ol> <p>Web UI Configuration:</p> <ul> <li>Catalog Name: Friendly name for the catalog</li> <li>Root Directory: Azure URL (e.g., <code>az://my-container/data</code>)</li> <li>Authentication Command: Optional CLI command for automatic authentication</li> </ul>"},{"location":"web-ui/catalog-management/#authentication-command-auto-execution","title":"Authentication Command Auto-Execution","text":"<p>NEW: Kirin's web UI can automatically execute authentication commands when accessing catalogs, eliminating the need for manual CLI authentication.</p> <p>How It Works:</p> <ol> <li>Store your authentication command in the catalog configuration (e.g.,    <code>aws sso login --profile my-profile</code>)</li> <li>When authentication fails, Kirin automatically executes the command</li> <li>If successful, Kirin retries the operation and shows your datasets</li> <li>If it fails, you'll see clear error messages with manual instructions</li> </ol> <p>Setup:</p> <p>When adding or editing a catalog, fill in the \"Authentication Command (Optional)\" field:</p> <ul> <li>AWS S3: <code>aws sso login --profile {{ aws_profile }}</code></li> <li>GCP GCS: <code>gcloud auth login</code></li> <li>Azure: <code>az login</code></li> </ul> <p>Benefits:</p> <ul> <li>Automatic retry: No manual intervention needed</li> <li>Seamless UX: Authentication happens in the background</li> <li>Clear feedback: Success/failure messages shown in UI</li> <li>Manual fallback: You can still authenticate manually if auto-auth fails</li> </ul> <p>Example:</p> <pre><code>Catalog: production-s3\nRoot Directory: s3://my-production-bucket/data\nAWS Profile: production\nAuth Command: aws sso login --profile production\n\nResult: When you click on this catalog, Kirin will automatically\nrun the auth command if authentication is needed.\n</code></pre>"},{"location":"web-ui/catalog-management/#multi-catalog-workflows","title":"Multi-Catalog Workflows","text":""},{"location":"web-ui/catalog-management/#organizing-catalogs","title":"Organizing Catalogs","text":"<p>Structure your catalogs by purpose:</p> <pre><code>s3-production: s3://production-bucket/data\ngcs-production: gs://production-bucket/data\n\n# Analytics catalogs\ngcs-analytics: gs://analytics-bucket/data\nazure-ml: az://ml-container/data\n</code></pre>"},{"location":"web-ui/catalog-management/#working-with-multiple-catalogs","title":"Working with Multiple Catalogs","text":"<p>The web UI allows you to manage multiple catalogs from a single interface:</p> <ol> <li>Add multiple catalogs using the \"Add Catalog\" button</li> <li>Switch between catalogs by clicking on catalog cards</li> <li>View datasets in each catalog separately</li> <li>Edit or delete catalogs as needed</li> </ol> <p>Each catalog maintains its own datasets and commit history independently.</p>"},{"location":"web-ui/catalog-management/#authentication-management","title":"Authentication Management","text":""},{"location":"web-ui/catalog-management/#aws-authentication","title":"AWS Authentication","text":""},{"location":"web-ui/catalog-management/#using-aws-profiles","title":"Using AWS Profiles","text":"<pre><code># Configure AWS profile\naws configure --profile production\naws configure --profile development\n\n# Use in web UI: Select profile from dropdown when creating S3 catalog\n</code></pre>"},{"location":"web-ui/catalog-management/#using-environment-variables","title":"Using Environment Variables","text":"<pre><code># Set environment variables\nexport AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\nexport AWS_DEFAULT_REGION=\"us-west-2\"\n\n# Use in web UI: Leave AWS Profile empty to use environment variables\n</code></pre>"},{"location":"web-ui/catalog-management/#using-iam-roles","title":"Using IAM Roles","text":"<p>For AWS EC2, ECS, or Lambda instances that have an associated IAM role, no explicit credentials are required. The web UI will automatically use the IAM role provided to the instance for authentication.</p>"},{"location":"web-ui/catalog-management/#gcp-authentication","title":"GCP Authentication","text":""},{"location":"web-ui/catalog-management/#application-default-credentials-recommended","title":"Application Default Credentials (Recommended)","text":"<pre><code># Set up ADC (one-time setup)\ngcloud auth application-default login\n\n# Use in web UI: No additional configuration needed\n# Root Directory: gs://my-bucket/data\n</code></pre>"},{"location":"web-ui/catalog-management/#service-account-keys-advanced","title":"Service Account Keys (Advanced)","text":"<pre><code># Create service account\ngcloud iam service-accounts create kirin-service\n\n# Download key file\ngcloud iam service-accounts keys create key.json \\\n  --iam-account=kirin-service@project.iam.gserviceaccount.com\n\n# Set environment variable\nexport GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/key.json\"\n\n# Use in web UI: No additional configuration needed\n</code></pre>"},{"location":"web-ui/catalog-management/#azure-authentication","title":"Azure Authentication","text":""},{"location":"web-ui/catalog-management/#azure-cli-recommended","title":"Azure CLI (Recommended)","text":"<pre><code># Authenticate with Azure\naz login\n\n# Use in web UI: No additional configuration needed\n# Root Directory: az://my-container/data\n</code></pre>"},{"location":"web-ui/catalog-management/#connection-string-advanced","title":"Connection String (Advanced)","text":"<pre><code># Set environment variable\nexport AZURE_STORAGE_CONNECTION_STRING=\"DefaultEndpointsProtocol=https;AccountName=myaccount;AccountKey=mykey\"\n\n# Use in web UI: No additional configuration needed\n</code></pre>"},{"location":"web-ui/catalog-management/#security-best-practices","title":"Security Best Practices","text":""},{"location":"web-ui/catalog-management/#credential-management","title":"Credential Management","text":""},{"location":"web-ui/catalog-management/#secure-storage","title":"Secure Storage","text":"<ul> <li>Use IAM roles when possible instead of access keys</li> <li>Rotate credentials regularly</li> <li>Use least privilege - only grant necessary permissions</li> <li>Monitor access through cloud provider audit logs</li> </ul>"},{"location":"web-ui/catalog-management/#environment-variables","title":"Environment Variables","text":"<pre><code># Set environment variables for sensitive data\nexport AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\nexport GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/key.json\"\nexport AZURE_STORAGE_ACCOUNT_NAME=\"myaccount\"\nexport AZURE_STORAGE_ACCOUNT_KEY=\"mykey\"\n</code></pre>"},{"location":"web-ui/catalog-management/#access-control","title":"Access Control","text":""},{"location":"web-ui/catalog-management/#aws-s3-permissions","title":"AWS S3 Permissions","text":"<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::{{ account_id }}:user/{{ username }}\"\n      },\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::{{ bucket_name }}\",\n        \"arn:aws:s3:::{{ bucket_name }}/*\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"web-ui/catalog-management/#gcp-iam-permissions","title":"GCP IAM Permissions","text":"<p>For Google Cloud Storage, ensure your service account or user has the following roles:</p> <ul> <li>Storage Object Admin: For full read/write access to objects</li> <li>Storage Legacy Bucket Reader: For listing bucket contents</li> </ul>"},{"location":"web-ui/catalog-management/#troubleshooting","title":"Troubleshooting","text":""},{"location":"web-ui/catalog-management/#common-issues","title":"Common Issues","text":""},{"location":"web-ui/catalog-management/#authentication-failures","title":"Authentication Failures","text":"<pre><code># Test AWS credentials\naws s3 ls s3://{{ bucket_name }}\n\n# Test GCS credentials\ngsutil ls gs://{{ bucket_name }}\n\n# Test Azure credentials\naz storage blob list --container-name {{ container_name }}\n</code></pre>"},{"location":"web-ui/catalog-management/#permission-errors","title":"Permission Errors","text":"<pre><code># Check S3 bucket permissions\naws s3api get-bucket-policy --bucket my-bucket\n\n# Check GCS bucket permissions\ngsutil iam get gs://my-bucket\n\n# Check Azure container permissions\naz storage container show --name my-container\n</code></pre>"},{"location":"web-ui/catalog-management/#connection-issues","title":"Connection Issues","text":"<pre><code># Test network connectivity\nping s3.amazonaws.com\nping storage.googleapis.com\nping blob.core.windows.net\n\n# Check DNS resolution\nnslookup s3.amazonaws.com\nnslookup storage.googleapis.com\n</code></pre>"},{"location":"web-ui/catalog-management/#performance-issues","title":"Performance Issues","text":""},{"location":"web-ui/catalog-management/#slow-operations","title":"Slow Operations","text":"<pre><code># Use appropriate file sizes\n# Split large files into chunks\ndataset.commit(\n    message=\"Add chunked data\",\n    add_files=[\"chunk_001.csv\", \"chunk_002.csv\", \"chunk_003.csv\"]\n)\n\n# Use compression for text files\ndataset.commit(\n    message=\"Add compressed data\",\n    add_files=[\"data.csv.gz\"]\n)\n</code></pre>"},{"location":"web-ui/catalog-management/#memory-issues","title":"Memory Issues","text":"<pre><code># Use chunked processing for large files\nwith dataset.local_files() as local_files:\n    for filename, local_path in local_files.items():\n        if filename.endswith('.csv'):\n            # Process in chunks\n            for chunk in pd.read_csv(local_path, chunksize=10000):\n                process_chunk(chunk)\n</code></pre>"},{"location":"web-ui/catalog-management/#next-steps","title":"Next Steps","text":"<ul> <li>Web UI Overview - Getting started with the web interface</li> <li>Cloud Storage Guide - Detailed cloud setup</li> <li>Basic Usage Guide - Core dataset operations</li> </ul>"},{"location":"web-ui/getting-started/","title":"Getting Started with the Web UI","text":"<p>This tutorial walks you through creating your first catalog and dataset using the Kirin web interface. By the end, you'll have a working setup and understand the basic workflow.</p>"},{"location":"web-ui/getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kirin installed (see Installation Guide)</li> <li>A storage location ready (local directory or cloud bucket)</li> <li>For cloud storage: Appropriate authentication configured</li> </ul>"},{"location":"web-ui/getting-started/#step-1-start-the-web-ui","title":"Step 1: Start the Web UI","text":"<p>Open your terminal and run:</p> <pre><code># Development environment\npixi run kirin ui\n\n# Production environment\nuv run kirin ui\n\n# One-time use\nuvx kirin ui\n</code></pre> <p>The server starts and prints a URL like:</p> <pre><code>Starting Kirin Web UI...\nServer running at http://127.0.0.1:61581\n</code></pre> <p>Copy this URL and open it in your browser.</p>"},{"location":"web-ui/getting-started/#step-2-create-your-first-catalog","title":"Step 2: Create Your First Catalog","text":"<p>A catalog connects Kirin to a storage location. You need at least one catalog before you can work with datasets.</p> <p>On the home page:</p> <ol> <li>Click the blue \"+ Add Catalog\" button</li> <li>Fill in the form:</li> <li>Catalog Name: Enter a friendly name (e.g., \"My Data Catalog\")</li> <li>Root Directory: Enter your storage path:<ul> <li>Local: <code>/path/to/your/data</code></li> <li>S3: <code>s3://your-bucket-name/path</code></li> <li>GCS: <code>gs://your-bucket-name/path</code></li> <li>Azure: <code>az://your-container-name/path</code></li> </ul> </li> <li>For cloud storage (optional):</li> <li>AWS Profile: Select from dropdown if using S3</li> <li>Authentication Command: Enter CLI command (e.g., <code>aws sso login      --profile production</code>)</li> <li>Click \"Create Catalog\"</li> </ol> <p>What happens:</p> <ul> <li>The catalog appears on your home page</li> <li>You can now view datasets, edit settings, or delete it</li> <li>The catalog is ready to use immediately</li> </ul>"},{"location":"web-ui/getting-started/#step-3-create-your-first-dataset","title":"Step 3: Create Your First Dataset","text":"<p>Datasets are versioned collections of files within a catalog.</p> <p>Navigate to your catalog:</p> <ol> <li>Click \"View Datasets\" on your catalog card</li> <li>Click the \"Create Dataset\" tab</li> <li>Fill in the form:</li> <li>Dataset Name: Use lowercase letters, numbers, and hyphens only      (e.g., <code>my-first-dataset</code>)</li> <li>Description: Optional description of what this dataset contains</li> <li>Click \"Create Dataset\"</li> </ol> <p>What happens:</p> <ul> <li>You're taken to the dataset view</li> <li>The dataset is empty and ready for files</li> <li>You can see three tabs: Files, History, and Commit</li> </ul>"},{"location":"web-ui/getting-started/#step-4-add-files-to-your-dataset","title":"Step 4: Add Files to Your Dataset","text":"<p>Now let's add some files to your dataset.</p> <p>In the dataset view:</p> <ol> <li>Click the \"Commit\" tab</li> <li>Upload files:</li> <li>Drag and drop files into the upload area, or</li> <li>Click \"Choose Files\" to browse your computer</li> <li>Enter a commit message:</li> <li>Describe what you're adding (e.g., \"Add initial data files\")</li> <li>Be descriptive\u2014this helps you understand changes later</li> <li>Click \"Create Commit\"</li> </ol> <p>What happens:</p> <ul> <li>Files are uploaded and stored</li> <li>A new commit appears in the History tab</li> <li>Files appear in the Files tab</li> <li>You can preview or download files</li> </ul>"},{"location":"web-ui/getting-started/#step-5-explore-your-dataset","title":"Step 5: Explore Your Dataset","text":"<p>Take a moment to explore what you've created.</p> <p>Files Tab:</p> <ul> <li>See all files in your dataset</li> <li>Click \"Preview\" to view file contents</li> <li>Click \"Download\" to save files locally</li> </ul> <p>History Tab:</p> <ul> <li>See your commit history (should show one commit)</li> <li>Click \"Browse Files\" to see files at that point in time</li> <li>Notice the commit hash, message, and timestamp</li> </ul> <p>Commit Tab:</p> <ul> <li>Upload more files or remove existing ones</li> <li>See Python code snippets for accessing your dataset programmatically</li> </ul>"},{"location":"web-ui/getting-started/#step-6-make-another-commit","title":"Step 6: Make Another Commit","text":"<p>Let's practice the workflow by making a second commit.</p> <p>Add more files:</p> <ol> <li>Go to the \"Commit\" tab</li> <li>Upload additional files (or the same files with updates)</li> <li>Write a commit message (e.g., \"Add updated sales data\")</li> <li>Click \"Create Commit\"</li> </ol> <p>What happens:</p> <ul> <li>A second commit appears in History</li> <li>The Files tab shows the current state (latest commit)</li> <li>You can browse either commit to see differences</li> </ul>"},{"location":"web-ui/getting-started/#step-7-browse-commit-history","title":"Step 7: Browse Commit History","text":"<p>See how your dataset has changed over time.</p> <p>In the History tab:</p> <ol> <li>You should see two commits (newest first)</li> <li>Click \"Browse Files\" on the older commit</li> <li>Compare the file list with the current Files tab</li> <li>Notice how commit messages help you understand what changed</li> </ol> <p>Key concepts:</p> <ul> <li>HEAD: The current commit (marked with a blue badge)</li> <li>Commit hash: Unique identifier for each commit</li> <li>Linear history: Commits form a chain, one after another</li> </ul>"},{"location":"web-ui/getting-started/#next-steps","title":"Next Steps","text":"<p>You've completed the basic workflow! Here's what to explore next:</p> <p>Common workflows:</p> <ul> <li>Web UI How-To Guide - Learn common tasks and workflows</li> <li>Catalog Management - Configure cloud storage   and authentication</li> <li>Basic Usage Guide - Deep dive into dataset   operations</li> </ul> <p>Advanced topics:</p> <ul> <li>Set up cloud storage catalogs (S3, GCS, Azure)</li> <li>Share datasets with your team</li> <li>Use Python API alongside the web UI</li> <li>Organize multiple catalogs and datasets</li> </ul>"},{"location":"web-ui/getting-started/#troubleshooting","title":"Troubleshooting","text":"<p>Server won't start:</p> <ul> <li>Check if the port is already in use</li> <li>Kirin will automatically use a different port\u2014check the terminal output</li> </ul> <p>Can't create catalog:</p> <ul> <li>Verify your storage path is correct</li> <li>For cloud storage, ensure you're authenticated</li> <li>Check file permissions for local paths</li> </ul> <p>Files won't upload:</p> <ul> <li>Verify you have write access to the catalog's storage location</li> <li>Check file size\u2014very large files may take time</li> <li>For cloud storage, verify your authentication is working</li> </ul> <p>Can't see datasets:</p> <ul> <li>Wait a few seconds for cloud catalogs to connect</li> <li>Verify your catalog path is correct</li> <li>Check that you have read access to the storage location</li> </ul> <p>For more help, see the Troubleshooting section.</p>"},{"location":"web-ui/how-to/","title":"Web UI How-To Guide","text":"<p>Practical guides for common tasks in the Kirin web interface. Each section walks you through a specific workflow with step-by-step instructions.</p>"},{"location":"web-ui/how-to/#daily-workflow-adding-new-data","title":"Daily Workflow: Adding New Data","text":"<p>Goal: Add new files to an existing dataset.</p> <p>Steps:</p> <ol> <li>Navigate to your dataset (Home \u2192 Catalog \u2192 Dataset)</li> <li>Click the \"Commit\" tab</li> <li>Upload your files:</li> <li>Drag and drop files into the upload area, or</li> <li>Click \"Choose Files\" to browse</li> <li>Write a descriptive commit message (e.g., \"Add Q4 sales data from CRM\")</li> <li>Click \"Create Commit\"</li> </ol> <p>Tips:</p> <ul> <li>Upload multiple files in one commit for related changes</li> <li>Use clear commit messages to understand changes later</li> <li>Check the summary before committing to verify what will be added</li> </ul>"},{"location":"web-ui/how-to/#updating-existing-files","title":"Updating Existing Files","text":"<p>Goal: Replace old files with updated versions.</p> <p>Steps:</p> <ol> <li>Go to your dataset's \"Commit\" tab</li> <li>Upload files with the same names as existing files</li> <li>Write a commit message explaining the update (e.g., \"Update customer data    with latest export\")</li> <li>Click \"Create Commit\"</li> </ol> <p>What happens:</p> <ul> <li>New files replace old ones with the same names</li> <li>Old versions remain in commit history</li> <li>You can browse old commits to see previous versions</li> </ul>"},{"location":"web-ui/how-to/#removing-files","title":"Removing Files","text":"<p>Goal: Remove files you no longer need.</p> <p>Steps:</p> <ol> <li>Go to your dataset's \"Commit\" tab</li> <li>In the \"Remove Files\" section, check boxes next to files to remove</li> <li>Write a commit message explaining why (e.g., \"Remove deprecated v1 data    files\")</li> <li>Click \"Create Commit\"</li> </ol> <p>Important:</p> <ul> <li>Files are removed from the current commit but remain in history</li> <li>You can always browse old commits to see removed files</li> <li>Consider if you really want to remove files or just stop using them</li> </ul>"},{"location":"web-ui/how-to/#combining-add-and-remove-operations","title":"Combining Add and Remove Operations","text":"<p>Goal: Add new files and remove old ones in a single commit.</p> <p>Steps:</p> <ol> <li>Go to the \"Commit\" tab</li> <li>Upload new files in the upload section</li> <li>Check boxes to remove files in the remove section</li> <li>Write a commit message describing all changes</li> <li>Click \"Create Commit\"</li> </ol> <p>Use case example:</p> <ul> <li>Adding updated data files while removing outdated ones</li> <li>Reorganizing dataset structure</li> <li>Cleaning up while adding new content</li> </ul>"},{"location":"web-ui/how-to/#viewing-file-contents","title":"Viewing File Contents","text":"<p>Goal: Quickly check what's in a file without downloading it.</p> <p>Steps:</p> <ol> <li>In the \"Files\" tab, click \"Preview\" next to any file</li> <li>For text files: See syntax-highlighted content</li> <li>For images: View the image directly</li> <li>For code files: See formatted code with highlighting</li> </ol> <p>Features:</p> <ul> <li>Syntax highlighting for code files</li> <li>Line numbers for text files</li> <li>Inline image display</li> <li>Source file links for generated files (like plots)</li> </ul> <p>Limitations:</p> <ul> <li>Very large files (&gt;1000 lines) show first 1000 lines</li> <li>Binary files show a message instead of content</li> <li>Some file types may not preview perfectly</li> </ul>"},{"location":"web-ui/how-to/#comparing-versions","title":"Comparing Versions","text":"<p>Goal: See what changed between two commits.</p> <p>Steps:</p> <ol> <li>Go to the \"History\" tab</li> <li>Note the commit hashes of the two versions you want to compare</li> <li>Click \"Browse Files\" on the older commit</li> <li>Note the files and their sizes</li> <li>Navigate back and click \"Browse Files\" on the newer commit</li> <li>Compare the file lists, sizes, and contents</li> </ol> <p>What to look for:</p> <ul> <li>Files added (appear in newer but not older)</li> <li>Files removed (appear in older but not newer)</li> <li>Files changed (same name, different size or hash)</li> <li>Total size differences</li> </ul> <p>Tip: Good commit messages make it easier to understand why changes were made.</p>"},{"location":"web-ui/how-to/#setting-up-a-cloud-catalog","title":"Setting Up a Cloud Catalog","text":"<p>Goal: Connect to an S3 bucket for team collaboration.</p> <p>Prerequisites:</p> <ul> <li>AWS account with S3 access</li> <li>AWS CLI installed and configured</li> <li>Appropriate IAM permissions</li> </ul> <p>Steps:</p> <ol> <li>Ensure you're authenticated with AWS:</li> </ol> <pre><code>aws sso login --profile your-profile\n# or\naws configure\n</code></pre> <ol> <li> <p>In the web UI, click \"+ Add Catalog\"</p> </li> <li> <p>Fill in the form:</p> </li> <li>Catalog Name: Something descriptive (e.g., \"Production S3 Data\")</li> <li>Root Directory: <code>s3://your-bucket-name/data</code></li> <li>AWS Profile: Select your profile from the dropdown</li> <li> <p>Authentication Command: <code>aws sso login --profile your-profile</code>      (optional, enables auto-authentication)</p> </li> <li> <p>Click \"Create Catalog\"</p> </li> </ol> <p>What happens:</p> <ul> <li>The catalog connects to your S3 bucket</li> <li>You can now create datasets that store data in S3</li> <li>Auto-authentication runs the command when needed</li> </ul> <p>For GCS:</p> <ul> <li>Use path: <code>gs://your-bucket-name/data</code></li> <li>Auth command: <code>gcloud auth login</code> or <code>gcloud auth application-default login</code></li> </ul> <p>For Azure:</p> <ul> <li>Use path: <code>az://your-container-name/data</code></li> <li>Auth command: <code>az login</code></li> </ul>"},{"location":"web-ui/how-to/#sharing-a-dataset-with-your-team","title":"Sharing a Dataset with Your Team","text":"<p>Goal: Let teammates access a dataset you created.</p> <p>Steps:</p> <ol> <li>Ensure the catalog is accessible:</li> <li>For cloud storage: Ensure teammates have access to the bucket/container</li> <li> <p>For local storage: Use a shared directory or network drive</p> </li> <li> <p>Share catalog configuration:</p> </li> <li>Catalog name</li> <li>Root directory path</li> <li> <p>Authentication requirements (if any)</p> </li> <li> <p>Teammates set up the catalog:</p> </li> <li>They create the same catalog in their web UI</li> <li>They configure authentication if needed</li> <li> <p>They can now see all datasets in that catalog</p> </li> <li> <p>Share dataset information:</p> </li> <li>Dataset name</li> <li>Catalog it's in</li> <li>Any relevant commit hashes or descriptions</li> </ol> <p>Tips:</p> <ul> <li>Use the \"Python Access Code\" section to share exact code for   programmatic access</li> <li>Document catalog setup in your team's wiki or docs</li> <li>Consider using consistent catalog names across the team</li> </ul>"},{"location":"web-ui/how-to/#organizing-multiple-catalogs","title":"Organizing Multiple Catalogs","text":"<p>Goal: Structure your catalogs for easy management.</p> <p>Strategies:</p> <p>By environment:</p> <ul> <li><code>production-data</code> \u2192 Production S3 bucket</li> <li><code>staging-data</code> \u2192 Staging S3 bucket</li> <li><code>local-dev</code> \u2192 Local development directory</li> </ul> <p>By team:</p> <ul> <li><code>analytics-team</code> \u2192 Analytics team's shared storage</li> <li><code>ml-team</code> \u2192 Machine learning team's storage</li> <li><code>research-team</code> \u2192 Research team's storage</li> </ul> <p>By project:</p> <ul> <li><code>customer-segmentation</code> \u2192 Project-specific storage</li> <li><code>price-optimization</code> \u2192 Another project's storage</li> </ul> <p>Best practices:</p> <ul> <li>Use consistent naming conventions</li> <li>Document catalog purposes</li> <li>Keep related datasets in the same catalog</li> <li>Don't create too many catalogs\u2014group related work together</li> </ul>"},{"location":"web-ui/how-to/#finding-files-in-large-datasets","title":"Finding Files in Large Datasets","text":"<p>Goal: Quickly locate specific files when you have many files.</p> <p>Steps:</p> <ol> <li>Go to your dataset's \"Files\" tab</li> <li>Use browser search (Ctrl+F / Cmd+F) to search file names</li> <li>Look for file type icons to filter visually</li> <li>Use the dataset's search functionality if available</li> </ol> <p>Tips:</p> <ul> <li>Use descriptive filenames to make files easier to find</li> <li>Group related files with consistent naming patterns</li> <li>Consider splitting very large datasets into smaller, focused ones</li> </ul>"},{"location":"web-ui/how-to/#exporting-python-code","title":"Exporting Python Code","text":"<p>Goal: Get Python code to access your dataset programmatically.</p> <p>Steps:</p> <ol> <li>Navigate to your dataset</li> <li>Look for the \"Python Access Code\" section (collapsible, usually at    the top)</li> <li>Click to expand it</li> <li>Click \"Copy\" to copy the code</li> <li>Paste into your Python script or notebook</li> </ol> <p>What you get:</p> <ul> <li>Complete code to create a Dataset object</li> <li>Proper authentication configuration</li> <li>Ready to use in your scripts</li> </ul> <p>Example output:</p> <pre><code>from kirin import Dataset\n\ndataset = Dataset(\n    root_dir=\"s3://my-bucket/data\",\n    name=\"my-dataset\",\n    aws_profile=\"production\"\n)\n\n# Checkout to latest commit\ndataset.checkout()\n</code></pre>"},{"location":"web-ui/how-to/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"web-ui/how-to/#cant-connect-to-cloud-storage","title":"Can't Connect to Cloud Storage","text":"<p>Symptoms: Catalog shows \"Error\" status or can't list datasets.</p> <p>Solutions:</p> <ol> <li>Check authentication:</li> <li>Run the auth command manually: <code>aws sso login --profile your-profile</code></li> <li>Verify credentials are valid</li> <li> <p>Check expiration for temporary credentials</p> </li> <li> <p>Verify permissions:</p> </li> <li>Ensure your credentials have read/write access to the bucket</li> <li> <p>Check IAM policies or bucket permissions</p> </li> <li> <p>Check network:</p> </li> <li>Verify you can access the cloud storage from your network</li> <li> <p>Check for firewall or VPN issues</p> </li> <li> <p>Review error messages:</p> </li> <li>The web UI shows specific error messages</li> <li>Use these to diagnose the issue</li> </ol>"},{"location":"web-ui/how-to/#files-wont-upload","title":"Files Won't Upload","text":"<p>Symptoms: Upload fails or hangs indefinitely.</p> <p>Solutions:</p> <ol> <li>Check file size:</li> <li>Very large files (&gt;100MB) may take time</li> <li> <p>Consider splitting large files</p> </li> <li> <p>Verify permissions:</p> </li> <li>Ensure you have write access to the catalog's storage</li> <li> <p>Check cloud storage permissions</p> </li> <li> <p>Check network:</p> </li> <li>For cloud storage, verify connection is stable</li> <li> <p>Try smaller files first to test</p> </li> <li> <p>Review browser console:</p> </li> <li>Check for JavaScript errors</li> <li>Look for network request failures</li> </ol>"},{"location":"web-ui/how-to/#cant-see-datasets","title":"Can't See Datasets","text":"<p>Symptoms: Catalog shows \"Unknown\" dataset count or empty list.</p> <p>Solutions:</p> <ol> <li>Wait for connection:</li> <li>Cloud catalogs may take a few seconds to connect</li> <li> <p>Refresh the page if it seems stuck</p> </li> <li> <p>Verify path:</p> </li> <li>Ensure the catalog path is correct</li> <li> <p>Check that the storage location exists</p> </li> <li> <p>Check authentication:</p> </li> <li>Verify you're authenticated with the cloud provider</li> <li> <p>Run auth commands manually if needed</p> </li> <li> <p>Verify permissions:</p> </li> <li>Ensure you have read access to list datasets</li> <li>Check IAM policies or storage permissions</li> </ol>"},{"location":"web-ui/how-to/#best-practices","title":"Best Practices","text":""},{"location":"web-ui/how-to/#commit-messages","title":"Commit Messages","text":"<p>Write clear, descriptive messages:</p> <ul> <li>\u2705 Good: \"Add Q4 2024 sales data from CRM export\"</li> <li>\u2705 Good: \"Update customer segmentation model with new features\"</li> <li>\u274c Bad: \"update\"</li> <li>\u274c Bad: \"files\"</li> </ul> <p>Why it matters: Good commit messages help you and your team understand what changed and why, especially when looking at history weeks or months later.</p>"},{"location":"web-ui/how-to/#dataset-organization","title":"Dataset Organization","text":"<p>Keep datasets focused:</p> <ul> <li>\u2705 Good: One dataset per use case or analysis</li> <li>\u274c Bad: One giant dataset with everything</li> </ul> <p>Why it matters: Focused datasets are easier to understand, navigate, and share with others.</p>"},{"location":"web-ui/how-to/#file-naming","title":"File Naming","text":"<p>Use descriptive, consistent names:</p> <ul> <li>\u2705 Good: <code>customer-transactions-2024-q4.csv</code>, <code>model-training-features-v2.json</code></li> <li>\u274c Bad: <code>data.csv</code>, <code>file1.json</code>, <code>stuff.xlsx</code></li> </ul> <p>Why it matters: Clear names make files easier to find and understand, especially when working with multiple files.</p>"},{"location":"web-ui/how-to/#next-steps","title":"Next Steps","text":"<ul> <li>Web UI Overview - Understand the concepts and   architecture</li> <li>Getting Started Tutorial - Step-by-step first   setup</li> <li>Catalog Management - Advanced catalog   configuration</li> </ul>"},{"location":"web-ui/overview/","title":"Web UI Overview","text":"<p>The Kirin web interface provides a visual, browser-based way to manage your data versioning workflows. It's designed for users who prefer graphical interfaces over command-line tools, and for teams who need shared visibility into datasets and their history.</p>"},{"location":"web-ui/overview/#what-is-the-web-ui","title":"What Is the Web UI?","text":"<p>The web UI is a full-featured interface to Kirin's data versioning system, running as a local web server that you access through your browser. It provides the same core functionality as the Kirin CLI, but through an interactive, visual interface.</p> <p>Key characteristics:</p> <ul> <li>Self-hosted: Runs locally on your machine\u2014no external services or   accounts required</li> <li>Browser-based: Works in any modern web browser</li> <li>HTMX-powered: Fast, dynamic updates without full page reloads</li> <li>Cloud-integrated: Works seamlessly with S3, GCS, Azure, and local   storage</li> </ul>"},{"location":"web-ui/overview/#why-does-it-exist","title":"Why Does It Exist?","text":"<p>The web UI addresses several needs that the CLI doesn't:</p> <p>Visual Exploration:</p> <ul> <li>Browse datasets and files without remembering commands</li> <li>See commit history as a timeline, not just log output</li> <li>Preview file contents directly in the browser</li> </ul> <p>Team Collaboration:</p> <ul> <li>Shared understanding of dataset structure and history</li> <li>Easier onboarding for team members unfamiliar with CLI tools</li> <li>Visual representation of changes over time</li> </ul> <p>Workflow Integration:</p> <ul> <li>Quick file uploads via drag-and-drop</li> <li>Immediate visual feedback on operations</li> <li>No need to switch between terminal and file browser</li> </ul>"},{"location":"web-ui/overview/#how-it-fits-into-kirin","title":"How It Fits into Kirin","text":"<p>The web UI is one of three ways to interact with Kirin:</p>"},{"location":"web-ui/overview/#1-web-ui-this-interface","title":"1. Web UI (this interface)","text":"<ul> <li>Best for: Visual exploration, quick operations, team collaboration</li> <li>Use when: You want to browse datasets, upload files interactively, or   share views with teammates</li> </ul>"},{"location":"web-ui/overview/#2-python-api","title":"2. Python API","text":"<ul> <li>Best for: Programmatic access, automation, integration with data science   workflows</li> <li>Use when: You're writing scripts, notebooks, or applications that need to   interact with Kirin</li> </ul>"},{"location":"web-ui/overview/#3-cli","title":"3. CLI","text":"<ul> <li>Best for: Command-line workflows, automation, CI/CD pipelines</li> <li>Use when: You're comfortable with terminals and need scriptable operations</li> </ul> <p>All three interfaces work with the same underlying data. A dataset created via the web UI can be accessed via Python API or CLI, and vice versa. They're different views of the same system.</p>"},{"location":"web-ui/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"web-ui/overview/#catalogs","title":"Catalogs","text":"<p>A catalog is a connection to a storage location. Think of it as a \"data source\" that Kirin can access.</p> <p>Storage backends:</p> <ul> <li>Local filesystem: Direct access to directories on your machine</li> <li>AWS S3: Cloud storage buckets</li> <li>Google Cloud Storage: GCS buckets</li> <li>Azure Blob Storage: Azure containers</li> </ul> <p>Key properties:</p> <ul> <li>Catalogs are independent\u2014each manages its own set of datasets</li> <li>Multiple catalogs can point to different storage locations</li> <li>Catalogs handle authentication and connection details</li> </ul> <p>Mental model: A catalog is like a \"workspace\" or \"project folder\" that contains multiple datasets.</p>"},{"location":"web-ui/overview/#datasets","title":"Datasets","text":"<p>A dataset is a versioned collection of files within a catalog. It's the primary unit of organization in Kirin.</p> <p>Key properties:</p> <ul> <li>Each dataset has a name and optional description</li> <li>Files are stored with content-addressed hashing (deduplication)</li> <li>History is linear\u2014each commit has one parent</li> <li>Datasets are independent\u2014changes to one don't affect others</li> </ul> <p>Mental model: A dataset is like a git repository, but for data files instead of source code.</p>"},{"location":"web-ui/overview/#commits","title":"Commits","text":"<p>A commit is a snapshot of files at a point in time. It records what files existed, their content hashes, and a message describing the change.</p> <p>Key properties:</p> <ul> <li>Each commit has a unique hash (content-addressed)</li> <li>Commits form a linear history (no branching)</li> <li>Commit messages describe what changed and why</li> <li>You can browse files at any point in history</li> </ul> <p>Mental model: A commit is like a \"save point\" or \"checkpoint\" in your data versioning workflow.</p>"},{"location":"web-ui/overview/#files","title":"Files","text":"<p>Files in Kirin are stored by content hash, not by name. This enables:</p> <ul> <li>Deduplication: Identical content stored once, even if filenames differ</li> <li>Integrity: Content verified by hash, ensuring data hasn't changed</li> <li>Efficiency: Same file content referenced multiple times without   duplication</li> </ul> <p>Key properties:</p> <ul> <li>Files are immutable once committed</li> <li>Multiple filenames can point to the same content</li> <li>Files can have metadata (e.g., source file links for generated plots)</li> <li>Original filenames are preserved for display and access</li> </ul> <p>Mental model: Files are like \"blobs\" in git\u2014content-addressed and immutable.</p>"},{"location":"web-ui/overview/#architecture","title":"Architecture","text":""},{"location":"web-ui/overview/#client-server-model","title":"Client-Server Model","text":"<p>The web UI runs as a local web server:</p> <pre><code>Browser \u2190\u2192 Web UI Server \u2190\u2192 Storage Backend\n         (FastAPI)        (fsspec)\n</code></pre> <p>Server components:</p> <ul> <li>FastAPI application: Handles HTTP requests and routing</li> <li>Template engine: Renders HTML pages (Jinja2)</li> <li>HTMX integration: Enables dynamic updates without JavaScript</li> <li>fsspec integration: Connects to various storage backends</li> </ul> <p>Communication:</p> <ul> <li>Browser sends HTTP requests (GET for pages, POST for actions)</li> <li>Server processes requests and interacts with storage</li> <li>Responses are HTML (with HTMX for dynamic updates)</li> <li>No API endpoints\u2014everything is page-based</li> </ul>"},{"location":"web-ui/overview/#storage-abstraction","title":"Storage Abstraction","text":"<p>The web UI uses the same storage abstraction as the Python API and CLI:</p> <pre><code>Web UI \u2192 Catalog \u2192 Dataset \u2192 Storage Backend\n</code></pre> <p>Layers:</p> <ol> <li>Web UI: User interface layer</li> <li>Catalog: Connection and configuration management</li> <li>Dataset: Versioning and file management</li> <li>Storage Backend: Actual file storage (local, S3, GCS, Azure)</li> </ol> <p>This abstraction means the web UI works identically with any storage backend, without special handling for different providers.</p>"},{"location":"web-ui/overview/#authentication-flow","title":"Authentication Flow","text":"<p>For cloud storage, authentication happens at the catalog level:</p> <p>Automatic authentication:</p> <ul> <li>Catalogs can store authentication commands (e.g., <code>aws sso login --profile   production</code>)</li> <li>When operations fail due to authentication, the web UI automatically runs   the stored command</li> <li>Success/failure is communicated to the user via UI messages</li> </ul> <p>Manual authentication:</p> <ul> <li>Users can authenticate via CLI before accessing catalogs</li> <li>Authentication state persists in the user's environment</li> <li>The web UI uses existing authentication from the environment</li> </ul> <p>Security model:</p> <ul> <li>Credentials are never stored in the web UI</li> <li>Authentication commands are stored, but not credentials themselves</li> <li>The web UI executes authentication commands in the user's environment</li> </ul>"},{"location":"web-ui/overview/#when-to-use-the-web-ui","title":"When to Use the Web UI","text":"<p>Use the web UI when:</p> <ul> <li>You want to explore datasets visually</li> <li>You need to quickly upload or preview files</li> <li>You're onboarding new team members</li> <li>You want to share dataset views with non-technical stakeholders</li> <li>You prefer graphical interfaces over command-line tools</li> </ul> <p>Use the Python API when:</p> <ul> <li>You're writing scripts or notebooks</li> <li>You need programmatic access to datasets</li> <li>You're integrating Kirin into data science workflows</li> <li>You want to automate dataset operations</li> </ul> <p>Use the CLI when:</p> <ul> <li>You're comfortable with command-line tools</li> <li>You need to script operations in shell scripts</li> <li>You're working in CI/CD pipelines</li> <li>You want lightweight, fast operations</li> </ul> <p>You can mix and match: Use the web UI for exploration and the Python API for automation in the same workflow.</p>"},{"location":"web-ui/overview/#design-principles","title":"Design Principles","text":"<p>The web UI follows several design principles:</p> <p>Simplicity First:</p> <ul> <li>Linear commit history (no branching complexity)</li> <li>Clear, focused interface without overwhelming options</li> <li>Progressive disclosure\u2014advanced features don't clutter the main interface</li> </ul> <p>Fast and Responsive:</p> <ul> <li>HTMX enables dynamic updates without full page reloads</li> <li>Operations are asynchronous where possible</li> <li>Timeout protection prevents UI from hanging</li> </ul> <p>Cloud-Agnostic:</p> <ul> <li>Same interface works with any storage backend</li> <li>Storage details are abstracted away</li> <li>Users don't need to think about storage provider differences</li> </ul> <p>User-Friendly:</p> <ul> <li>Clear error messages with actionable guidance</li> <li>Visual feedback for all operations</li> <li>Helpful defaults and sensible constraints</li> </ul>"},{"location":"web-ui/overview/#limitations-and-considerations","title":"Limitations and Considerations","text":"<p>Local server:</p> <ul> <li>The web UI runs on your machine\u2014it's not a hosted service</li> <li>You need to keep the server running while using it</li> <li>Port conflicts may require using a different port</li> </ul> <p>Browser-based:</p> <ul> <li>Requires a modern web browser</li> <li>Some operations (like large file uploads) depend on browser capabilities</li> <li>Offline use is limited\u2014you need the server running</li> </ul> <p>Not for everything:</p> <ul> <li>Very large datasets (&gt;10GB) may be slow to browse</li> <li>Bulk operations are often faster via CLI or Python API</li> <li>Advanced operations may require CLI or Python API</li> </ul> <p>Security:</p> <ul> <li>The web UI runs locally and doesn't expose data to the internet</li> <li>Authentication is handled by your environment (AWS, GCS, Azure CLIs)</li> <li>No credentials are stored in the web UI itself</li> </ul>"},{"location":"web-ui/overview/#integration-with-other-tools","title":"Integration with Other Tools","text":"<p>Python API:</p> <ul> <li>The web UI shows Python code snippets for accessing datasets</li> <li>You can copy-paste code from the UI into your scripts</li> <li>Both use the same underlying Kirin library</li> </ul> <p>CLI:</p> <ul> <li>CLI operations appear in the web UI's commit history</li> <li>You can use CLI for bulk operations and web UI for exploration</li> <li>Both work with the same datasets and catalogs</li> </ul> <p>Data Science Tools:</p> <ul> <li>Datasets created in the web UI can be loaded into pandas, polars, etc.</li> <li>File previews help you understand data before loading</li> <li>Python code snippets integrate with your existing workflows</li> </ul> <p>Version Control:</p> <ul> <li>Kirin's linear history complements git's branching model</li> <li>Use git for code, Kirin for data</li> <li>Commit messages in Kirin follow similar best practices to git</li> </ul>"},{"location":"web-ui/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started Tutorial - Step-by-step guide to   your first catalog and dataset</li> <li>Web UI How-To Guide - Common workflows and tasks</li> <li>Catalog Management - Advanced catalog   configuration</li> <li>Basic Usage Guide - Core dataset   operations</li> </ul>"}]}