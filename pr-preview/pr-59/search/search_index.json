{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Kirin Documentation","text":"<p>Welcome to Kirin - simplified \"git\" for data versioning!</p>"},{"location":"#what-is-kirin","title":"What is Kirin?","text":"<p>Kirin is a simplified tool for version-controlling data using content-addressed storage. It provides linear commit history for datasets without the complexity of branching and merging.</p> <p>Key Benefits:</p> <ul> <li>Linear versioning: Simple, Git-like commits without branching   complexity</li> <li>Content-addressed storage: Files stored by content hash for   integrity and deduplication</li> <li>Cloud support: Works with S3, GCS, Azure, and more</li> <li>Ergonomic API: Designed for data science workflows</li> <li>Zero-copy operations: Efficient handling of large files</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Create a catalog (works with local and cloud storage)\ncatalog = Catalog(root_dir=\"/path/to/data\")  # Local storage\ncatalog = Catalog(root_dir=\"s3://my-bucket\")  # S3 storage\n\n# Get or create a dataset\nds = catalog.get_dataset(\"my_dataset\")\n\n# Commit files\ncommit_hash = ds.commit(message=\"Initial commit\", add_files=[\"file1.csv\"])\n\n# Work with files locally\nwith ds.local_files() as local_files:\n    csv_path = local_files[\"file1.csv\"]\n    content = Path(csv_path).read_text()\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Quickstart - Get up and running in 5   minutes</li> <li>Installation - Installation options   and setup</li> <li>Core Concepts - Understanding   datasets, commits, and content-addressing</li> </ul>"},{"location":"#user-guides","title":"User Guides","text":"<ul> <li>Basic Usage - Essential workflows for working   with datasets</li> <li>Cloud Storage - Set up and use cloud storage   backends</li> <li>Working with Files - File operations and   data science integration</li> <li>Commit Management - Understanding and   working with commit history</li> </ul>"},{"location":"#web-ui","title":"Web UI","text":"<ul> <li>Web UI Overview - Getting started with the web   interface</li> <li>Catalog Management - Advanced catalog   configuration</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Storage Format - Technical storage details</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<ul> <li>Architecture Overview - System architecture   and design principles</li> </ul>"},{"location":"#why-kirin-exists","title":"Why Kirin Exists","text":"<p>Kirin addresses critical needs in machine learning and data science workflows:</p> <ul> <li>Linear Data Versioning: Track changes to datasets with simple, linear commits</li> <li>Content-Addressed Storage: Ensure data integrity and enable deduplication</li> <li>Multi-Backend Support: Work with S3, GCS, Azure, local filesystem, and more</li> <li>Serverless Architecture: No dedicated servers required</li> <li>Ergonomic Python API: Focus on ease of use and developer experience</li> <li>File Versioning: Track changes to individual files over time</li> </ul>"},{"location":"#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Experiment tracking: Version your training data and model inputs</li> <li>Data pipeline versioning: Track changes in ETL processes</li> <li>Collaborative research: Share datasets with exact version control</li> <li>Reproducible analysis: Ensure you can recreate your results</li> <li>MLOps workflows: Deploy models with exact data dependencies</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code># Development (with pixi)\ngit clone git@github.com:nll-ai/kirin\ncd kirin\npixi install\npixi run kirin ui\n\n# Production (with uv)\nuv tool install kirin\nuv run kirin ui\n\n# One-time use (with uvx)\nuvx kirin ui\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ol> <li>Quickstart - Try Kirin with a simple example</li> <li>Core Concepts - Understand how Kirin works</li> <li>Basic Usage - Learn common workflows</li> <li>Cloud Storage - Set up cloud storage</li> </ol>"},{"location":"cloud-storage-auth/","title":"Cloud Storage Authentication Guide","text":"<p>When using <code>kirin</code> with cloud storage backends (S3, GCS, Azure, etc.), you need to provide authentication credentials. This guide shows you how to authenticate with different cloud providers using the new cloud-agnostic authentication parameters.</p>"},{"location":"cloud-storage-auth/#new-cloud-agnostic-authentication-recommended","title":"New Cloud-Agnostic Authentication (Recommended)","text":"<p>Kirin now supports cloud-agnostic authentication parameters that work across all cloud providers:</p>"},{"location":"cloud-storage-auth/#awss3-authentication","title":"AWS/S3 Authentication","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Using AWS profile\ncatalog = Catalog(\n    root_dir=\"s3://my-bucket/data\",\n    aws_profile=\"my-profile\"\n)\n\n# Using Dataset with AWS profile\ndataset = Dataset(\n    root_dir=\"s3://my-bucket/data\",\n    name=\"my-dataset\",\n    aws_profile=\"my-profile\"\n)\n</code></pre>"},{"location":"cloud-storage-auth/#gcpgcs-authentication","title":"GCP/GCS Authentication","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Using service account key file\ncatalog = Catalog(\n    root_dir=\"gs://my-bucket/data\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"my-project\"\n)\n\n# Using Dataset with GCS credentials\ndataset = Dataset(\n    root_dir=\"gs://my-bucket/data\",\n    name=\"my-dataset\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"my-project\"\n)\n</code></pre>"},{"location":"cloud-storage-auth/#azure-blob-storage-authentication","title":"Azure Blob Storage Authentication","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Using connection string\ncatalog = Catalog(\n    root_dir=\"az://my-container/data\",\n    azure_connection_string=os.getenv(\"AZURE_CONNECTION_STRING\")\n)\n\n# Using account name and key\ndataset = Dataset(\n    root_dir=\"az://my-container/data\",\n    name=\"my-dataset\",\n    azure_account_name=\"my-account\",\n    azure_account_key=\"my-key\"\n)\n</code></pre>"},{"location":"cloud-storage-auth/#web-ui-integration","title":"Web UI Integration","text":"<p>The web UI also supports cloud authentication through the <code>CatalogConfig.to_catalog()</code> method:</p> <pre><code>from kirin.web.config import CatalogConfig\n\n# Create catalog config with cloud auth\nconfig = CatalogConfig(\n    id=\"my-catalog\",\n    name=\"My Catalog\",\n    root_dir=\"s3://my-bucket/data\",\n    aws_profile=\"my-profile\"\n)\n\n# Convert to runtime catalog\ncatalog = config.to_catalog()\n</code></pre>"},{"location":"cloud-storage-auth/#legacy-authentication-methods","title":"Legacy Authentication Methods","text":"<p>The following sections show the legacy authentication methods that still work but are less convenient than the new cloud-agnostic approach.</p>"},{"location":"cloud-storage-auth/#google-cloud-storage-gcs","title":"Google Cloud Storage (GCS)","text":""},{"location":"cloud-storage-auth/#error-you-might-see","title":"Error You Might See","text":"<pre><code>gcsfs.retry.HttpError: Anonymous caller does not have storage.objects.list access\nto the Google Cloud Storage bucket. Permission 'storage.objects.list' denied on\nresource (or it may not exist)., 401\n</code></pre>"},{"location":"cloud-storage-auth/#solutions","title":"Solutions","text":""},{"location":"cloud-storage-auth/#option-1-application-default-credentials-recommended","title":"Option 1: Application Default Credentials (Recommended)","text":"<p>Use <code>gcloud</code> CLI to set up credentials:</p> <pre><code># Install gcloud CLI first, then:\ngcloud auth application-default login\n</code></pre> <p>Then use kirin normally:</p> <pre><code>from kirin.dataset import Dataset\n\n# Will automatically use your gcloud credentials\nds = Dataset(root_dir=\"gs://my-bucket/datasets\", dataset_name=\"my_data\")\n</code></pre>"},{"location":"cloud-storage-auth/#option-2-service-account-key-file","title":"Option 2: Service Account Key File","text":"<pre><code>from kirin.dataset import Dataset\nimport fsspec\n\n# Create filesystem with service account\nfs = fsspec.filesystem(\n    'gs',\n    token='/path/to/service-account-key.json'\n)\n\n# Pass it to Dataset\nds = Dataset(\n    root_dir=\"gs://my-bucket/datasets\",\n    dataset_name=\"my_data\",\n    fs=fs  # Use authenticated filesystem\n)\n</code></pre>"},{"location":"cloud-storage-auth/#option-3-environment-variable","title":"Option 3: Environment Variable","text":"<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/service-account-key.json\"\n</code></pre> <pre><code>from kirin.dataset import Dataset\n\n# Will automatically use the credentials from environment variable\nds = Dataset(root_dir=\"gs://my-bucket/datasets\", dataset_name=\"my_data\")\n</code></pre>"},{"location":"cloud-storage-auth/#option-4-pass-credentials-directly","title":"Option 4: Pass Credentials Directly","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    'gs',\n    project='my-project-id',\n    token='cloud'  # Uses gcloud credentials\n)\n\nds = Dataset(root_dir=\"gs://my-bucket/datasets\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#amazon-s3","title":"Amazon S3","text":""},{"location":"cloud-storage-auth/#option-1-aws-cli-credentials-recommended","title":"Option 1: AWS CLI Credentials (Recommended)","text":"<pre><code># Configure AWS credentials\naws configure\n</code></pre> <p>Then use normally:</p> <pre><code>from kirin.dataset import Dataset\n\nds = Dataset(root_dir=\"s3://my-bucket/datasets\", dataset_name=\"my_data\")\n</code></pre>"},{"location":"cloud-storage-auth/#option-2-environment-variables","title":"Option 2: Environment Variables","text":"<pre><code>export AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\nexport AWS_DEFAULT_REGION=\"us-east-1\"\n</code></pre>"},{"location":"cloud-storage-auth/#option-3-pass-credentials-explicitly","title":"Option 3: Pass Credentials Explicitly","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    's3',\n    key='your-access-key',\n    secret='your-secret-key',\n    client_kwargs={'region_name': 'us-east-1'}\n)\n\nds = Dataset(root_dir=\"s3://my-bucket/datasets\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#s3-compatible-services-minio-backblaze-b2-digitalocean-spaces","title":"S3-Compatible Services (Minio, Backblaze B2, DigitalOcean Spaces)","text":""},{"location":"cloud-storage-auth/#minio-example","title":"Minio Example","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    's3',\n    key='your-access-key',\n    secret='your-secret-key',\n    client_kwargs={\n        'endpoint_url': 'http://localhost:9000'  # Your Minio endpoint\n    }\n)\n\nds = Dataset(root_dir=\"s3://my-bucket/datasets\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#backblaze-b2-example","title":"Backblaze B2 Example","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    's3',\n    key='your-application-key-id',\n    secret='your-application-key',\n    client_kwargs={\n        'endpoint_url': 'https://s3.us-west-002.backblazeb2.com'\n    }\n)\n\nds = Dataset(root_dir=\"s3://my-bucket/datasets\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#azure-blob-storage","title":"Azure Blob Storage","text":""},{"location":"cloud-storage-auth/#option-1-azure-cli-credentials","title":"Option 1: Azure CLI Credentials","text":"<pre><code>az login\n</code></pre>"},{"location":"cloud-storage-auth/#option-2-connection-string","title":"Option 2: Connection String","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    'az',\n    connection_string='your-connection-string'\n)\n\nds = Dataset(root_dir=\"az://container/path\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#option-3-account-key","title":"Option 3: Account Key","text":"<pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\nfs = fsspec.filesystem(\n    'az',\n    account_name='your-account-name',\n    account_key='your-account-key'\n)\n\nds = Dataset(root_dir=\"az://container/path\", dataset_name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#general-pattern","title":"General Pattern","text":"<p>For any cloud provider, the recommended pattern is:</p> <ol> <li>New Cloud-Agnostic Approach (Recommended):</li> </ol> <pre><code>from kirin import Catalog, Dataset\n\n# For AWS/S3\ncatalog = Catalog(root_dir=\"s3://bucket/path\", aws_profile=\"my-profile\")\ndataset = Dataset(root_dir=\"s3://bucket/path\", name=\"my_data\",\n                 aws_profile=\"my-profile\")\n\n# For GCS\ncatalog = Catalog(root_dir=\"gs://bucket/path\", gcs_token=\"/path/to/key.json\",\n                 gcs_project=\"my-project\")\ndataset = Dataset(root_dir=\"gs://bucket/path\", name=\"my_data\",\n                 gcs_token=\"/path/to/key.json\", gcs_project=\"my-project\")\n\n# For Azure\ncatalog = Catalog(root_dir=\"az://container/path\", azure_connection_string=\"...\")\ndataset = Dataset(root_dir=\"az://container/path\", name=\"my_data\", azure_connection_string=\"...\")\n</code></pre> <ol> <li>Auto-detect (works if credentials are already configured):</li> </ol> <pre><code>ds = Dataset(root_dir=\"protocol://bucket/path\", name=\"my_data\")\n</code></pre> <ol> <li>Legacy explicit authentication (still supported):</li> </ol> <pre><code>import fsspec\nfrom kirin.dataset import Dataset\n\n# Create authenticated filesystem\nfs = fsspec.filesystem('protocol', **auth_kwargs)\n\n# Pass to Dataset\nds = Dataset(root_dir=\"protocol://bucket/path\", name=\"my_data\", fs=fs)\n</code></pre>"},{"location":"cloud-storage-auth/#testing-without-credentials","title":"Testing Without Credentials","text":"<p>For local development/testing without cloud access, you can use:</p> <pre><code># In-memory filesystem (no cloud needed)\nfrom kirin.dataset import Dataset\n\nds = Dataset(root_dir=\"memory://test-data\", dataset_name=\"my_data\")\n</code></pre>"},{"location":"cloud-storage-auth/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cloud-storage-auth/#issue-anonymous-caller-or-access-denied","title":"Issue: \"Anonymous caller\" or \"Access Denied\"","text":"<ul> <li>Cause: No credentials provided</li> <li>Solution: Set up credentials using one of the methods above</li> </ul>"},{"location":"cloud-storage-auth/#issue-permission-denied","title":"Issue: \"Permission denied\"","text":"<ul> <li>Cause: Credentials don't have required permissions</li> <li>Solution: Ensure your IAM role/service account has read/write permissions   on the bucket</li> </ul>"},{"location":"cloud-storage-auth/#issue-bucket-does-not-exist","title":"Issue: \"Bucket does not exist\"","text":"<ul> <li>Cause: Bucket name is incorrect or doesn't exist</li> <li>Solution: Create the bucket first or check the bucket name</li> </ul>"},{"location":"cloud-storage-auth/#issue-import-errors-like-no-module-named-s3fs","title":"Issue: Import errors like \"No module named 's3fs'\"","text":"<ul> <li>Cause: Cloud storage package not installed</li> <li>Solution: Install required package:</li> </ul> <pre><code>pip install s3fs      # For S3\npip install gcsfs     # For GCS\npip install adlfs     # For Azure\n</code></pre>"},{"location":"cloud-storage-auth/#known-issues","title":"Known Issues","text":""},{"location":"cloud-storage-auth/#macos-python-313-ssl-certificate-verification","title":"macOS Python 3.13 SSL Certificate Verification","text":"<p>On macOS with Python 3.13, you may encounter SSL certificate verification errors when using cloud storage backends. This is a known Python/macOS issue.</p> <p>Workaround: The web UI skips connection testing during backend creation. Backends are validated when actually used. If you encounter SSL errors during actual usage, install certificates:</p> <pre><code>/Applications/Python\\ 3.13/Install\\ Certificates.command\n</code></pre> <p>Or use <code>certifi</code>:</p> <pre><code>pip install --upgrade certifi\n</code></pre>"},{"location":"cloud-storage-auth/#web-ui-auto-authentication","title":"Web UI Auto-Authentication","text":"<p>NEW: The Kirin web UI can automatically execute authentication commands when needed.</p>"},{"location":"cloud-storage-auth/#configuring-auto-authentication","title":"Configuring Auto-Authentication","text":"<p>When creating or editing a catalog in the web UI, you can provide an optional authentication command:</p> <p>AWS S3:</p> <pre><code>Auth Command: aws sso login --profile {{ aws_profile }}\n</code></pre> <p>GCP GCS:</p> <pre><code>Auth Command: gcloud auth login\n</code></pre> <p>Azure Blob Storage:</p> <pre><code>Auth Command: az login\n</code></pre>"},{"location":"cloud-storage-auth/#how-auto-authentication-works","title":"How Auto-Authentication Works","text":"<ol> <li>Catalog Access: When you click on a catalog, Kirin attempts to    connect</li> <li>Authentication Check: If authentication fails or times out, Kirin    checks for stored auth command</li> <li>Auto-Execution: If found, Kirin executes the command automatically    (30-second timeout)</li> <li>Retry: If authentication succeeds, Kirin retries loading the    datasets</li> <li>Feedback: Success/failure messages are shown in the UI</li> </ol>"},{"location":"cloud-storage-auth/#benefits","title":"Benefits","text":"<ul> <li>No manual CLI: Authentication happens automatically in the web UI</li> <li>Faster workflow: No need to switch to terminal and run commands</li> <li>Clear feedback: You see exactly what happened (success or failure)</li> <li>Manual fallback: You can still authenticate manually if needed</li> </ul>"},{"location":"cloud-storage-auth/#timeout-protection","title":"Timeout Protection","text":"<p>To prevent the UI from hanging on slow or failed connections:</p> <ul> <li>10-second timeout for catalog operations (listing datasets)</li> <li>5-second timeout for individual dataset loading</li> <li>30-second timeout for authentication commands</li> <li>Clear error messages when timeouts occur</li> </ul>"},{"location":"cloud-storage-auth/#security-considerations","title":"Security Considerations","text":"<ul> <li>Commands are stored locally in your catalog configuration file</li> <li>No passwords stored: Auth commands only contain profile names or   trigger browser-based OAuth</li> <li>Subprocess isolation: Commands run in isolated subprocesses with   timeout protection</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>High-level system architecture and design principles for Kirin.</p>"},{"location":"architecture/overview/#system-architecture","title":"System Architecture","text":"<p>Kirin implements a simplified content-addressed storage system with the following key components:</p> <pre><code>graph TB\n    subgraph \"Kirin Core Components\"\n        DA[\"Dataset API&lt;br/&gt;\u2022 File operations&lt;br/&gt;\u2022 Commit management&lt;br/&gt;\u2022 Versioning\"]\n        CS[\"Commit Store&lt;br/&gt;\u2022 Linear history&lt;br/&gt;\u2022 Metadata&lt;br/&gt;\u2022 References\"]\n        COS[\"Content Store&lt;br/&gt;\u2022 Content hash&lt;br/&gt;\u2022 Deduplication&lt;br/&gt;\u2022 Backend agnostic\"]\n    end\n\n    subgraph \"Storage Layer\"\n        FS[\"FSSpec Layer&lt;br/&gt;\u2022 Local FS&lt;br/&gt;\u2022 S3&lt;br/&gt;\u2022 GCS&lt;br/&gt;\u2022 Azure&lt;br/&gt;\u2022 Other backends\"]\n    end\n\n    DA --&gt; CS\n    DA --&gt; COS\n    CS --&gt; COS\n    COS --&gt; FS</code></pre>"},{"location":"architecture/overview/#core-design-principles","title":"Core Design Principles","text":""},{"location":"architecture/overview/#1-simplified-data-versioning","title":"1. Simplified Data Versioning","text":"<p>Kirin is simplified \"git\" for data - follows git conventions but with linear-only history:</p> <ul> <li>Linear Commits: Simple, linear commit history without branching complexity</li> <li>Content-Addressed Storage: Files stored by content hash for integrity and deduplication</li> <li>Ergonomic Python API: Focus on ease of use and developer experience</li> <li>Backend-Agnostic: Works with any storage backend via fsspec</li> <li>No Branching: Linear-only commit history to avoid complexity</li> </ul>"},{"location":"architecture/overview/#2-content-addressed-storage-design","title":"2. Content-Addressed Storage Design","text":"<p>CRITICAL: Files are stored without file extensions in the content-addressed storage system:</p> <ul> <li>Storage Path: <code>root_dir/data/{hash[:2]}/{hash[2:]}</code> (e.g.,   <code>data/ab/cdef1234...</code>)</li> <li>No Extensions: Original <code>.csv</code>, <code>.txt</code>, <code>.json</code> extensions are not   preserved in storage</li> <li>Metadata Storage: File extensions are stored as metadata in the <code>File</code>   entity's <code>name</code> attribute</li> <li>Extension Restoration: When files are downloaded or accessed, they get   their original names back</li> <li>Content Integrity: Files are identified purely by content hash, ensuring   data integrity</li> <li>Deduplication: Identical content (regardless of original filename) is   stored only once</li> </ul>"},{"location":"architecture/overview/#3-file-access-patterns","title":"3. File Access Patterns","text":"<p>Kirin provides simple file access through standard Python file operations:</p> <ul> <li>Temporary file downloads: Files are downloaded to temporary locations   when accessed</li> <li>Standard file handles: Files are accessed through normal Python file   objects</li> <li>Automatic cleanup: Temporary files are automatically cleaned up when   file handles are closed</li> <li>Streaming support: Large files can be streamed through fsspec backends   for efficient transfer</li> </ul>"},{"location":"architecture/overview/#key-benefits","title":"Key Benefits","text":""},{"location":"architecture/overview/#for-data-scientists","title":"For Data Scientists","text":"<ul> <li>Linear Data Versioning: Track changes to datasets with simple, linear commits</li> <li>Content-Addressed Storage: Ensure data integrity and enable deduplication</li> <li>Multi-Backend Support: Work with S3, GCS, Azure, local filesystem, and more</li> <li>Serverless Architecture: No dedicated servers required</li> <li>Ergonomic Python API: Focus on ease of use and developer experience</li> <li>File Versioning: Track changes to individual files over time</li> </ul>"},{"location":"architecture/overview/#for-data-engineers","title":"For Data Engineers","text":"<ul> <li>Backend-agnostic: Works with any storage backend via fsspec</li> <li>Automatic deduplication: Identical files stored once, saving space</li> <li>Content integrity: Files stored by content hash for data integrity</li> <li>Performance optimized: Chunked processing for large files</li> <li>Extensible: Easy to add new backends and features</li> </ul>"},{"location":"architecture/overview/#user-personas-and-jobs-to-be-done","title":"User Personas and Jobs to be Done","text":""},{"location":"architecture/overview/#data-scientist-ml-engineer","title":"Data Scientist / ML Engineer","text":"<p>Jobs to be Done:</p> <ol> <li>Track Experiment Data: \"I need to keep track of which datasets were    used in which experiments so I can reproduce my results.\"</li> <li>Find and Use the Right Data Version: \"I need to identify and access    specific versions of datasets for training models.\"</li> <li>Collaborate with Team Members: \"I need to share datasets with    colleagues in a way that ensures we're all using the same exact data.\"</li> <li>Document Data Transformations: \"I need to track how raw data is    transformed into model-ready data.\"</li> </ol>"},{"location":"architecture/overview/#data-engineer","title":"Data Engineer","text":"<p>Jobs to be Done:</p> <ol> <li>Manage Data Pipelines: \"I need to ensure data pipelines produce    consistent, traceable outputs.\"</li> <li>Optimize Storage Usage: \"I need to handle large datasets efficiently    without wasting storage.\"</li> <li>Support Multiple Storage Solutions: \"I need to work with data across    various storage systems our organization uses.\"</li> <li>Ensure Data Governance: \"I need to track who accesses what data and    how it's used.\"</li> </ol>"},{"location":"architecture/overview/#data-team-manager-lead","title":"Data Team Manager / Lead","text":"<p>Jobs to be Done:</p> <ol> <li>Ensure Reproducibility: \"I need to guarantee that our team's work is    reproducible for scientific integrity and audit purposes.\"</li> <li>Manage Technical Debt: \"I need to understand data dependencies to    prevent cascading failures when data changes.\"</li> <li>Accelerate Onboarding: \"I need new team members to quickly understand    our data ecosystem.\"</li> <li>Support Regulatory Compliance: \"I need to demonstrate data provenance    for regulatory compliance.\"</li> </ol>"},{"location":"architecture/overview/#mlops-engineer","title":"MLOps Engineer","text":"<p>Jobs to be Done:</p> <ol> <li>Deploy Models with Data Dependencies: \"I need to package models with    their exact data dependencies.\"</li> <li>Monitor Data Drift: \"I need to compare production data against training    data to detect drift.\"</li> <li>Implement Data-Centric CI/CD: \"I need automated tests that verify data    quality across pipeline stages.\"</li> <li>Roll Back Data When Needed: \"I need to quickly revert to previous data    versions if issues arise.\"</li> </ol>"},{"location":"architecture/overview/#feature-to-job-mapping","title":"Feature-to-Job Mapping","text":""},{"location":"architecture/overview/#content-addressed-storage","title":"Content-Addressed Storage","text":"<ul> <li>Jobs: Track Experiment Data, Find and Use the Right Data Version,   Collaborate with Team Members, Ensure Reproducibility</li> <li>Users: Data Scientist, ML Engineer, Team Lead, Laboratory Scientist</li> </ul>"},{"location":"architecture/overview/#automatic-lineage-tracking","title":"Automatic Lineage Tracking","text":"<ul> <li>Jobs: Document Data Transformations, Manage Data Pipelines,   Track Sample Lineage, Manage Technical Debt</li> <li>Users: Data Scientist, Data Engineer, Laboratory Scientist</li> </ul>"},{"location":"architecture/overview/#backend-agnostic-storage","title":"Backend-Agnostic Storage","text":"<ul> <li>Jobs: Support Multiple Storage Solutions, Optimize Storage Usage,   Manage Collaborative Research</li> <li>Users: Data Engineer, MLOps Engineer, Laboratory Scientist</li> </ul>"},{"location":"architecture/overview/#dataset-versioning","title":"Dataset Versioning","text":"<ul> <li>Jobs: Deploy Models with Data Dependencies, Roll Back Data When Needed,   Monitor Data Drift, Ensure Experimental Reproducibility</li> <li>Users: MLOps Engineer, Data Engineer, Laboratory Scientist</li> </ul>"},{"location":"architecture/overview/#usage-tracking","title":"Usage Tracking","text":"<ul> <li>Jobs: Document Data Usage, Ensure Data Governance,   Support Regulatory Compliance, Document Methods and Parameters</li> <li>Users: Team Lead, Laboratory Scientist</li> </ul>"},{"location":"architecture/overview/#streaming-file-access","title":"Streaming File Access","text":"<ul> <li>Jobs: Optimize Storage Usage, Handle Large Datasets</li> <li>Users: Data Engineer, MLOps Engineer</li> </ul>"},{"location":"architecture/overview/#data-catalog","title":"Data Catalog","text":"<ul> <li>Jobs: Accelerate Onboarding, Find the Right Data Version,   Manage Collaborative Research</li> <li>Users: Team Lead, Data Scientist, Laboratory Scientist</li> </ul>"},{"location":"architecture/overview/#path-based-api","title":"Path-Based API","text":"<ul> <li>Jobs: Implement Data-Centric CI/CD, Manage Data Pipelines</li> <li>Users: MLOps Engineer, Data Engineer</li> </ul>"},{"location":"architecture/overview/#system-flow","title":"System Flow","text":""},{"location":"architecture/overview/#1-data-ingestion-flow","title":"1. Data Ingestion Flow","text":"<ol> <li>User provides files to be tracked</li> <li>Files are hashed and stored in content store</li> <li>A commit is created with references to file versions</li> <li>The commit is recorded in the linear history</li> </ol>"},{"location":"architecture/overview/#2-data-access-flow","title":"2. Data Access Flow","text":"<ol> <li>User requests a specific version of a file or dataset</li> <li>System resolves the logical path to a content hash</li> <li>Content is retrieved from the storage backend</li> <li>Content is provided to the user</li> </ol>"},{"location":"architecture/overview/#3-data-processing-flow","title":"3. Data Processing Flow","text":"<ol> <li>User accesses input data files</li> <li>Processing is performed on the data</li> <li>Output files are stored in Kirin</li> <li>New commit is created with updated files</li> </ol>"},{"location":"architecture/overview/#linear-vs-branching","title":"Linear vs. Branching","text":"<p>Kirin uses linear commit history instead of Git's branching model:</p> <p>Linear History (Kirin):</p> <pre><code>graph LR\n    A[Commit A] --&gt; B[Commit B]\n    B --&gt; C[Commit C]\n    C --&gt; D[Commit D]</code></pre> <p>Branching History (Git):</p> <pre><code>graph TD\n    A[Commit A] --&gt; B[Commit B]\n    B --&gt; C[Commit C]\n    B --&gt; D[Commit D]\n    D --&gt; E[Commit E]</code></pre> <p>Benefits of Linear History:</p> <ul> <li>Simpler: No merge conflicts or complex branching</li> <li>Clearer: Easy to understand data evolution</li> <li>Safer: No risk of losing data through complex merges</li> <li>Faster: No need to resolve merge conflicts</li> </ul>"},{"location":"architecture/overview/#backend-agnostic-design","title":"Backend-Agnostic Design","text":"<p>Kirin works with any storage backend through the fsspec library:</p> <p>Supported Backends:</p> <ul> <li>Local filesystem: <code>/path/to/data</code></li> <li>AWS S3: <code>s3://bucket/path</code></li> <li>Google Cloud Storage: <code>gs://bucket/path</code></li> <li>Azure Blob Storage: <code>az://container/path</code></li> <li>S3-compatible services: Minio, Backblaze B2, DigitalOcean Spaces, Wasabi</li> <li>And many more: Dropbox, Google Drive, etc. (sync/auth handled by backend)</li> </ul> <p>Benefits:</p> <ul> <li>Flexibility: Use any storage backend</li> <li>Scalability: Scale from local to cloud</li> <li>Portability: Move between backends easily</li> <li>Cost optimization: Choose the right storage for your needs</li> </ul>"},{"location":"architecture/overview/#system-flow-diagrams","title":"System Flow Diagrams","text":""},{"location":"architecture/overview/#data-ingestion-flow","title":"Data Ingestion Flow","text":"<pre><code>sequenceDiagram\n    participant U as User\n    participant D as Dataset API\n    participant CS as Content Store\n    participant CM as Commit Store\n\n    U-&gt;&gt;D: Provide files to track\n    D-&gt;&gt;CS: Hash and store files\n    CS--&gt;&gt;D: Return content hashes\n    D-&gt;&gt;CM: Create commit with file references\n    CM--&gt;&gt;D: Save commit to linear history\n    D--&gt;&gt;U: Return commit hash</code></pre>"},{"location":"architecture/overview/#data-access-flow","title":"Data Access Flow","text":"<pre><code>sequenceDiagram\n    participant U as User\n    participant D as Dataset API\n    participant CS as Content Store\n    participant FS as FSSpec Layer\n\n    U-&gt;&gt;D: Request specific file version\n    D-&gt;&gt;D: Resolve logical path to content hash\n    D-&gt;&gt;CS: Retrieve content by hash\n    CS-&gt;&gt;FS: Read from storage backend\n    FS--&gt;&gt;CS: Return file content\n    CS--&gt;&gt;D: Return content bytes\n    D--&gt;&gt;U: Provide file to user</code></pre>"},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Storage Format - Technical storage details</li> </ul>"},{"location":"getting-started/core-concepts/","title":"Core Concepts","text":"<p>Understanding the fundamental concepts behind Kirin's data versioning system.</p>"},{"location":"getting-started/core-concepts/#what-is-kirin","title":"What is Kirin?","text":"<p>Kirin is a simplified tool for version-controlling data using content-addressed storage. It provides linear commit history for datasets without the complexity of branching and merging.</p>"},{"location":"getting-started/core-concepts/#key-concepts","title":"Key Concepts","text":""},{"location":"getting-started/core-concepts/#datasets","title":"Datasets","text":"<p>A dataset is a logical collection of files that you want to version together. Think of it as a folder that tracks changes over time.</p> <pre><code>from kirin import Dataset\n\n# Create a dataset\ndataset = Dataset(root_dir=\"/path/to/data\", name=\"my_dataset\")\n</code></pre> <p>Characteristics:</p> <ul> <li>Contains multiple files</li> <li>Has a linear commit history</li> <li>Can be shared and collaborated on</li> <li>Maintains data integrity through content-addressing</li> </ul>"},{"location":"getting-started/core-concepts/#files","title":"Files","text":"<p>A file in Kirin represents a versioned file with content-addressed storage. Files are immutable once created and identified by their content hash.</p> <pre><code># Get a file from the current commit\nfile_obj = dataset.get_file(\"data.csv\")\nprint(f\"File hash: {file_obj.hash}\")\nprint(f\"File size: {file_obj.size} bytes\")\nprint(f\"Content type: {file_obj.content_type}\")\nprint(f\"Short hash: {file_obj.short_hash}\")\n</code></pre> <p>Key Properties:</p> <ul> <li>Content-addressed: Identified by content hash, not filename</li> <li>Immutable: Cannot be changed once created</li> <li>Deduplicated: Identical content stored only once</li> <li>Backend-agnostic: Works with any storage backend</li> </ul>"},{"location":"getting-started/core-concepts/#commits","title":"Commits","text":"<p>A commit represents an immutable snapshot of files at a point in time. Commits form a linear history with single parent relationships.</p> <pre><code># Create a commit\ncommit_hash = dataset.commit(\n    message=\"Add new data\",\n    add_files=[\"data.csv\", \"config.json\"]\n)\n\n# Get commit information\ncommit = dataset.get_commit(commit_hash)\nprint(f\"Commit: {commit.hash}\")\nprint(f\"Message: {commit.message}\")\nprint(f\"Timestamp: {commit.timestamp}\")\nprint(f\"Files: {commit.list_files()}\")\nprint(f\"Short hash: {commit.short_hash}\")\n</code></pre> <p>Characteristics:</p> <ul> <li>Linear history: No branching, simple parent-child relationships</li> <li>Immutable: Cannot be changed once created</li> <li>Atomic: All files in a commit are added/removed together</li> <li>Traceable: Full history of changes over time</li> </ul>"},{"location":"getting-started/core-concepts/#content-addressed-storage","title":"Content-Addressed Storage","text":"<p>Content-addressed storage means files are stored and identified by their content hash, not their filename or location.</p> <p>Benefits:</p> <ul> <li>Data integrity: Files cannot be corrupted without detection</li> <li>Deduplication: Identical content stored only once</li> <li>Efficient storage: Saves space by avoiding duplicate files</li> <li>Tamper-proof: Any change to content changes the hash</li> </ul> <p>Storage Layout:</p> <pre><code>data/\n\u251c\u2500\u2500 ab/                  # First two characters of hash\n\u2502   \u2514\u2500\u2500 cdef1234...      # Rest of hash (no file extensions)\n\u2514\u2500\u2500 ...\n</code></pre> <p>Important: Files are stored without extensions in the content store. Original extensions are preserved as metadata in the File entity's <code>name</code> attribute and restored when files are accessed.</p>"},{"location":"getting-started/core-concepts/#catalogs","title":"Catalogs","text":"<p>A catalog is a collection of datasets that you want to manage together. It's like a workspace for multiple related datasets.</p> <pre><code>from kirin import Catalog\n\n# Create a catalog\ncatalog = Catalog(root_dir=\"/path/to/data\")\n\n# List all datasets\ndatasets = catalog.datasets()\nprint(f\"Available datasets: {datasets}\")\n\n# Get a specific dataset\ndataset = catalog.get_dataset(\"my_dataset\")\n</code></pre> <p>Use Cases:</p> <ul> <li>Project organization: Group related datasets</li> <li>Team collaboration: Share multiple datasets</li> <li>Workflow management: Organize data processing pipelines</li> </ul>"},{"location":"getting-started/core-concepts/#how-it-works","title":"How It Works","text":""},{"location":"getting-started/core-concepts/#1-file-storage","title":"1. File Storage","text":"<p>When you add a file to Kirin:</p> <ol> <li>Hash calculation: File content is hashed (SHA256)</li> <li>Content storage: File stored at <code>data/{hash[:2]}/{hash[2:]}</code></li> <li>Metadata tracking: Original filename stored as metadata</li> <li>Deduplication: If file already exists, no duplicate storage</li> </ol>"},{"location":"getting-started/core-concepts/#2-commit-process","title":"2. Commit Process","text":"<p>When you create a commit:</p> <ol> <li>File staging: Files to be added/removed are identified</li> <li>Hash resolution: Content hashes are calculated/resolved</li> <li>Commit creation: New commit object created with file references</li> <li>History update: Commit added to linear history</li> </ol>"},{"location":"getting-started/core-concepts/#3-data-access","title":"3. Data Access","text":"<p>When you access files:</p> <ol> <li>Commit resolution: Current commit or specific commit identified</li> <li>File lookup: File references resolved to content hashes</li> <li>Content retrieval: Files retrieved from content store</li> <li>Extension restoration: Original filenames restored</li> </ol>"},{"location":"getting-started/core-concepts/#linear-vs-branching","title":"Linear vs. Branching","text":"<p>Kirin uses linear commit history instead of Git's branching model:</p> <p>Linear History (Kirin):</p> <pre><code>Commit A \u2192 Commit B \u2192 Commit C \u2192 Commit D\n</code></pre> <p>Branching History (Git):</p> <pre><code>Commit A \u2192 Commit B \u2192 Commit C\n         \u2198\n           Commit D \u2192 Commit E\n</code></pre> <p>Benefits of Linear History:</p> <ul> <li>Simpler: No merge conflicts or complex branching</li> <li>Clearer: Easy to understand data evolution</li> <li>Safer: No risk of losing data through complex merges</li> <li>Faster: No need to resolve merge conflicts</li> </ul>"},{"location":"getting-started/core-concepts/#creating-branches-with-new-datasets","title":"Creating \"Branches\" with New Datasets","text":"<p>If you need branching-like functionality, create a new dataset using existing files:</p> <pre><code># Original dataset\noriginal_dataset = catalog.get_dataset(\"experiment_v1\")\n\n# Create a \"branch\" by starting a new dataset with existing files\nbranch_dataset = catalog.create_dataset(\"experiment_v2\")\n\n# Copy files from the original dataset to the new one\nwith original_dataset.local_files() as local_files:\n    for filename, local_path in local_files.items():\n        # Copy file to new dataset\n        import shutil\n        shutil.copy2(local_path, filename)\n\n# Commit the copied files to the new dataset\nbranch_dataset.commit(\n    message=\"Branch from experiment_v1\",\n    add_files=[\"data.csv\", \"config.json\"]\n)\n\n# Now you can develop independently in each dataset\noriginal_dataset.commit(\"Continue original work\", add_files=[\"new_data.csv\"])\nbranch_dataset.commit(\"Try different approach\", add_files=[\"alternative_data.csv\"])\n</code></pre> <p>Benefits of Dataset-based \"Branching\":</p> <ul> <li>Clear separation: Each dataset is independent</li> <li>Easy comparison: Compare datasets side by side</li> <li>No conflicts: No merge conflicts between datasets</li> <li>Flexible: Can share files between datasets as needed</li> </ul>"},{"location":"getting-started/core-concepts/#backend-agnostic-design","title":"Backend-Agnostic Design","text":"<p>Kirin works with any storage backend through the fsspec library:</p> <p>Supported Backends:</p> <ul> <li>Local filesystem: <code>/path/to/data</code></li> <li>AWS S3: <code>s3://bucket/path</code></li> <li>Google Cloud Storage: <code>gs://bucket/path</code></li> <li>Azure Blob Storage: <code>az://container/path</code></li> <li>And many more: Dropbox, Google Drive, etc. (sync/auth handled by backend)</li> </ul> <p>Benefits:</p> <ul> <li>Flexibility: Use any storage backend</li> <li>Scalability: Scale from local to cloud</li> <li>Portability: Move between backends easily</li> <li>Cost optimization: Choose the right storage for your needs</li> </ul>"},{"location":"getting-started/core-concepts/#zero-copy-operations","title":"Zero-Copy Operations","text":"<p>Kirin is designed with zero-copy philosophy for efficient large file handling:</p> <p>Zero-Copy Features:</p> <ul> <li>Memory-mapped files: Avoid loading entire files into memory</li> <li>Chunked processing: Process data incrementally using libraries like pandas</li> <li>Direct transfers: Stream between storage backends</li> <li>Reference-based operations: Use references instead of copying</li> </ul> <p>Benefits:</p> <ul> <li>Memory efficient: Handle files larger than RAM</li> <li>Fast operations: No unnecessary data copying</li> <li>Scalable: Work with datasets of any size</li> </ul>"},{"location":"getting-started/core-concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart - Try Kirin with a simple example</li> <li>Basic Usage Guide - Learn common workflows</li> <li>Working with Files - File operations and   patterns</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Choose the installation method that best fits your use case.</p>"},{"location":"getting-started/installation/#option-1-pixi-recommended-for-development","title":"Option 1: Pixi (Recommended for Development)","text":"<p>Best for contributors and developers who want the full development environment.</p> <pre><code># Clone and install\ngit clone git@github.com:nll-ai/kirin\ncd kirin\npixi install\n\n# Set up SSL certificates for cloud storage (one-time setup)\npixi run setup-ssl\n\n# Start the web UI\npixi run kirin ui\n</code></pre> <p>Benefits:</p> <ul> <li>Full development environment with all dependencies</li> <li>Easy to contribute to the project</li> <li>Includes testing and development tools</li> </ul>"},{"location":"getting-started/installation/#option-2-uv-tool-recommended-for-production","title":"Option 2: UV Tool (Recommended for Production)","text":"<p>Best for users who want a clean, isolated installation.</p> <pre><code># Install with uv\nuv tool install kirin\n\n# Set up SSL certificates (one-time setup)\nuv run python -m kirin.setup_ssl\n\n# Start the web UI\nuv run kirin ui\n</code></pre> <p>Benefits:</p> <ul> <li>Clean, isolated installation</li> <li>Easy to update and manage</li> <li>No system Python conflicts</li> </ul>"},{"location":"getting-started/installation/#option-3-uvx-one-time-use","title":"Option 3: UVX (One-time Use)","text":"<p>Best for trying out Kirin without permanent installation.</p> <pre><code># Run directly with uvx\nuvx kirin ui\n\n# If SSL issues occur, set up certificates\nuvx python -m kirin.setup_ssl\n</code></pre> <p>Benefits:</p> <ul> <li>No permanent installation</li> <li>Always uses latest version</li> <li>Good for experimentation</li> </ul>"},{"location":"getting-started/installation/#ssl-certificate-setup","title":"SSL Certificate Setup","text":"<p>When using isolated Python environments (pixi, uv, conda), SSL certificates are not automatically available. This affects HTTPS connections to cloud storage providers.</p>"},{"location":"getting-started/installation/#automatic-setup-recommended","title":"Automatic Setup (Recommended)","text":"<pre><code># Works with any Python environment - detects automatically\npython -m kirin.setup_ssl\n</code></pre>"},{"location":"getting-started/installation/#manual-setup-if-automatic-setup-fails","title":"Manual Setup (if automatic setup fails)","text":"<pre><code># The automatic setup script handles this automatically\n# But if you need to do it manually, use the Python executable path:\npython -c \"import sys; print('Python path:', sys.executable)\"\n# Then create ssl directory next to that path and copy certificates\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code># Check if Kirin is installed\npython -c \"import kirin; print('Kirin version:', kirin.__version__)\"\n\n# Test HTTPS connection\npython -c \"import requests; r = requests.get('https://storage.googleapis.com'); \\\nprint('HTTPS works:', r.status_code)\"\n\n# Or use the automatic setup\npython -m kirin.setup_ssl\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#ssl-certificate-issues","title":"SSL Certificate Issues","text":"<p>If you get SSL errors when connecting to cloud storage:</p> <ol> <li>Run the SSL setup: <code>python -m kirin.setup_ssl</code></li> <li>Check your Python environment: Make sure you're using the right Python</li> <li>Verify cloud credentials: Ensure your cloud authentication is set up correctly</li> </ol>"},{"location":"getting-started/installation/#import-errors","title":"Import Errors","text":"<p>If you get import errors:</p> <ol> <li>Check your Python environment: Make sure you're using the right Python</li> <li>Verify installation: Run <code>python -c \"import kirin\"</code></li> <li>Check dependencies: Ensure all required packages are installed</li> </ol>"},{"location":"getting-started/installation/#cloud-authentication-issues","title":"Cloud Authentication Issues","text":"<p>If you have trouble with cloud storage:</p> <ol> <li>See the Cloud Storage Guide for detailed setup</li> <li>Check your credentials: Verify AWS profiles, GCS tokens, etc.</li> <li>Test connectivity: Try a simple cloud operation first</li> </ol>"},{"location":"getting-started/installation/#development-setup","title":"Development Setup","text":"<p>If you want to contribute to Kirin:</p> <pre><code># Clone the repository\ngit clone git@github.com:nll-ai/kirin\ncd kirin\n\n# Install with pixi\npixi install\n\n# Set up SSL certificates\npixi run setup-ssl\n\n# Run tests\npixi run -e tests pytest\n\n# Start development server\npixi run kirin ui\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Core Concepts - Understanding how Kirin works</li> <li>Quickstart - Get started with your first dataset</li> <li>Cloud Storage Guide - Set up cloud storage</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>Get up and running with Kirin in 5 minutes!</p>"},{"location":"getting-started/quickstart/#what-is-kirin","title":"What is Kirin?","text":"<p>Kirin is simplified \"git\" for data - it provides linear versioning for datasets with content-addressed storage. Think of it as Git, but designed specifically for data scientists working with large datasets.</p>"},{"location":"getting-started/quickstart/#5-minute-quickstart","title":"5-Minute Quickstart","text":""},{"location":"getting-started/quickstart/#1-install-kirin","title":"1. Install Kirin","text":"<pre><code># Option 1: Using pixi (recommended for development)\ngit clone git@github.com:nll-ai/kirin\ncd kirin\npixi install\n\n# Option 2: Using uv tool (recommended for production)\nuv tool install kirin\n\n# Option 3: Using pip\npip install kirin\n</code></pre>"},{"location":"getting-started/quickstart/#2-create-your-first-dataset","title":"2. Create Your First Dataset","text":"<pre><code>from kirin import Catalog, Dataset\nfrom pathlib import Path\n\n# Create a catalog (works with local and cloud storage)\ncatalog = Catalog(root_dir=\"/path/to/data\")  # Local storage\n\n# Get or create a dataset\nds = catalog.get_dataset(\"my_first_dataset\")\n\n# Add some files to your dataset\ncommit_hash = ds.commit(\n    message=\"Initial commit\",\n    add_files=[\"data.csv\", \"config.json\"]\n)\n\nprint(f\"Created commit: {commit_hash}\")\n</code></pre>"},{"location":"getting-started/quickstart/#3-work-with-your-data","title":"3. Work with Your Data","text":"<pre><code># Checkout the latest commit\nds.checkout()\n\n# Access files from current commit\nfiles = ds.files\nprint(f\"Files in current commit: {list(files.keys())}\")\n\n# Work with files locally (recommended approach)\nwith ds.local_files() as local_files:\n    # Access files as local paths\n    csv_path = local_files[\"data.csv\"]\n\n    # Read file content\n    content = Path(csv_path).read_text()\n    print(f\"File content: {content[:100]}...\")\n</code></pre> <p>Tip: When you display a dataset, commit, or catalog in a notebook cell (e.g., <code>ds</code>), Kirin shows an interactive HTML view. Click \"Copy Code to Access\" on any file to copy code snippets to your clipboard. You can customize the variable name used in snippets by setting <code>dataset._repr_variable_name = \"my_name\"</code>.</p> <p>Note: The \"Copy Code to Access\" button requires browser clipboard access. It works in Jupyter/Marimo notebooks viewed in a web browser, but may not work in VSCode's embedded notebook viewer (as of December 2025).</p>"},{"location":"getting-started/quickstart/#4-view-your-commit-history","title":"4. View Your Commit History","text":"<pre><code># Get commit history\nhistory = ds.history(limit=10)\nfor commit in history:\n    print(f\"{commit.short_hash}: {commit.message}\")\n\n# Checkout a specific commit\nds.checkout(commit_hash)\n</code></pre>"},{"location":"getting-started/quickstart/#5-start-the-web-ui-optional","title":"5. Start the Web UI (Optional)","text":"<pre><code># Development (with pixi)\npixi run kirin ui\n\n# Production (with uv)\nuv run kirin ui\n\n# One-time use (with uvx)\nuvx kirin ui\n</code></pre> <p>The web UI provides a graphical interface for browsing datasets, viewing commit history, and managing your data catalogs.</p>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Detailed installation options</li> <li>Core Concepts - Understanding datasets, commits, and content-addressing</li> <li>Basic Usage Guide - Common workflows and patterns</li> <li>Cloud Storage Guide - Working with S3, GCS, Azure</li> </ul>"},{"location":"getting-started/quickstart/#key-benefits","title":"Key Benefits","text":"<ul> <li>Linear versioning: Simple, Git-like commits without branching complexity</li> <li>Content-addressed storage: Files stored by content hash for integrity and deduplication</li> <li>Cloud support: Works with S3, GCS, Azure, and more</li> <li>Ergonomic API: Designed for data science workflows</li> <li>Zero-copy operations: Efficient handling of large files</li> </ul>"},{"location":"getting-started/quickstart/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Experiment tracking: Version your training data and model inputs</li> <li>Data pipeline versioning: Track changes in ETL processes</li> <li>Collaborative research: Share datasets with exact version control</li> <li>Reproducible analysis: Ensure you can recreate your results</li> </ul>"},{"location":"guides/basic-usage/","title":"Basic Usage","text":"<p>Learn the essential workflows for working with Kirin datasets.</p>"},{"location":"guides/basic-usage/#creating-and-managing-datasets","title":"Creating and Managing Datasets","text":""},{"location":"guides/basic-usage/#create-a-new-dataset","title":"Create a New Dataset","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Create a catalog\ncatalog = Catalog(root_dir=\"/path/to/data\")\n\n# Create a new dataset\ndataset = catalog.create_dataset(\n    name=\"my_experiment\",\n    description=\"ML experiment data for Q1 2024\"\n)\n\nprint(f\"Created dataset: {dataset.name}\")\n</code></pre>"},{"location":"guides/basic-usage/#get-an-existing-dataset","title":"Get an Existing Dataset","text":"<pre><code># Get an existing dataset\ndataset = catalog.get_dataset(\"my_experiment\")\n\n# Check if dataset exists\nif dataset:\n    print(f\"Dataset has {len(dataset.history())} commits\")\nelse:\n    print(\"Dataset not found\")\n</code></pre>"},{"location":"guides/basic-usage/#adding-files-to-a-dataset","title":"Adding Files to a Dataset","text":"<pre><code># Add single file\ncommit_hash = dataset.commit(\n    message=\"Add initial data\",\n    add_files=[\"data.csv\"]\n)\n\n# Add multiple files\ncommit_hash = dataset.commit(\n    message=\"Add processed data\",\n    add_files=[\"data.csv\", \"config.json\", \"results.txt\"]\n)\n\nprint(f\"Created commit: {commit_hash}\")\n</code></pre>"},{"location":"guides/basic-usage/#removing-files-from-a-dataset","title":"Removing Files from a Dataset","text":"<pre><code># Remove single file\ndataset.commit(\n    message=\"Remove old data\",\n    remove_files=[\"old_data.csv\"]\n)\n\n# Remove multiple files\ndataset.commit(\n    message=\"Clean up dataset\",\n    remove_files=[\"temp1.csv\", \"temp2.json\"]\n)\n</code></pre>"},{"location":"guides/basic-usage/#combined-operations","title":"Combined Operations","text":"<pre><code># Add and remove files in same commit\ndataset.commit(\n    message=\"Update dataset\",\n    add_files=[\"new_data.csv\"],\n    remove_files=[\"old_data.csv\"]\n)\n</code></pre>"},{"location":"guides/basic-usage/#working-with-commits","title":"Working with Commits","text":""},{"location":"guides/basic-usage/#viewing-commit-history","title":"Viewing Commit History","text":"<pre><code># Get all commits\nhistory = dataset.history()\nfor commit in history:\n    print(f\"{commit.short_hash}: {commit.message}\")\n    print(f\"  Files: {commit.list_files()}\")\n    print(f\"  Date: {commit.timestamp}\")\n\n# Get limited history\nrecent_commits = dataset.history(limit=5)\n</code></pre>"},{"location":"guides/basic-usage/#checking-out-commits","title":"Checking Out Commits","text":"<pre><code># Checkout latest commit (default)\ndataset.checkout()\n\n# Checkout specific commit\ndataset.checkout(commit_hash)\n\n# Get current commit info\ncurrent_commit = dataset.current_commit\nif current_commit:\n    print(f\"Current commit: {current_commit.short_hash}\")\n    print(f\"Message: {current_commit.message}\")\n</code></pre>"},{"location":"guides/basic-usage/#comparing-commits","title":"Comparing Commits","text":"<pre><code># Get two commits\ncommit1 = dataset.get_commit(hash1)\ncommit2 = dataset.get_commit(hash2)\n\nif commit1 and commit2:\n    # Compare file lists\n    files1 = set(commit1.list_files())\n    files2 = set(commit2.list_files())\n\n    added = files2 - files1\n    removed = files1 - files2\n    common = files1 &amp; files2\n\n    print(f\"Added files: {added}\")\n    print(f\"Removed files: {removed}\")\n    print(f\"Common files: {common}\")\n</code></pre>"},{"location":"guides/basic-usage/#working-with-files","title":"Working with Files","text":""},{"location":"guides/basic-usage/#file-information","title":"File Information","text":"<pre><code># List files in current commit\nfiles = dataset.files\nprint(f\"Files: {list(files.keys())}\")\n\n# Check if file exists\nif dataset.has_file(\"data.csv\"):\n    print(\"File exists\")\n\n# Get file object\nfile_obj = dataset.get_file(\"data.csv\")\nif file_obj:\n    print(f\"File size: {file_obj.size} bytes\")\n    print(f\"Content hash: {file_obj.hash}\")\n</code></pre>"},{"location":"guides/basic-usage/#local-file-access-recommended-pattern","title":"Local File Access (Recommended Pattern)","text":"<p>The <code>local_files()</code> context manager is the recommended way to work with files:</p> <pre><code>from pathlib import Path\nimport pandas as pd\n\n# Work with files locally\nwith dataset.local_files() as local_files:\n    for filename, local_path in local_files.items():\n        print(f\"{filename} -&gt; {local_path}\")\n\n        # Process files with standard Python libraries\n        if filename.endswith('.csv'):\n            df = pd.read_csv(local_path)\n            print(f\"CSV shape: {df.shape}\")\n        elif filename.endswith('.json'):\n            import json\n            data = json.loads(Path(local_path).read_text())\n            print(f\"JSON keys: {list(data.keys())}\")\n</code></pre> <p>Benefits of local_files():</p> <ul> <li>Library compatibility: Works with pandas, polars, etc.</li> <li>Automatic cleanup: Files cleaned up when done</li> <li>Standard paths: Use normal Python file operations</li> <li>Memory efficient: No need to load entire files into memory</li> </ul>"},{"location":"guides/basic-usage/#dataset-information","title":"Dataset Information","text":""},{"location":"guides/basic-usage/#get-dataset-info","title":"Get Dataset Info","text":"<pre><code># Check if dataset is empty\nif dataset.is_empty():\n    print(\"Dataset has no commits\")\nelse:\n    print(f\"Dataset has {len(dataset.history())} commits\")\n\n# Get dataset information\ninfo = dataset.get_info()\nprint(f\"Dataset info: {info}\")\n\n# Convert to dictionary\ndataset_dict = dataset.to_dict()\nprint(f\"Serialized dataset: {dataset_dict}\")\n</code></pre>"},{"location":"guides/basic-usage/#cleanup-operations","title":"Cleanup Operations","text":"<pre><code># Clean up orphaned files (files not referenced by any commit)\nremoved_count = dataset.cleanup_orphaned_files()\nprint(f\"Removed {removed_count} orphaned files\")\n</code></pre>"},{"location":"guides/basic-usage/#common-workflows","title":"Common Workflows","text":""},{"location":"guides/basic-usage/#experiment-tracking","title":"Experiment Tracking","text":"<pre><code># Track ML experiment\ndataset = catalog.get_dataset(\"ml_experiment\")\n\n# Add training data\ndataset.commit(\n    message=\"Add training data\",\n    add_files=[\"train.csv\", \"train_labels.csv\"]\n)\n\n# Add model\ndataset.commit(\n    message=\"Add trained model\",\n    add_files=[\"model.pkl\", \"model_metrics.json\"]\n)\n\n# Add results\ndataset.commit(\n    message=\"Add experiment results\",\n    add_files=[\"results.csv\", \"plots.png\"]\n)\n</code></pre>"},{"location":"guides/basic-usage/#data-pipeline-versioning","title":"Data Pipeline Versioning","text":"<pre><code># Track ETL pipeline\ndataset = catalog.get_dataset(\"etl_pipeline\")\n\n# Raw data\ndataset.commit(\n    message=\"Add raw data\",\n    add_files=[\"raw_data.csv\"]\n)\n\n# Processed data\ndataset.commit(\n    message=\"Add processed data\",\n    add_files=[\"processed_data.csv\", \"processing_log.txt\"]\n)\n\n# Final output\ndataset.commit(\n    message=\"Add final output\",\n    add_files=[\"final_data.csv\", \"summary.json\"]\n)\n</code></pre>"},{"location":"guides/basic-usage/#collaborative-research","title":"Collaborative Research","text":"<pre><code># Share dataset with team\ndataset = catalog.get_dataset(\"research_data\")\n\n# Add initial data\ndataset.commit(\n    message=\"Initial dataset\",\n    add_files=[\"raw_data.csv\", \"metadata.json\"]\n)\n\n# Team member adds analysis\ndataset.commit(\n    message=\"Add analysis results\",\n    add_files=[\"analysis.py\", \"results.csv\", \"plots.png\"]\n)\n</code></pre>"},{"location":"guides/basic-usage/#best-practices","title":"Best Practices","text":""},{"location":"guides/basic-usage/#commit-messages","title":"Commit Messages","text":"<p>Use descriptive commit messages:</p> <pre><code># Good commit messages\ndataset.commit(\"Add Q1 sales data\", add_files=[\"sales_q1.csv\"])\ndataset.commit(\"Fix data quality issues\", add_files=[\"sales_q1.csv\"])\ndataset.commit(\"Add customer segmentation\", add_files=[\"segments.csv\"])\n\n# Avoid vague messages\ndataset.commit(\"Update\", add_files=[\"data.csv\"])  # Too vague\ndataset.commit(\"Fix\", add_files=[\"data.csv\"])     # Too vague\n</code></pre>"},{"location":"guides/basic-usage/#file-organization","title":"File Organization","text":"<p>Organize files logically:</p> <pre><code># Good organization\ndataset.commit(\"Add raw data\", add_files=[\"raw/sales.csv\", \"raw/customers.csv\"])\ndataset.commit(\"Add processed data\", add_files=[\"processed/sales_clean.csv\"])\ndataset.commit(\"Add analysis\", add_files=[\"analysis/results.csv\", \"analysis/plots.png\"])\n\n# Avoid flat structure\ndataset.commit(\"Add files\", add_files=[\"sales.csv\", \"customers.csv\",\n                                        \"results.csv\", \"plots.png\"])\n</code></pre>"},{"location":"guides/basic-usage/#regular-commits","title":"Regular Commits","text":"<p>Commit changes regularly:</p> <pre><code># Commit after each logical step\ndataset.commit(\"Add initial data\", add_files=[\"data.csv\"])\n# ... process data ...\ndataset.commit(\"Add processed data\", add_files=[\"processed.csv\"])\n# ... analyze data ...\ndataset.commit(\"Add analysis\", add_files=[\"results.csv\"])\n</code></pre>"},{"location":"guides/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Working with Files - Advanced file operations</li> <li>Commit Management - Understanding commit history</li> <li>Cloud Storage - Working with cloud backends</li> </ul>"},{"location":"guides/cloud-storage/","title":"Cloud Storage","text":"<p>Set up and use Kirin with cloud storage backends like S3, GCS, and Azure.</p>"},{"location":"guides/cloud-storage/#overview","title":"Overview","text":"<p>Kirin supports multiple cloud storage backends through the fsspec library. You can use the same API whether you're working with local files or cloud storage.</p> <p>Supported Backends:</p> <ul> <li>AWS S3: <code>s3://bucket/path</code></li> <li>Google Cloud Storage: <code>gs://bucket/path</code></li> <li>Azure Blob Storage: <code>az://container/path</code></li> <li>And many more: Dropbox, Google Drive, etc.</li> </ul>"},{"location":"guides/cloud-storage/#authentication-methods","title":"Authentication Methods","text":""},{"location":"guides/cloud-storage/#awss3-authentication","title":"AWS/S3 Authentication","text":""},{"location":"guides/cloud-storage/#using-aws-profile-recommended","title":"Using AWS Profile (Recommended)","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Using AWS profile\ncatalog = Catalog(\n    root_dir=\"s3://{{ bucket_name }}/data\",\n    aws_profile=\"{{ aws_profile }}\"\n)\n\n# Using Dataset with AWS profile\ndataset = Dataset(\n    root_dir=\"s3://my-bucket/data\",\n    name=\"{{ dataset_name }}\",\n    aws_profile=\"my-profile\"\n)\n</code></pre>"},{"location":"guides/cloud-storage/#using-environment-variables","title":"Using Environment Variables","text":"<p>Set environment variables in your shell or system:</p> <pre><code># Set AWS credentials\nexport AWS_ACCESS_KEY_ID={{ access_key_id }}\nexport AWS_SECRET_ACCESS_KEY={{ secret_access_key }}\nexport AWS_DEFAULT_REGION={{ region }}\n\n# Set Azure credentials\nexport AZURE_CONNECTION_STRING={{ azure_connection_string }}\n</code></pre> <p>Then use without explicit credentials:</p> <pre><code># Environment variables are automatically detected\ncatalog = Catalog(root_dir=\"s3://{{ bucket_name }}/data\")\n</code></pre>"},{"location":"guides/cloud-storage/#using-iam-roles-ec2ecslambda","title":"Using IAM Roles (EC2/ECS/Lambda)","text":"<pre><code># No explicit credentials needed - uses IAM role automatically\ncatalog = Catalog(root_dir=\"s3://my-bucket/data\")\n</code></pre>"},{"location":"guides/cloud-storage/#using-aws-sso","title":"Using AWS SSO","text":"<pre><code># After running: aws sso login\ncatalog = Catalog(\n    root_dir=\"s3://{{ bucket_name }}/data\",\n    aws_profile=\"{{ sso_profile_name }}\"\n)\n</code></pre>"},{"location":"guides/cloud-storage/#gcpgcs-authentication","title":"GCP/GCS Authentication","text":""},{"location":"guides/cloud-storage/#using-service-account-key-file","title":"Using Service Account Key File","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Using service account key file\ncatalog = Catalog(\n    root_dir=\"gs://{{ bucket_name }}/data\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"{{ project_id }}\"\n)\n\n# Using Dataset with GCS credentials\ndataset = Dataset(\n    root_dir=\"gs://{{ bucket_name }}/data\",\n    name=\"{{ dataset_name }}\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"{{ project_id }}\"\n)\n</code></pre>"},{"location":"guides/cloud-storage/#using-application-default-credentials","title":"Using Application Default Credentials","text":"<pre><code># Set up ADC (one-time setup)\n# gcloud auth application-default login\n\n# Use without explicit credentials (automatically detects ADC)\ncatalog = Catalog(root_dir=\"gs://my-bucket/data\")\n</code></pre>"},{"location":"guides/cloud-storage/#using-environment-variables-gcs","title":"Using Environment Variables (GCS)","text":"<pre><code>import os\n\n# Set environment variable\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/path/to/service-account.json\"\n\n# Use without explicit credentials (automatically detects environment)\ncatalog = Catalog(root_dir=\"gs://my-bucket/data\")\n</code></pre>"},{"location":"guides/cloud-storage/#using-workload-identity-gkekubernetes","title":"Using Workload Identity (GKE/Kubernetes)","text":"<p>When running on GKE with Workload Identity configured, Application Default Credentials automatically detect credentials from the metadata server (which is how Workload Identity works):</p> <pre><code># No explicit credentials needed - uses ADC (which includes Workload Identity on GKE)\ncatalog = Catalog(root_dir=\"gs://my-bucket/data\")\n</code></pre>"},{"location":"guides/cloud-storage/#azure-blob-storage-authentication","title":"Azure Blob Storage Authentication","text":""},{"location":"guides/cloud-storage/#using-connection-string","title":"Using Connection String","text":"<pre><code>from kirin import Catalog, Dataset\n\n# Using connection string\ncatalog = Catalog(\n    root_dir=\"az://{{ container_name }}/data\",\n    azure_connection_string=os.getenv(\"AZURE_CONNECTION_STRING\")\n)\n\n# Using Dataset with connection string\ndataset = Dataset(\n    root_dir=\"az://{{ container_name }}/data\",\n    name=\"{{ dataset_name }}\",\n    azure_connection_string=os.getenv(\"AZURE_CONNECTION_STRING\")\n)\n</code></pre>"},{"location":"guides/cloud-storage/#using-account-name-and-key","title":"Using Account Name and Key","text":"<pre><code>catalog = Catalog(\n    root_dir=\"az://{{ container_name }}/data\",\n    azure_account_name=\"myaccount\",\n    azure_account_key=\"mykey\"\n)\n</code></pre>"},{"location":"guides/cloud-storage/#using-environment-variables-azure","title":"Using Environment Variables (Azure)","text":"<pre><code>import os\n\n# Set environment variables\nos.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"] = \"myaccount\"\nos.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"] = \"mykey\"\n\n# Use without explicit credentials (automatically detects environment)\ncatalog = Catalog(root_dir=\"az://my-container/data\")\n</code></pre>"},{"location":"guides/cloud-storage/#using-azure-cli-authentication","title":"Using Azure CLI Authentication","text":"<pre><code># After running: az login\n# No explicit credentials needed - uses Azure CLI authentication\ncatalog = Catalog(root_dir=\"az://my-container/data\")\n</code></pre>"},{"location":"guides/cloud-storage/#using-managed-identity-azure-vmsapp-service","title":"Using Managed Identity (Azure VMs/App Service)","text":"<pre><code># No explicit credentials needed - uses managed identity\ncatalog = Catalog(root_dir=\"az://my-container/data\")\n</code></pre>"},{"location":"guides/cloud-storage/#working-with-cloud-storage","title":"Working with Cloud Storage","text":""},{"location":"guides/cloud-storage/#basic-operations","title":"Basic Operations","text":"<pre><code># Create catalog with cloud storage\ncatalog = Catalog(root_dir=\"s3://my-bucket/data\")\n\n# Create dataset\ndataset = catalog.create_dataset(\"cloud_dataset\")\n\n# Add files (same API as local storage)\ncommit_hash = dataset.commit(\n    message=\"Add data to cloud\",\n    add_files=[\"data.csv\", \"config.json\"]\n)\n\n# Work with files (same API as local storage)\nwith dataset.local_files() as local_files:\n    for filename, local_path in local_files.items():\n        print(f\"{filename} -&gt; {local_path}\")\n        # Process files normally\n</code></pre>"},{"location":"guides/cloud-storage/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/cloud-storage/#processing-large-files","title":"Processing Large Files","text":"<pre><code># For large files, use chunked processing\nwith dataset.local_files() as local_files:\n    if \"large_data.csv\" in local_files:\n        local_path = local_files[\"large_data.csv\"]\n        # Use pandas chunking for large files\n        for chunk in pd.read_csv(local_path, chunksize=10000):\n            print(f\"Processing chunk with {len(chunk)} rows\")\n            process_chunk(chunk)\n</code></pre>"},{"location":"guides/cloud-storage/#batch-operations","title":"Batch Operations","text":"<pre><code># Batch multiple operations for better performance\nfiles_to_add = [\"file1.csv\", \"file2.csv\", \"file3.csv\"]\ndataset.commit(\n    message=\"Add multiple files\",\n    add_files=files_to_add\n)\n</code></pre>"},{"location":"guides/cloud-storage/#error-handling","title":"Error Handling","text":"<pre><code>import boto3\nfrom botocore.exceptions import ClientError\n\ntry:\n    catalog = Catalog(root_dir=\"s3://my-bucket/data\")\n    dataset = catalog.get_dataset(\"my-dataset\")\nexcept ClientError as e:\n    if e.response['Error']['Code'] == 'NoSuchBucket':\n        print(\"Bucket does not exist\")\n    elif e.response['Error']['Code'] == 'AccessDenied':\n        print(\"Access denied - check your credentials\")\n    else:\n        print(f\"AWS error: {e}\")\nexcept Exception as e:\n    print(f\"General error: {e}\")\n</code></pre>"},{"location":"guides/cloud-storage/#web-ui-cloud-integration","title":"Web UI Cloud Integration","text":""},{"location":"guides/cloud-storage/#setting-up-cloud-catalogs","title":"Setting Up Cloud Catalogs","text":"<p>The web UI supports cloud storage through a simple interface:</p> <ol> <li>Authenticate with your cloud provider using their CLI tools:</li> </ol> <pre><code># AWS\naws configure\n\n# GCP\ngcloud auth login\n\n# Azure\naz login\n</code></pre> <ol> <li>Create catalog in web UI:</li> <li>Click \"Add Catalog\" in the web interface</li> <li>Enter catalog details (ID, name, root directory)</li> <li>For S3: Select AWS profile from dropdown</li> <li> <p>For GCS/Azure: Ensure credentials are configured via environment variables      or CLI</p> </li> <li> <p>Authentication handling:</p> </li> <li>S3: Web UI provides profile selection</li> <li>GCS/Azure: Requires pre-configured credentials (environment variables,      CLI auth, etc.)</li> </ol>"},{"location":"guides/cloud-storage/#cloud-authentication-in-web-ui","title":"Cloud Authentication in Web UI","text":"<ol> <li>Create catalog with cloud URL: Use <code>s3://</code>, <code>gs://</code>, or <code>az://</code> URLs</li> <li>AWS Profile Selection: Web UI provides AWS profile dropdown for S3    authentication</li> <li>Other Cloud Providers: For GCS and Azure, authentication must be    configured programmatically or via environment variables</li> <li>Credentials stored securely: AWS profiles saved in catalog configuration</li> <li>Automatic authentication: Subsequent uses authenticate automatically</li> </ol>"},{"location":"guides/cloud-storage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/cloud-storage/#common-issues","title":"Common Issues","text":""},{"location":"guides/cloud-storage/#ssl-certificate-errors","title":"SSL Certificate Errors","text":"<pre><code># Set up SSL certificates for isolated Python environments\npython -m kirin.setup_ssl\n</code></pre>"},{"location":"guides/cloud-storage/#authentication-failures","title":"Authentication Failures","text":"<pre><code># Check your credentials\nimport boto3\n\n# Test AWS credentials\nsession = boto3.Session(profile_name=\"my-profile\")\ns3 = session.client('s3')\ns3.list_buckets()  # Should work without errors\n</code></pre>"},{"location":"guides/cloud-storage/#permission-issues","title":"Permission Issues","text":"<pre><code># Check bucket permissions\nimport boto3\n\ns3 = boto3.client('s3')\ntry:\n    s3.head_bucket(Bucket='my-bucket')\n    print(\"Bucket accessible\")\nexcept ClientError as e:\n    print(f\"Bucket not accessible: {e}\")\n</code></pre>"},{"location":"guides/cloud-storage/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/cloud-storage/#use-appropriate-regions","title":"Use Appropriate Regions","text":"<pre><code># Use same region as your compute resources\ncatalog = Catalog(\n    root_dir=\"s3://{{ bucket_name }}/data\",\n    aws_profile=\"{{ aws_profile }}\"\n)\n# Ensure bucket is in same region as your compute\n</code></pre>"},{"location":"guides/cloud-storage/#optimize-file-sizes","title":"Optimize File Sizes","text":"<pre><code># For very large files, consider chunking\n# Split large files into smaller chunks\ndataset.commit(\n    message=\"Add chunked data\",\n    add_files=[\"chunk_001.csv\", \"chunk_002.csv\", \"chunk_003.csv\"]\n)\n</code></pre>"},{"location":"guides/cloud-storage/#use-compression","title":"Use Compression","text":"<pre><code># Compress files before adding to reduce storage costs\nimport gzip\nimport shutil\n\n# Compress file\nwith open(\"data.csv\", \"rb\") as f_in:\n    with gzip.open(\"data.csv.gz\", \"wb\") as f_out:\n        shutil.copyfileobj(f_in, f_out)\n\n# Add compressed file\ndataset.commit(\n    message=\"Add compressed data\",\n    add_files=[\"data.csv.gz\"]\n)\n</code></pre>"},{"location":"guides/cloud-storage/#best-practices","title":"Best Practices","text":""},{"location":"guides/cloud-storage/#security","title":"Security","text":"<ul> <li>Use IAM roles when possible instead of access keys</li> <li>Rotate credentials regularly</li> <li>Use least privilege - only grant necessary permissions</li> <li>Monitor access through cloud provider audit logs</li> </ul>"},{"location":"guides/cloud-storage/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Use appropriate storage classes (S3 Standard, IA, Glacier)</li> <li>Enable lifecycle policies for automatic archival</li> <li>Monitor usage through cloud provider dashboards</li> <li>Use compression for text files</li> </ul>"},{"location":"guides/cloud-storage/#performance","title":"Performance","text":"<ul> <li>Use same region as your compute resources</li> <li>Batch operations when possible</li> <li>Use chunked processing for large files</li> <li>Consider CDN for frequently accessed data</li> </ul>"},{"location":"guides/cloud-storage/#next-steps","title":"Next Steps","text":"<ul> <li>Working with Files - Advanced file operations</li> <li>Web UI Overview - Using the web interface</li> <li>Basic Usage - Core dataset operations</li> </ul>"},{"location":"guides/commit-management/","title":"Commit Management","text":"<p>Understanding and working with Kirin's linear commit history.</p>"},{"location":"guides/commit-management/#understanding-commits","title":"Understanding Commits","text":""},{"location":"guides/commit-management/#what-is-a-commit","title":"What is a Commit?","text":"<p>A commit in Kirin represents an immutable snapshot of files at a point in time. Unlike Git, Kirin uses a linear commit history - each commit has exactly one parent commit, creating a simple chain of changes.</p> <pre><code>Commit A \u2192 Commit B \u2192 Commit C \u2192 Commit D\n</code></pre>"},{"location":"guides/commit-management/#commit-properties","title":"Commit Properties","text":"<p>Every commit has these key properties:</p> <ul> <li>Hash: Unique identifier (SHA256)</li> <li>Message: Human-readable description</li> <li>Timestamp: When the commit was created</li> <li>Parent: Reference to previous commit (linear history)</li> <li>Files: Dictionary of files in this commit</li> </ul> <pre><code># Get commit information\ncommit = dataset.get_commit(commit_hash)\nif commit:\n    print(f\"Commit hash: {commit.hash}\")\n    print(f\"Short hash: {commit.short_hash}\")\n    print(f\"Message: {commit.message}\")\n    print(f\"Timestamp: {commit.timestamp}\")\n    print(f\"Parent: {commit.parent_hash}\")\n    print(f\"Files: {commit.list_files()}\")\n    print(f\"Total size: {commit.get_total_size()} bytes\")\n</code></pre>"},{"location":"guides/commit-management/#creating-commits","title":"Creating Commits","text":""},{"location":"guides/commit-management/#commit-method-parameters","title":"Commit Method Parameters","text":"<p>The <code>commit()</code> method accepts the following parameters:</p> <ul> <li>message (required): Human-readable description of the changes</li> <li>add_files (optional): List of file paths, model objects, or plot   objects to add</li> <li>remove_files (optional): List of filenames to remove from the dataset</li> </ul> <pre><code># Add new files\ndataset.commit(message=\"Add new data\", add_files=[\"data.csv\", \"metadata.json\"])\n\n# Remove files\ndataset.commit(message=\"Remove old files\", remove_files=[\"old_data.csv\"])\n\n# Add and remove files in the same commit\ndataset.commit(\n    message=\"Update dataset\",\n    add_files=[\"new_data.csv\"],\n    remove_files=[\"old_data.csv\"]\n)\n</code></pre>"},{"location":"guides/commit-management/#committing-plot-objects","title":"Committing Plot Objects","text":"<p>You can commit matplotlib and plotly figure objects directly - they are automatically converted to files with format auto-detection (SVG for vector plots, WebP for raster plots):</p> <pre><code>import matplotlib.pyplot as plt\n\n# Create a plot\nfig, ax = plt.subplots()\nax.plot([1, 2, 3], [1, 4, 9])\nax.set_title(\"Training Progress\")\n\n# Commit the plot object directly\ndataset.commit(\n    message=\"Add training visualization\",\n    add_files=[fig]  # Automatically converted to SVG\n)\n\n# Mix plots with other files\nfig2, ax2 = plt.subplots()\nax2.scatter([1, 2, 3], [4, 5, 6])\n\ndataset.commit(\n    message=\"Add analysis plots\",\n    add_files=[fig, fig2, \"data.csv\"]  # Plots + regular file\n)\n</code></pre> <p>Format Auto-Detection:</p> <ul> <li> <p>SVG (vector): Default format for matplotlib and plotly figures. Best for line   plots, scatter plots, and other vector-based visualizations. Provides infinite   scalability without quality loss.</p> </li> <li> <p>WebP (raster): Automatically used for plots with raster elements (e.g., images,   heatmaps). Provides good compression while maintaining quality.</p> </li> </ul> <p>The format is automatically chosen based on the plot type. You don't need to specify it manually - Kirin handles it for you.</p>"},{"location":"guides/commit-management/#error-handling","title":"Error Handling","text":"<p>The <code>commit()</code> method can raise the following exceptions:</p> <ul> <li>ValueError: If no changes are specified (both <code>add_files</code> and   <code>remove_files</code> are empty)</li> <li>FileNotFoundError: If a file in <code>add_files</code> doesn't exist</li> </ul> <pre><code>try:\n    # This will raise ValueError\n    dataset.commit(message=\"No changes\")\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n\ntry:\n    # This will raise FileNotFoundError if file doesn't exist\n    dataset.commit(message=\"Add file\", add_files=[\"nonexistent.csv\"])\nexcept FileNotFoundError as e:\n    print(f\"File not found: {e}\")\n</code></pre>"},{"location":"guides/commit-management/#working-with-commit-history","title":"Working with Commit History","text":""},{"location":"guides/commit-management/#viewing-history","title":"Viewing History","text":"<pre><code># Get all commits\nhistory = dataset.history()\nfor commit in history:\n    print(f\"{commit.short_hash}: {commit.message}\")\n    print(f\"  Date: {commit.timestamp}\")\n    print(f\"  Files: {commit.list_files()}\")\n    print()\n\n# Get limited history\nrecent_commits = dataset.history(limit=5)\nfor commit in recent_commits:\n    print(f\"{commit.short_hash}: {commit.message}\")\n</code></pre>"},{"location":"guides/commit-management/#navigating-history","title":"Navigating History","text":"<pre><code># Checkout latest commit (default)\ndataset.checkout()\n\n# Checkout specific commit\ndataset.checkout(commit_hash)\n\n# Get current commit\ncurrent_commit = dataset.current_commit\nif current_commit:\n    print(f\"Current commit: {current_commit.short_hash}\")\n    print(f\"Message: {current_commit.message}\")\n</code></pre>"},{"location":"guides/commit-management/#comparing-commits","title":"Comparing Commits","text":"<pre><code>def compare_commits(dataset, commit1_hash, commit2_hash):\n    \"\"\"Compare two commits to see what changed.\"\"\"\n    commit1 = dataset.get_commit(commit1_hash)\n    commit2 = dataset.get_commit(commit2_hash)\n\n    if not commit1 or not commit2:\n        print(\"One or both commits not found\")\n        return\n\n    files1 = set(commit1.list_files())\n    files2 = set(commit2.list_files())\n\n    added = files2 - files1\n    removed = files1 - files2\n    common = files1 &amp; files2\n\n    print(f\"Added files: {added}\")\n    print(f\"Removed files: {removed}\")\n    print(f\"Common files: {common}\")\n\n    # Check if common files changed\n    for filename in common:\n        file1 = commit1.get_file(filename)\n        file2 = commit2.get_file(filename)\n        if file1.hash != file2.hash:\n            print(f\"Changed: {filename}\")\n\n# Use the comparison function\ncompare_commits(dataset, \"abc123\", \"def456\")\n</code></pre>"},{"location":"guides/commit-management/#commit-workflows","title":"Commit Workflows","text":""},{"location":"guides/commit-management/#linear-development","title":"Linear Development","text":"<p>Kirin's linear history is perfect for data science workflows:</p> <pre><code># Initial data\ndataset.commit(message=\"Add raw data\", add_files=[\"raw_data.csv\"])\n\n# Data cleaning\ndataset.commit(message=\"Clean data\", add_files=[\"cleaned_data.csv\"])\n\n# Feature engineering\ndataset.commit(message=\"Add features\", add_files=[\"features.csv\"])\n\n# Model training\ndataset.commit(message=\"Add trained model\", add_files=[\"model.pkl\"])\n\n# Results\ndataset.commit(message=\"Add results\", add_files=[\"results.csv\", \"plots.png\"])\n</code></pre>"},{"location":"guides/commit-management/#experiment-tracking","title":"Experiment Tracking","text":"<p>Track different experiments as separate commits:</p> <pre><code># Experiment 1: Random Forest\ndataset.commit(message=\"RF experiment\", add_files=[\"rf_model.pkl\", \"rf_results.csv\"])\n\n# Experiment 2: Gradient Boosting\ndataset.commit(message=\"GB experiment\", add_files=[\"gb_model.pkl\", \"gb_results.csv\"])\n\n# Experiment 3: Neural Network\ndataset.commit(message=\"NN experiment\", add_files=[\"nn_model.pkl\", \"nn_results.csv\"])\n</code></pre>"},{"location":"guides/commit-management/#data-pipeline-versioning","title":"Data Pipeline Versioning","text":"<p>Version your data processing pipeline outputs:</p> <pre><code># Raw data ingestion\ndataset.commit(message=\"Ingest raw data\", add_files=[\"raw/sales.csv\", \"raw/customers.csv\"])\n\n# Data validation\ndataset.commit(message=\"Validate data\", add_files=[\"validated/sales.csv\", \"validated/customers.csv\"])\n\n# Data transformation\ndataset.commit(message=\"Transform data\", add_files=[\"transformed/sales_clean.csv\"])\n\n# Feature engineering\ndataset.commit(message=\"Create features\", add_files=[\"features/engineered_features.csv\"])\n\n# Final output\ndataset.commit(message=\"Final dataset\", add_files=[\"final/dataset.csv\"])\n</code></pre>"},{"location":"guides/commit-management/#advanced-commit-operations","title":"Advanced Commit Operations","text":""},{"location":"guides/commit-management/#commit-information","title":"Commit Information","text":"<pre><code>def analyze_commit(commit):\n    \"\"\"Analyze a commit for detailed information.\"\"\"\n    print(f\"Commit: {commit.short_hash}\")\n    print(f\"Message: {commit.message}\")\n    print(f\"Date: {commit.timestamp}\")\n    print(f\"Parent: {commit.parent_hash}\")\n    print(f\"Files: {len(commit.files)}\")\n    print(f\"Total size: {commit.get_total_size()} bytes\")\n\n    # File details\n    for filename, file_obj in commit.files.items():\n        print(f\"  {filename}: {file_obj.size} bytes ({file_obj.short_hash})\")\n\n# Analyze current commit\ncurrent_commit = dataset.current_commit\nif current_commit:\n    analyze_commit(current_commit)\n</code></pre>"},{"location":"guides/commit-management/#commit-statistics","title":"Commit Statistics","text":"<pre><code>def commit_statistics(dataset):\n    \"\"\"Get statistics about the commit history.\"\"\"\n    history = dataset.history()\n\n    if not history:\n        print(\"No commits found\")\n        return\n\n    total_commits = len(history)\n    total_size = sum(commit.get_total_size() for commit in history)\n    avg_size = total_size / total_commits\n\n    print(f\"Total commits: {total_commits}\")\n    print(f\"Total size: {total_size / (1024*1024):.1f} MB\")\n    print(f\"Average commit size: {avg_size / (1024*1024):.1f} MB\")\n\n    # File frequency\n    file_counts = {}\n    for commit in history:\n        for filename in commit.list_files():\n            file_counts[filename] = file_counts.get(filename, 0) + 1\n\n    print(f\"Most frequently changed files:\")\n    for filename, count in sorted(file_counts.items(), key=lambda x: x[1], reverse=True)[:5]:\n        print(f\"  {filename}: {count} commits\")\n\n# Get statistics\ncommit_statistics(dataset)\n</code></pre>"},{"location":"guides/commit-management/#commit-best-practices","title":"Commit Best Practices","text":""},{"location":"guides/commit-management/#commit-messages","title":"Commit Messages","text":"<p>Write clear, descriptive commit messages:</p> <pre><code># Good commit messages\ndataset.commit(message=\"Add Q1 2024 sales data\", add_files=[\"sales_q1_2024.csv\"])\ndataset.commit(message=\"Fix data quality issues in customer records\", add_files=[\"customers_cleaned.csv\"])\ndataset.commit(message=\"Add feature engineering for ML model\", add_files=[\"features.csv\"])\n\n# Avoid vague messages\ndataset.commit(message=\"Update\", add_files=[\"data.csv\"])\ndataset.commit(message=\"Fix\", add_files=[\"file.csv\"])\ndataset.commit(message=\"Add stuff\", add_files=[\"file1.csv\", \"file2.csv\"])\n</code></pre>"},{"location":"guides/commit-management/#commit-frequency","title":"Commit Frequency","text":"<p>Commit changes regularly:</p> <pre><code># Commit after each logical step\ndataset.commit(message=\"Add raw data\", add_files=[\"raw_data.csv\"])\n# ... process data ...\ndataset.commit(message=\"Add cleaned data\", add_files=[\"cleaned_data.csv\"])\n# ... analyze data ...\ndataset.commit(message=\"Add analysis results\", add_files=[\"results.csv\"])\n</code></pre>"},{"location":"guides/commit-management/#atomic-commits","title":"Atomic Commits","text":"<p>Make commits atomic (single logical change):</p> <pre><code># Good: Single logical change\ndataset.commit(message=\"Add customer data\", add_files=[\"customers.csv\"])\n\n# Good: Related changes together\ndataset.commit(message=\"Update customer data and add validation\",\n               add_files=[\"customers_updated.csv\", \"validation_rules.json\"])\n\n# Avoid: Unrelated changes\ndataset.commit(message=\"Add customer data and fix bug\",\n               add_files=[\"customers.csv\", \"bug_fix.py\"])\n</code></pre>"},{"location":"guides/commit-management/#working-with-specific-commits","title":"Working with Specific Commits","text":""},{"location":"guides/commit-management/#accessing-files-from-specific-commits","title":"Accessing Files from Specific Commits","text":"<pre><code>def get_files_from_commit(dataset, commit_hash):\n    \"\"\"Get files from a specific commit.\"\"\"\n    commit = dataset.get_commit(commit_hash)\n    if not commit:\n        print(\"Commit not found\")\n        return\n\n    # Checkout the commit\n    dataset.checkout(commit_hash)\n\n    # Access files\n    files = dataset.files\n    print(f\"Files in commit {commit_hash}:\")\n    for filename, file_obj in files.items():\n        print(f\"  {filename}: {file_obj.size} bytes\")\n\n    return files\n\n# Get files from specific commit\nfiles = get_files_from_commit(dataset, \"abc123\")\n</code></pre>"},{"location":"guides/commit-management/#commit-history-visualization","title":"Commit History Visualization","text":""},{"location":"guides/commit-management/#commit-timeline","title":"Commit Timeline","text":"<pre><code>def create_timeline(dataset):\n    \"\"\"Create a timeline of commits.\"\"\"\n    history = dataset.history()\n\n    print(\"Commit Timeline:\")\n    print(\"=\" * 30)\n\n    for commit in history:\n        date_str = commit.timestamp.strftime(\"%Y-%m-%d %H:%M\")\n        print(f\"{date_str} | {commit.short_hash} | {commit.message}\")\n\n# Create timeline\ncreate_timeline(dataset)\n</code></pre>"},{"location":"guides/commit-management/#troubleshooting-commits","title":"Troubleshooting Commits","text":""},{"location":"guides/commit-management/#finding-lost-commits","title":"Finding Lost Commits","text":"<pre><code>def find_commit_by_message(dataset, search_term):\n    \"\"\"Find commits by message content.\"\"\"\n    history = dataset.history()\n\n    for commit in history:\n        if search_term.lower() in commit.message.lower():\n            print(f\"Found: {commit.short_hash} - {commit.message}\")\n            return commit\n\n    print(f\"No commits found matching '{search_term}'\")\n    return None\n\n# Find commit\ncommit = find_commit_by_message(dataset, \"model\")\n</code></pre>"},{"location":"guides/commit-management/#recovering-from-mistakes","title":"Recovering from Mistakes","text":"<pre><code>def recover_from_mistake(dataset, good_commit_hash):\n    \"\"\"Recover from a mistake by checking out a good commit.\"\"\"\n    # Checkout the good commit\n    dataset.checkout(good_commit_hash)\n\n    # Verify we're on the right commit\n    current_commit = dataset.current_commit\n    if current_commit and current_commit.hash == good_commit_hash:\n        print(f\"Successfully recovered to commit {good_commit_hash}\")\n        print(f\"Current commit: {current_commit.message}\")\n    else:\n        print(\"Failed to recover to specified commit\")\n\n# Recover from mistake\nrecover_from_mistake(dataset, \"abc123\")\n</code></pre>"},{"location":"guides/commit-management/#next-steps","title":"Next Steps","text":"<ul> <li>Working with Files - File operations and patterns</li> <li>Basic Usage - Core dataset operations</li> <li>Cloud Storage - Working with cloud backends</li> </ul>"},{"location":"guides/model-versioning/","title":"Model Versioning with Kirin","text":"<p>Kirin provides powerful model versioning capabilities that make it easy to track, compare, and manage machine learning models throughout their lifecycle. This guide shows you how to use Kirin for model versioning workflows.</p>"},{"location":"guides/model-versioning/#overview","title":"Overview","text":"<p>Model versioning with Kirin enables you to:</p> <ul> <li>Track model artifacts alongside rich metadata (hyperparameters, metrics,   training info)</li> <li>Tag models for different stages (dev, staging, production) or versions</li> <li>Query and discover models by performance metrics, tags, or custom criteria</li> <li>Compare model versions to understand what changed between iterations</li> <li>Maintain linear history with simple, git-like semantics</li> <li>Store models anywhere - local filesystem, S3, GCS, Azure, etc.</li> </ul>"},{"location":"guides/model-versioning/#basic-model-versioning-workflow","title":"Basic Model Versioning Workflow","text":""},{"location":"guides/model-versioning/#1-initialize-a-model-registry","title":"1. Initialize a Model Registry","text":"<pre><code>from kirin import Dataset\n\n# Create a model registry (works with any storage backend)\nmodel_registry = Dataset(\n    root_dir=\"s3://my-bucket/models\",  # or local path, GCS, Azure, etc.\n    name=\"sentiment_classifier\"\n)\n</code></pre>"},{"location":"guides/model-versioning/#2-save-your-first-model","title":"2. Save Your First Model","text":""},{"location":"guides/model-versioning/#option-a-committing-model-objects-directly-recommended-for-scikit-learn","title":"Option A: Committing Model Objects Directly (Recommended for scikit-learn)","text":"<p>Kirin can automatically handle scikit-learn model objects, serializing them and extracting hyperparameters and metrics:</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Load and prepare data\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Train model\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Commit model object directly - everything is automatic!\ncommit_hash = model_registry.commit(\n    message=\"Initial baseline model\",\n    add_files=[model],  # Just pass the model object\n    metadata={\n        \"accuracy\": model.score(X_test, y_test),  # Data-dependent metrics\n        \"dataset\": \"iris\",\n    },\n    tags=[\"baseline\", \"v1.0\"]\n)\n\n# Hyperparameters and metrics are automatically extracted:\n# - model.get_params() \u2192 metadata[\"models\"][\"model\"][\"hyperparameters\"]\n# - model.feature_importances_ \u2192 metadata[\"models\"][\"model\"][\"metrics\"]\n# - Source file linking \u2192 metadata[\"models\"][\"model\"][\"source_file\"]\n</code></pre>"},{"location":"guides/model-versioning/#option-b-manual-serialization-for-pytorch-tensorflow-etc","title":"Option B: Manual Serialization (For PyTorch, TensorFlow, etc.)","text":"<p>For frameworks that don't have automatic support yet, manually serialize:</p> <pre><code>import torch\n\n# Save your model files\ntorch.save(model.state_dict(), \"model_weights.pt\")\n\n# Commit with metadata and tags\ncommit_hash = model_registry.commit(\n    message=\"Initial baseline model\",\n    add_files=[\"model_weights.pt\", \"config.json\"],\n    metadata={\n        \"framework\": \"pytorch\",\n        \"accuracy\": 0.87,\n        \"f1_score\": 0.85,\n        \"hyperparameters\": {\n            \"learning_rate\": 0.001,\n            \"epochs\": 10,\n            \"batch_size\": 32\n        },\n        \"training_data\": \"sentiment_v1\",\n        \"model_size_mb\": 0.5\n    },\n    tags=[\"baseline\", \"v1.0\"]\n)\n</code></pre>"},{"location":"guides/model-versioning/#3-save-improved-models","title":"3. Save Improved Models","text":"<p>With scikit-learn (automatic):</p> <pre><code># Train an improved model\nimproved_model = RandomForestClassifier(\n    n_estimators=200,  # More trees\n    max_depth=10,  # Deeper trees\n    random_state=42\n)\nimproved_model.fit(X_train, y_train)\n\n# Commit improved model - hyperparameters auto-extracted\ncommit_hash = model_registry.commit(\n    message=\"Improved model with more trees\",\n    add_files=[improved_model],\n    metadata={\n        \"accuracy\": improved_model.score(X_test, y_test),\n        \"improvements\": [\"More trees\", \"Deeper trees\"]\n    },\n    tags=[\"improved\", \"v2.0\"]\n)\n</code></pre> <p>With PyTorch (manual):</p> <pre><code># Train an improved model\n# ... training code ...\n\n# Save improved model\ntorch.save(improved_model.state_dict(), \"model_weights_v2.pt\")\n\n# Commit with updated metadata\ncommit_hash = model_registry.commit(\n    message=\"Improved model with better regularization\",\n    add_files=[\"model_weights_v2.pt\"],\n    metadata={\n        \"framework\": \"pytorch\",\n        \"accuracy\": 0.92,  # Improved!\n        \"f1_score\": 0.90,\n        \"hyperparameters\": {\n            \"learning_rate\": 0.0005,\n            \"epochs\": 15,\n            \"batch_size\": 32,\n            \"weight_decay\": 0.01  # Added regularization\n        },\n        \"training_data\": \"sentiment_v2\",\n        \"improvements\": [\"Better regularization\", \"More epochs\"]\n    },\n    tags=[\"improved\", \"v2.0\", \"production\"]\n)\n</code></pre>"},{"location":"guides/model-versioning/#committing-model-objects-directly","title":"Committing Model Objects Directly","text":"<p>Kirin supports committing scikit-learn model objects directly, automatically handling serialization, hyperparameter extraction, and metrics extraction. This simplifies your workflow significantly.</p>"},{"location":"guides/model-versioning/#basic-usage","title":"Basic Usage","text":"<p>Simply pass the model object to <code>commit()</code>:</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\n\n# Train your model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Commit directly - no manual serialization needed!\ncommit_hash = dataset.commit(\n    message=\"Initial model\",\n    add_files=[model],  # Model object, not file path\n    metadata={\"accuracy\": model.score(X_test, y_test)}\n)\n</code></pre> <p>Kirin automatically:</p> <ul> <li>Serializes the model using joblib (saves as <code>model.pkl</code>)</li> <li>Extracts hyperparameters via <code>model.get_params()</code></li> <li>Extracts available metrics (feature_importances_, coef_, etc.)</li> <li>Links the source script that created the model</li> <li>Structures metadata in <code>metadata[\"models\"][\"model\"]</code></li> </ul>"},{"location":"guides/model-versioning/#model-specific-metadata","title":"Model-Specific Metadata","text":"<p>When you have multiple models with different metrics, use the <code>metadata[\"models\"]</code> structure to provide model-specific metadata:</p> <p>Important: The keys under <code>metadata[\"models\"]</code> must match the model variable names exactly. Kirin auto-detects variable names (e.g., <code>rf_model</code>, <code>lr_model</code>) and uses them as keys in the metadata structure. If your metadata keys don't match the variable names, the metadata won't be properly merged with auto-extracted metadata. Always use the same variable names in both your code and your metadata structure for consistency.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Train multiple models\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\nrf_accuracy = rf_model.score(X_test, y_test)\n\nlr_model = LogisticRegression(random_state=42)\nlr_model.fit(X_train, y_train)\nlr_accuracy = lr_model.score(X_test, y_test)\n\n# Commit with model-specific metadata\ncommit_hash = dataset.commit(\n    message=\"Compare RandomForest vs LogisticRegression\",\n    add_files=[rf_model, lr_model],\n    metadata={\n        \"models\": {\n            \"rf_model\": {\n                \"accuracy\": rf_accuracy,  # Model-specific\n                \"f1_score\": 0.93\n            },\n            \"lr_model\": {\n                \"accuracy\": lr_accuracy,  # Different accuracy\n                \"f1_score\": 0.85\n            }\n        },\n        \"dataset\": \"iris\",  # Shared metadata (top-level)\n        \"test_size\": 0.2\n    }\n)\n</code></pre> <p>Metadata Structure:</p> <p>The final metadata will be structured as:</p> <pre><code>{\n    \"models\": {\n        \"rf_model\": {\n            \"model_type\": \"RandomForestClassifier\",  # Auto-extracted\n            \"hyperparameters\": {...},  # Auto-extracted\n            \"metrics\": {...},  # Auto-extracted\n            \"sklearn_version\": \"1.3.0\",  # Auto-extracted\n            \"accuracy\": 0.95,  # User-provided\n            \"f1_score\": 0.93,  # User-provided\n            \"source_file\": \"ml-workflow.py\",  # Auto-detected\n            \"source_hash\": \"...\"\n        },\n        \"lr_model\": {\n            \"model_type\": \"LogisticRegression\",  # Auto-extracted\n            \"hyperparameters\": {...},  # Auto-extracted\n            \"metrics\": {...},  # Auto-extracted\n            \"sklearn_version\": \"1.3.0\",  # Auto-extracted\n            \"accuracy\": 0.87,  # User-provided\n            \"f1_score\": 0.85,  # User-provided\n            \"source_file\": \"ml-workflow.py\",  # Auto-detected\n            \"source_hash\": \"...\"\n        }\n    },\n    \"dataset\": \"iris\",  # Top-level (shared)\n    \"test_size\": 0.2\n}\n</code></pre>"},{"location":"guides/model-versioning/#metadata-merging","title":"Metadata Merging","text":"<p>Kirin automatically merges auto-extracted metadata with your provided metadata:</p> <ol> <li>Auto-extracted first: Hyperparameters, metrics, sklearn_version, and    source info are extracted and added to each model's entry</li> <li>User-provided merges in: Your model-specific metadata (via    <code>metadata[\"models\"][var_name]</code>) is merged, overriding auto-extracted values    on conflicts</li> <li>Top-level metadata: Metadata outside the <code>models</code> dict applies to the    entire commit (shared context)</li> </ol> <p>Example:</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\n\n# Train a model\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Commit with both auto-extracted and user-provided metadata\ncommit_hash = dataset.commit(\n    message=\"Model with custom metadata\",\n    add_files=[model],\n    metadata={\n        \"models\": {\n            \"model\": {\n                \"accuracy\": 0.95,  # User-provided metric\n                \"f1_score\": 0.93,  # User-provided metric\n                \"custom_note\": \"Trained on v2 dataset\",  # Custom field\n            }\n        },\n        \"dataset\": \"iris\",  # Top-level metadata\n    }\n)\n\n# The final metadata structure will be:\n# {\n#     \"models\": {\n#         \"model\": {\n#             \"model_type\": \"RandomForestClassifier\",  # Auto-extracted\n#             \"hyperparameters\": {  # Auto-extracted\n#                 \"n_estimators\": 100,\n#                 \"max_depth\": 5,\n#                 \"random_state\": 42,\n#                 # ... all other hyperparameters\n#             },\n#             \"metrics\": {  # Auto-extracted\n#                 \"feature_importances\": [...],\n#                 \"n_features_in\": 4,\n#                 # ... other available metrics\n#             },\n#             \"sklearn_version\": \"1.3.0\",  # Auto-extracted\n#             \"accuracy\": 0.95,  # User-provided (merged in)\n#             \"f1_score\": 0.93,  # User-provided (merged in)\n#             \"custom_note\": \"Trained on v2 dataset\",  # User-provided (merged in)\n#             \"source_file\": \"ml-workflow.py\",  # Auto-extracted\n#             \"source_hash\": \"...\"  # Auto-extracted\n#         }\n#     },\n#     \"dataset\": \"iris\"  # Top-level metadata\n# }\n</code></pre>"},{"location":"guides/model-versioning/#mixed-files-and-models","title":"Mixed Files and Models","text":"<p>You can mix model objects with regular file paths:</p> <pre><code>dataset.commit(\n    message=\"Model with plots and config\",\n    add_files=[\n        model,  # Model object (auto-serialized)\n        \"plot1.svg\",  # Regular file path\n        \"config.json\",  # Regular file path\n    ],\n    metadata={\"accuracy\": 0.95}\n)\n</code></pre>"},{"location":"guides/model-versioning/#when-to-use-model-objects-vs-file-paths","title":"When to Use Model Objects vs File Paths","text":"<p>Use model objects when:</p> <ul> <li>Working with scikit-learn models</li> <li>You want automatic hyperparameter/metrics extraction</li> <li>You want source file linking</li> <li>You want simplified workflow</li> </ul> <p>Use file paths when:</p> <ul> <li>Working with PyTorch, TensorFlow, or other frameworks (not yet supported)</li> <li>Models are already serialized</li> <li>You need explicit control over serialization format</li> <li>You're migrating existing workflows</li> </ul>"},{"location":"guides/model-versioning/#querying-and-discovery","title":"Querying and Discovery","text":""},{"location":"guides/model-versioning/#find-models-by-tags","title":"Find Models by Tags","text":"<pre><code># Find all production models\nproduction_models = model_registry.find_commits(tags=[\"production\"])\n\n# Find models with multiple tags\nv2_models = model_registry.find_commits(tags=[\"v2.0\", \"production\"])\n</code></pre>"},{"location":"guides/model-versioning/#find-models-by-performance-metrics","title":"Find Models by Performance Metrics","text":"<p>When models are committed as objects, their metadata is nested under <code>metadata[\"models\"][var_name]</code>. Here's how to query them:</p> <pre><code># Find high-accuracy models (checking nested model metadata)\ndef has_high_accuracy(metadata):\n    \"\"\"Check if any model in commit has high accuracy.\"\"\"\n    if \"models\" in metadata:\n        for model_name, model_meta in metadata[\"models\"].items():\n            if model_meta.get(\"accuracy\", 0) &gt; 0.9:\n                return True\n    # Also check top-level accuracy (for backward compatibility)\n    return metadata.get(\"accuracy\", 0) &gt; 0.9\n\nhigh_accuracy = model_registry.find_commits(\n    metadata_filter=has_high_accuracy\n)\n\n# Find models by specific model type\ndef is_sklearn_model(metadata):\n    \"\"\"Check if commit contains scikit-learn models.\"\"\"\n    if \"models\" in metadata:\n        for model_name, model_meta in metadata[\"models\"].items():\n            if model_meta.get(\"model_type\", \"\").startswith(\"RandomForest\"):\n                return True\n    return False\n\nrf_models = model_registry.find_commits(metadata_filter=is_sklearn_model)\n\n# Complex queries: find best models with multiple criteria\ndef is_best_model(metadata):\n    \"\"\"Find models with high accuracy and f1_score.\"\"\"\n    if \"models\" in metadata:\n        for model_name, model_meta in metadata[\"models\"].items():\n            accuracy = model_meta.get(\"accuracy\", 0)\n            f1_score = model_meta.get(\"f1_score\", 0)\n            if accuracy &gt; 0.9 and f1_score &gt; 0.85:\n                return True\n    return False\n\nbest_models = model_registry.find_commits(\n    tags=[\"production\"],\n    metadata_filter=is_best_model\n)\n\n# For top-level metadata (backward compatibility or non-model commits)\nhigh_accuracy_simple = model_registry.find_commits(\n    metadata_filter=lambda m: m.get(\"accuracy\", 0) &gt; 0.9\n)\n</code></pre>"},{"location":"guides/model-versioning/#compare-model-versions","title":"Compare Model Versions","text":"<pre><code># Compare two model versions\ncomparison = model_registry.compare_commits(\n    \"abc123def\",  # First model hash\n    \"xyz789ghi\"   # Second model hash\n)\n\nprint(\"Metadata changes:\")\nprint(comparison[\"metadata_diff\"][\"changed\"])\nprint(\"Tag changes:\")\nprint(comparison[\"tags_diff\"])\n</code></pre>"},{"location":"guides/model-versioning/#loading-and-using-models","title":"Loading and Using Models","text":""},{"location":"guides/model-versioning/#checkout-specific-model-version","title":"Checkout Specific Model Version","text":"<pre><code># Checkout a specific model version\nmodel_registry.checkout(\"abc123def\")\n\n# Access files from that version\nwith model_registry.local_files() as files:\n    # Files are lazily downloaded when accessed\n    model_path = files[\"model_weights.pt\"]\n    config_path = files[\"config.json\"]\n\n    # Load your model\n    model = torch.load(model_path)\n</code></pre>"},{"location":"guides/model-versioning/#get-model-information","title":"Get Model Information","text":"<pre><code># Get current model info\ncurrent_commit = model_registry.current_commit\nprint(f\"Model: {current_commit.message}\")\nprint(f\"Tags: {current_commit.tags}\")\n\n# Access model metadata (nested structure)\nif \"models\" in current_commit.metadata:\n    for model_name, model_meta in current_commit.metadata[\"models\"].items():\n        print(f\"\\nModel: {model_name}\")\n        print(f\"  Type: {model_meta.get('model_type')}\")\n        print(f\"  scikit-learn version: {model_meta.get('sklearn_version', 'N/A')}\")\n        print(f\"  Accuracy: {model_meta.get('accuracy', 'N/A')}\")\n        print(f\"  Hyperparameters: {model_meta.get('hyperparameters', {})}\")\n        print(f\"  Metrics: {model_meta.get('metrics', {})}\")\n        print(f\"  Source: {model_meta.get('source_file', 'N/A')}\")\n\n# For top-level metadata (backward compatibility)\nif \"accuracy\" in current_commit.metadata:\n    print(f\"Accuracy: {current_commit.metadata['accuracy']}\")\n\n# List all files in current version\nfor filename in model_registry.list_files():\n    file_obj = model_registry.get_file(filename)\n    print(f\"{filename}: {file_obj.size} bytes\")\n</code></pre>"},{"location":"guides/model-versioning/#metadata-schema-conventions","title":"Metadata Schema Conventions","text":"<p>While Kirin doesn't enforce a specific schema, here are recommended conventions:</p>"},{"location":"guides/model-versioning/#core-model-information","title":"Core Model Information","text":"<p>For model objects (recommended):</p> <p>When committing model objects, metadata is automatically structured under <code>metadata[\"models\"][var_name]</code>:</p> <pre><code># When committing model objects, structure is automatic\nmetadata = {\n    \"models\": {\n        \"model\": {  # Key matches variable name\n            # Auto-extracted (for scikit-learn)\n            \"model_type\": \"RandomForestClassifier\",\n            \"hyperparameters\": {\n                \"n_estimators\": 100,\n                \"max_depth\": 5,\n                # ... all hyperparameters auto-extracted\n            },\n            \"metrics\": {\n                \"feature_importances\": [...],\n                \"n_features_in\": 4,\n                # ... available metrics auto-extracted\n            },\n            \"sklearn_version\": \"1.3.0\",  # Auto-extracted\n            \"source_file\": \"ml-workflow.py\",  # Auto-detected\n            \"source_hash\": \"...\",  # Auto-detected\n\n            # User-provided\n            \"accuracy\": 0.92,\n            \"f1_score\": 0.90,\n            \"precision\": 0.91,\n            \"recall\": 0.89,\n            \"training_data\": \"dataset_v2\",\n            \"training_time_seconds\": 1200,\n        }\n    },\n    # Top-level metadata (shared across all models in commit)\n    \"dataset\": \"iris\",\n    \"test_size\": 0.2,\n}\n</code></pre> <p>For manual serialization (backward compatibility):</p> <p>For frameworks not yet supported or when manually serializing:</p> <pre><code>metadata = {\n    # Required\n    \"framework\": \"pytorch\",  # or \"tensorflow\", \"sklearn\", etc.\n    \"version\": \"2.1.0\",      # Semantic versioning\n\n    # Performance metrics\n    \"accuracy\": 0.92,\n    \"f1_score\": 0.90,\n    \"precision\": 0.91,\n    \"recall\": 0.89,\n\n    # Model configuration\n    \"hyperparameters\": {\n        \"learning_rate\": 0.001,\n        \"epochs\": 10,\n        \"batch_size\": 32,\n        \"optimizer\": \"adam\"\n    },\n\n    # Training information\n    \"training_data\": \"dataset_v2\",\n    \"training_time_seconds\": 1200,\n    \"model_size_mb\": 0.5,\n\n    # Optional: domain-specific info\n    \"domain\": \"medical\",\n    \"use_cases\": [\"patient_feedback\", \"clinical_notes\"]\n}\n</code></pre>"},{"location":"guides/model-versioning/#tag-conventions","title":"Tag Conventions","text":"<pre><code># Staging tags\ntags = [\"dev\", \"staging\", \"production\"]\n\n# Version tags\ntags = [\"v1.0\", \"v2.0\", \"v2.1\"]\n\n# Domain tags\ntags = [\"medical\", \"financial\", \"general\"]\n\n# Status tags\ntags = [\"baseline\", \"improved\", \"experimental\"]\n</code></pre>"},{"location":"guides/model-versioning/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guides/model-versioning/#model-staging-pipeline","title":"Model Staging Pipeline","text":"<pre><code># Development model\ndev_commit = model_registry.commit(\n    message=\"Experimental model with new architecture\",\n    add_files=[\"model.pt\"],\n    metadata={\"accuracy\": 0.88, \"framework\": \"pytorch\"},\n    tags=[\"dev\", \"experimental\"]\n)\n\n# Promote to staging\nstaging_commit = model_registry.commit(\n    message=\"Promote experimental model to staging\",\n    add_files=[\"model.pt\"],  # Same model, different tags\n    metadata={\"accuracy\": 0.88, \"framework\": \"pytorch\"},\n    tags=[\"staging\", \"v2.1-beta\"]\n)\n\n# Promote to production\nprod_commit = model_registry.commit(\n    message=\"Release v2.1 to production\",\n    add_files=[\"model.pt\"],\n    metadata={\"accuracy\": 0.88, \"framework\": \"pytorch\"},\n    tags=[\"production\", \"v2.1\"]\n)\n</code></pre>"},{"location":"guides/model-versioning/#ab-testing-models","title":"A/B Testing Models","text":"<pre><code># Model A\nmodel_a = model_registry.commit(\n    message=\"Model A - Original architecture\",\n    add_files=[\"model_a.pt\"],\n    metadata={\"accuracy\": 0.89, \"architecture\": \"original\"},\n    tags=[\"ab-test\", \"model-a\"]\n)\n\n# Model B\nmodel_b = model_registry.commit(\n    message=\"Model B - Improved architecture\",\n    add_files=[\"model_b.pt\"],\n    metadata={\"accuracy\": 0.92, \"architecture\": \"improved\"},\n    tags=[\"ab-test\", \"model-b\"]\n)\n\n# Find A/B test models\nab_models = model_registry.find_commits(tags=[\"ab-test\"])\n</code></pre>"},{"location":"guides/model-versioning/#domain-specific-models","title":"Domain-Specific Models","text":"<pre><code># General model\ngeneral_model = model_registry.commit(\n    message=\"General sentiment classifier\",\n    add_files=[\"general_model.pt\"],\n    metadata={\n        \"accuracy\": 0.90,\n        \"domain\": \"general\",\n        \"training_data\": \"general_reviews\"\n    },\n    tags=[\"general\", \"v1.0\"]\n)\n\n# Medical domain model\nmedical_model = model_registry.commit(\n    message=\"Medical sentiment classifier\",\n    add_files=[\"medical_model.pt\"],\n    metadata={\n        \"accuracy\": 0.85,  # Lower on general data\n        \"domain_accuracy\": 0.94,  # Higher on medical data\n        \"domain\": \"medical\",\n        \"training_data\": \"medical_reviews\"\n    },\n    tags=[\"medical\", \"domain-specific\", \"v1.1\"]\n)\n</code></pre>"},{"location":"guides/model-versioning/#best-practices","title":"Best Practices","text":""},{"location":"guides/model-versioning/#1-consistent-metadata-schema","title":"1. Consistent Metadata Schema","text":"<p>Define a standard metadata schema for your team:</p> <pre><code>def create_model_metadata(accuracy, f1_score, hyperparams, training_info):\n    \"\"\"Create standardized metadata for model commits.\"\"\"\n    return {\n        \"framework\": \"pytorch\",\n        \"accuracy\": accuracy,\n        \"f1_score\": f1_score,\n        \"hyperparameters\": hyperparams,\n        \"training_data\": training_info[\"dataset\"],\n        \"training_time_seconds\": training_info[\"duration\"],\n        \"model_size_mb\": training_info[\"size_mb\"],\n        \"timestamp\": datetime.now().isoformat()\n    }\n</code></pre>"},{"location":"guides/model-versioning/#2-meaningful-commit-messages","title":"2. Meaningful Commit Messages","text":"<pre><code># Good commit messages\n\"Initial baseline model - BERT fine-tuned on customer reviews\"\n\"Improved model v2.0 - Added regularization and more training data\"\n\"Hotfix v2.0.1 - Fixed tokenization bug in production model\"\n\"Domain adaptation - Medical sentiment classifier\"\n\n# Avoid vague messages\n\"Updated model\"\n\"New version\"\n\"Changes\"\n</code></pre>"},{"location":"guides/model-versioning/#3-tag-management","title":"3. Tag Management","text":"<p>Use consistent tagging strategies:</p> <pre><code># Semantic versioning\ntags = [\"v1.0.0\", \"v1.1.0\", \"v2.0.0\"]\n\n# Staging pipeline\ntags = [\"dev\", \"staging\", \"production\"]\n\n# Feature flags\ntags = [\"feature-xyz\", \"experimental\", \"deprecated\"]\n</code></pre>"},{"location":"guides/model-versioning/#integration-with-ml-workflows","title":"Integration with ML Workflows","text":""},{"location":"guides/model-versioning/#with-experiment-tracking","title":"With Experiment Tracking","text":"<pre><code># Log to both Kirin and your experiment tracker\nimport wandb\n\n# Start experiment\nwandb.init(project=\"sentiment-classifier\")\n\n# Train model\nmodel, metrics = train_model()\n\n# Log to experiment tracker\nwandb.log(metrics)\n\n# Commit to Kirin\ncommit_hash = model_registry.commit(\n    message=f\"Experiment {wandb.run.name}\",\n    add_files=[\"model.pt\"],\n    metadata={\n        **metrics,\n        \"experiment_id\": wandb.run.id,\n        \"run_name\": wandb.run.name\n    },\n    tags=[\"experiment\", wandb.run.name]\n)\n</code></pre>"},{"location":"guides/model-versioning/#with-model-serving","title":"With Model Serving","text":"<pre><code># Deploy model from Kirin\ndef deploy_model(commit_hash):\n    model_registry.checkout(commit_hash)\n\n    with model_registry.local_files() as files:\n        model = torch.load(files[\"model.pt\"])\n        config = json.load(open(files[\"config.json\"]))\n\n    # Deploy to your serving infrastructure\n    deploy_to_production(model, config)\n    print(f\"Deployed model {commit_hash[:8]}\")\n</code></pre>"},{"location":"guides/model-versioning/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/model-versioning/#common-issues","title":"Common Issues","text":""},{"location":"guides/model-versioning/#my-metadata-isnt-being-saved","title":"My metadata isn't being saved","text":"<p>Make sure you're passing the <code>metadata</code> parameter to <code>commit()</code>. Check that your metadata is JSON-serializable.</p>"},{"location":"guides/model-versioning/#cant-find-my-models-with-find_commits","title":"Can't find my models with <code>find_commits()</code>","text":"<p>Verify your filter function returns a boolean. Use <code>lambda m: m.get(\"key\", default) &gt; value</code> for safe access.</p>"},{"location":"guides/model-versioning/#files-arent-downloading-with-local_files","title":"Files aren't downloading with <code>local_files()</code>","text":"<p>Files are lazily loaded. Access them through the dictionary: <code>local_files[\"filename\"]</code> to trigger download.</p>"},{"location":"guides/model-versioning/#how-do-i-migrate-from-other-model-versioning-tools","title":"How do I migrate from other model versioning tools?","text":"<p>Kirin can work alongside other tools. You can import models from MLflow, DVC, etc., by saving their artifacts and committing them to Kirin.</p>"},{"location":"guides/model-versioning/#performance-tips","title":"Performance Tips","text":"<ul> <li>Use <code>limit</code> parameter in <code>find_commits()</code> for large datasets</li> <li>Store large models in cloud storage (S3, GCS) for better performance</li> <li>Use <code>local_files()</code> context manager to ensure cleanup of temporary files</li> <li>Consider using tags for frequently queried model categories</li> </ul>"},{"location":"guides/model-versioning/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the API Reference for detailed method documentation</li> <li>Check out the Model Versioning   Demo for a complete example</li> <li>Learn about Cloud Storage Integration for production deployments</li> </ul>"},{"location":"guides/working-with-files/","title":"Working with Files","text":"<p>Advanced file operations and integration patterns for data science workflows.</p> <p>Notebook Tip: When working in Jupyter or Marimo notebooks, displaying a dataset (e.g., <code>dataset</code>) shows an interactive HTML view. Click \"Copy Code to Access\" on any file to get code snippets that automatically use your variable name!</p>"},{"location":"guides/working-with-files/#file-access-patterns","title":"File Access Patterns","text":""},{"location":"guides/working-with-files/#local-files-context-manager-recommended","title":"Local Files Context Manager (Recommended)","text":"<p>The <code>local_files()</code> context manager is the recommended way to work with files:</p> <pre><code>from pathlib import Path\nimport pandas as pd\nimport polars as pl\n\n# Work with files locally\nwith dataset.local_files() as local_files:\n    for filename, local_path in local_files.items():\n        print(f\"{filename} -&gt; {local_path}\")\n\n        # Process files with standard Python libraries\n        if filename.endswith('.csv'):\n            # Use pandas\n            df = pd.read_csv(local_path)\n            print(f\"CSV shape: {df.shape}\")\n\n            # Or use polars\n            df_polars = pl.read_csv(local_path)\n            print(f\"Polars shape: {df_polars.shape}\")\n\n        elif filename.endswith('.parquet'):\n            # Read parquet files\n            df = pd.read_parquet(local_path)\n            print(f\"Parquet shape: {df.shape}\")\n\n        elif filename.endswith('.json'):\n            import json\n            data = json.loads(Path(local_path).read_text())\n            print(f\"JSON keys: {list(data.keys())}\")\n</code></pre> <p>Benefits:</p> <ul> <li>Library compatibility: Works with pandas, polars, numpy, etc.</li> <li>Automatic cleanup: Files cleaned up when done</li> <li>Standard paths: Use normal Python file operations</li> <li>Memory efficient: No need to load entire files into memory</li> </ul>"},{"location":"guides/working-with-files/#working-with-data-science-libraries","title":"Working with Data Science Libraries","text":""},{"location":"guides/working-with-files/#pandas-examples","title":"Pandas Examples","text":"<pre><code>import pandas as pd\n\nwith dataset.local_files() as local_files:\n    # Read CSV files\n    if \"data.csv\" in local_files:\n        df = pd.read_csv(local_files[\"data.csv\"])\n        print(f\"DataFrame shape: {df.shape}\")\n\n        # Process data\n        df_processed = df.dropna().reset_index(drop=True)\n\n        # Save processed data\n        df_processed.to_csv(\"processed_data.csv\")\n\n        # Commit processed data\n        dataset.commit(\n            message=\"Add processed data\",\n            add_files=[\"processed_data.csv\"]\n        )\n</code></pre>"},{"location":"guides/working-with-files/#polars-examples","title":"Polars Examples","text":"<pre><code>import polars as pl\n\nwith dataset.local_files() as local_files:\n    # Read CSV files with Polars\n    if \"data.csv\" in local_files:\n        df = pl.read_csv(local_files[\"data.csv\"])\n        print(f\"Polars DataFrame shape: {df.shape}\")\n\n        # Process data with Polars\n        df_processed = df.filter(pl.col(\"value\").is_not_null())\n\n        # Save processed data\n        df_processed.write_csv(\"processed_data.csv\")\n\n        # Commit processed data\n        dataset.commit(\n            message=\"Add processed data\",\n            add_files=[\"processed_data.csv\"]\n        )\n</code></pre>"},{"location":"guides/working-with-files/#numpy-examples","title":"NumPy Examples","text":"<pre><code>import numpy as np\n\nwith dataset.local_files() as local_files:\n    # Read CSV files as NumPy arrays\n    if \"data.csv\" in local_files:\n        df = pd.read_csv(local_files[\"data.csv\"])\n        array = df.values\n        print(f\"NumPy array shape: {array.shape}\")\n\n        # Process with NumPy\n        processed_array = np.nan_to_num(array)\n\n        # Save processed array\n        np.savetxt(\"processed_data.csv\", processed_array, delimiter=\",\")\n\n        # Commit processed data\n        dataset.commit(\n            message=\"Add processed array\",\n            add_files=[\"processed_data.csv\"]\n        )\n</code></pre>"},{"location":"guides/working-with-files/#scikit-learn-examples","title":"Scikit-learn Examples","text":"<pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport joblib\n\nwith dataset.local_files() as local_files:\n    # Load training data\n    if \"train.csv\" in local_files:\n        df = pd.read_csv(local_files[\"train.csv\"])\n        X = df.drop(\"target\", axis=1)\n        y = df[\"target\"]\n\n        # Split data\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n        # Train model\n        model = RandomForestClassifier(n_estimators=100)\n        model.fit(X_train, y_train)\n\n        # Save model\n        joblib.dump(model, \"model.pkl\")\n\n        # Save test data\n        X_test.to_csv(\"X_test.csv\", index=False)\n        y_test.to_csv(\"y_test.csv\", index=False)\n\n        # Commit model and test data\n        dataset.commit(\n            message=\"Add trained model and test data\",\n            add_files=[\"model.pkl\", \"X_test.csv\", \"y_test.csv\"]\n        )\n</code></pre>"},{"location":"guides/working-with-files/#best-practices","title":"Best Practices","text":""},{"location":"guides/working-with-files/#file-naming","title":"File Naming","text":"<p>Use descriptive, consistent file names:</p> <pre><code># Good naming\ndataset.commit(\"Add Q1 sales data\", add_files=[\"sales_q1_2024.csv\"])\ndataset.commit(\"Add processed data\", add_files=[\"sales_q1_2024_processed.csv\"])\ndataset.commit(\"Add model results\", add_files=[\"model_results_q1_2024.json\"])\n\n# Avoid vague names\ndataset.commit(\"Add data\", add_files=[\"data.csv\"])\ndataset.commit(\"Update\", add_files=[\"file1.csv\", \"file2.csv\"])\n</code></pre>"},{"location":"guides/working-with-files/#file-organization","title":"File Organization","text":"<p>Organize files in logical directories:</p> <pre><code># Good organization\ndataset.commit(\"Add raw data\", add_files=[\"raw/sales.csv\", \"raw/customers.csv\"])\ndataset.commit(\"Add processed data\", add_files=[\"processed/sales_clean.csv\"])\ndataset.commit(\"Add analysis\", add_files=[\"analysis/results.csv\", \"analysis/plots.png\"])\n\n# Avoid flat structure\ndataset.commit(\"Add files\", add_files=[\"sales.csv\", \"customers.csv\",\n                                        \"results.csv\", \"plots.png\"])\n</code></pre>"},{"location":"guides/working-with-files/#next-steps","title":"Next Steps","text":"<ul> <li>Commit Management - Understanding commit   history</li> <li>Cloud Storage - Working with cloud backends</li> <li>Basic Usage - Core dataset operations</li> </ul>"},{"location":"how-to/manage-multiple-datasets/","title":"Manage Multiple Datasets","text":"<p>To run this notebook, click on the molab shield above or run the following command at the terminal:</p> <pre><code>uvx marimo edit --sandbox --mcp --no-token --watch https://github.com/nll-ai/kirin/blob/main/docs/how-to/manage-multiple-datasets.py\n</code></pre> <pre><code>import marimo as mo\n</code></pre>"},{"location":"how-to/manage-multiple-datasets/#manage-multiple-datasets","title":"Manage Multiple Datasets","text":"<p>This guide shows you how to organize and manage multiple datasets using Kirin's Catalog feature. You'll learn to create catalogs, organize datasets, and perform cross-dataset operations.</p> <pre><code>import tempfile\nfrom pathlib import Path\n\nimport polars as pl\n\nfrom kirin import Catalog\n\ntemp_dir = Path(tempfile.mkdtemp())\ncatalog = Catalog(root_dir=temp_dir)\n\nprint(f\"\u2705 Catalog created at: {temp_dir}\")\n</code></pre>"},{"location":"how-to/manage-multiple-datasets/#create-datasets","title":"Create Datasets","text":"<p>Create datasets within your catalog. Each dataset can have its own purpose, but they all share the same content-addressed storage for automatic deduplication.</p> <p>Note: Datasets don't appear in <code>catalog.datasets()</code> until after the first commit. This is because directories aren't created until they contain objects (for S3/GCS/Azure compatibility).</p> <pre><code>sales_ds = catalog.create_dataset(\n    \"sales_data\",\n    \"Quarterly sales data with product information and revenue tracking\",\n)\n\ncustomer_ds = catalog.create_dataset(\n    \"customer_data\",\n    \"Customer profiles, demographics, and purchase history\",\n)\n\nanalytics_ds = catalog.create_dataset(\n    \"analytics\",\n    \"Data analysis scripts, models, and derived insights\",\n)\n\nprint(\"\u2705 Created 3 dataset instances\")\nprint(\"   Note: They won't appear in catalog.datasets() until first commit\")\n</code></pre>"},{"location":"how-to/manage-multiple-datasets/#prepare-sample-data","title":"Prepare Sample Data","text":"<p>Create sample data files for demonstration purposes.</p> <pre><code>sales_data_dir = temp_dir / \"sales_data\"\nsales_data_dir.mkdir(exist_ok=True)\n\nq1_sales = sales_data_dir / \"q1_sales.csv\"\nq1_sales.write_text(\"\"\"product,price,quantity,revenue,date\nWidget A,29.99,100,2999.00,2024-01-15\nWidget B,19.99,150,2998.50,2024-01-16\nWidget C,39.99,75,2999.25,2024-01-17\nWidget A,29.99,120,3598.80,2024-01-18\nWidget B,19.99,200,3998.00,2024-01-19\"\"\")\n\nproducts = sales_data_dir / \"products.json\"\nproducts.write_text(\"\"\"{\n\"products\": [\n    {\"id\": \"A\", \"name\": \"Widget A\", \"category\": \"Electronics\", \"cost\": 15.00},\n    {\"id\": \"B\", \"name\": \"Widget B\", \"category\": \"Accessories\", \"cost\": 8.00},\n    {\"id\": \"C\", \"name\": \"Widget C\", \"category\": \"Premium\", \"cost\": 25.00}\n]\n}\"\"\")\n\ncustomer_data_dir = temp_dir / \"customer_data\"\ncustomer_data_dir.mkdir(exist_ok=True)\n\ncustomers = customer_data_dir / \"customers.csv\"\ncustomers.write_text(\"\"\"customer_id,name,email,age,segment,registration_date\nC001,Alice Johnson,alice@email.com,28,Premium,2023-06-15\nC002,Bob Smith,bob@email.com,35,Standard,2023-08-22\nC003,Carol Davis,carol@email.com,42,Premium,2023-04-10\nC004,David Wilson,david@email.com,31,Standard,2023-09-05\nC005,Eve Brown,eve@email.com,26,Premium,2023-07-18\"\"\")\n\nanalytics_data_dir = temp_dir / \"analytics\"\nanalytics_data_dir.mkdir(exist_ok=True)\n\nanalysis_results = analytics_data_dir / \"analysis_results.json\"\nanalysis_results.write_text(\"\"\"{\n\"analysis_date\": \"2024-01-20\",\n\"total_revenue\": 12595.55,\n\"top_product\": \"Widget B\",\n\"customer_segments\": {\n    \"Premium\": 3,\n    \"Standard\": 2\n}\n}\"\"\")\n\nprint(\"\u2705 Created sample data files\")\n</code></pre>"},{"location":"how-to/manage-multiple-datasets/#commit-files-to-datasets","title":"Commit Files to Datasets","text":"<p>Commit files to each dataset. After the first commit, datasets will appear in <code>catalog.datasets()</code>.</p> <pre><code>sales_commit = sales_ds.commit(\n    message=\"Initial commit: Add Q1 sales data and product catalog\",\n    add_files=[\n        str(sales_data_dir / \"q1_sales.csv\"),\n        str(sales_data_dir / \"products.json\"),\n    ],\n)\n\nprint(f\"\u2705 Committed to sales_data: {sales_commit[:8]}\")\n</code></pre> <pre><code>customer_commit = customer_ds.commit(\n    message=\"Initial commit: Add customer profiles\",\n    add_files=[\n        str(customer_data_dir / \"customers.csv\"),\n    ],\n)\n\nprint(f\"\u2705 Committed to customer_data: {customer_commit[:8]}\")\n</code></pre> <pre><code>analytics_commit = analytics_ds.commit(\n    message=\"Initial commit: Add analysis results\",\n    add_files=[\n        str(analytics_data_dir / \"analysis_results.json\"),\n    ],\n)\n\nprint(f\"\u2705 Committed to analytics: {analytics_commit[:8]}\")\n</code></pre>"},{"location":"how-to/manage-multiple-datasets/#list-all-datasets","title":"List All Datasets","text":"<p>Now that commits have been made, datasets appear in <code>catalog.datasets()</code>. Use this to discover and access all datasets in your catalog.</p> <pre><code>dataset_info = []\nfor listed_name in catalog.datasets():\n    current_ds = catalog.get_dataset(listed_name)\n    ds_info = current_ds.get_info()\n    dataset_info.append(\n        {\n            \"name\": listed_name,\n            \"description\": ds_info[\"description\"],\n            \"files\": len(current_ds.files),\n            \"commits\": ds_info[\"commit_count\"],\n        }\n    )\n\ninfo_content = f\"**Total Datasets**: {len(catalog)}\\n\\n\"\ninfo_content += \"**Dataset Details:**\\n\\n\"\nfor info_item in dataset_info:\n    info_content += f\"- **{info_item['name']}**: {info_item['description']}\\n\"\n    info_content += f\"  - Files: {info_item['files']}\\n\"\n    info_content += f\"  - Commits: {info_item['commits']}\\n\"\n\nmo.md(info_content)\n</code></pre>"},{"location":"how-to/manage-multiple-datasets/#access-dataset-files","title":"Access Dataset Files","text":"<p>Access files from datasets using the dataset instances you created, or retrieve datasets from the catalog.</p> <pre><code>print(\"\u2705 Dataset files:\")\nprint(f\"   - sales_data: {len(sales_ds.files)} files\")\nprint(f\"   - customer_data: {len(customer_ds.files)} files\")\n</code></pre>"},{"location":"how-to/manage-multiple-datasets/#perform-cross-dataset-analysis","title":"Perform Cross-Dataset Analysis","text":"<p>Access files from multiple datasets simultaneously to perform cross-dataset operations.</p> <pre><code>with (\n    sales_ds.local_files() as sales_files,\n    customer_ds.local_files() as customer_files,\n):\n    sales_df = pl.read_csv(sales_files[\"q1_sales.csv\"])\n    customers_df = pl.read_csv(customer_files[\"customers.csv\"])\n\n    sales_summary = (\n        sales_df.group_by(\"product\")\n        .agg(\n            [\n                pl.col(\"quantity\").sum().alias(\"total_quantity\"),\n                pl.col(\"revenue\").sum().alias(\"total_revenue\"),\n            ]\n        )\n        .sort(\"total_revenue\", descending=True)\n    )\n\n    print(\"\ud83d\udcca Sales Summary by Product:\")\n    print(sales_summary)\n\n    print(\"\\n\ud83d\udc65 Customer Statistics:\")\n    print(f\"   Total customers: {customers_df.height}\")\n    premium_count = customers_df.filter(pl.col(\"segment\") == \"Premium\").height\n    print(f\"   Premium customers: {premium_count}\")\n</code></pre>"},{"location":"how-to/manage-multiple-datasets/#update-datasets","title":"Update Datasets","text":"<p>Add new data to existing datasets by creating new commits. You can use <code>catalog.get_dataset()</code> to retrieve a dataset and commit to it.</p> <pre><code>q2_sales = temp_dir / \"q2_sales.csv\"\nq2_sales.write_text(\"\"\"product,price,quantity,revenue,date\nWidget A,29.99,120,3598.80,2024-04-15\nWidget B,19.99,180,3598.20,2024-04-16\nWidget C,39.99,90,3599.10,2024-04-17\nWidget A,29.99,150,4498.50,2024-04-18\nWidget B,19.99,220,4397.80,2024-04-19\"\"\")\n\nsales_ds_update = catalog.get_dataset(\"sales_data\")\nq2_commit = sales_ds_update.commit(\n    message=\"Add Q2 sales data\",\n    add_files=[str(q2_sales)],\n)\n\nprint(f\"\u2705 Added Q2 data to sales_data: {q2_commit[:8]}\")\nprint(f\"   Total commits in sales_data: {len(sales_ds_update.history())}\")\n</code></pre>"},{"location":"how-to/manage-multiple-datasets/#iterate-over-all-datasets","title":"Iterate Over All Datasets","text":"<p>Use <code>catalog.datasets()</code> to iterate over all datasets that have at least one commit and perform operations on each one.</p> <pre><code>print(\"\ud83d\udccb All Datasets Overview\")\nprint(\"=\" * 40)\n\nfor overview_name in catalog.datasets():\n    iter_ds = catalog.get_dataset(overview_name)\n    ds_files = iter_ds.list_files()\n    ds_history = iter_ds.history(limit=1)\n\n    print(f\"\\n\ud83d\udcc1 {overview_name}:\")\n    print(f\"   Files: {', '.join(ds_files) if ds_files else 'None'}\")\n    if ds_history:\n        latest_commit = ds_history[0]\n        commit_msg = f\"{latest_commit.short_hash} - {latest_commit.message}\"\n        print(f\"   Latest commit: {commit_msg}\")\n    print(f\"   Total commits: {len(iter_ds.history())}\")\n</code></pre> <pre><code>summary_content = f\"**Total Datasets**: {len(catalog)}\\n\\n\"\nsummary_content += f\"**All Datasets**: {', '.join(catalog.datasets())}\\n\\n\"\nsummary_content += \"**Key Benefits:**\\n\"\nsummary_content += \"- \u2705 Centralized management of multiple datasets\\n\"\nsummary_content += (\n    \"- \u2705 Shared content-addressed storage (automatic deduplication)\\n\"\n)\nsummary_content += \"- \u2705 Easy dataset discovery and listing\\n\"\nsummary_content += \"- \u2705 Cross-dataset operations\\n\"\nsummary_content += \"- \u2705 Cloud storage support (S3, GCS, Azure)\\n\"\n\nmo.md(summary_content)\n</code></pre>"},{"location":"how-to/setup-cloud-storage/","title":"Setup Cloud Storage for Kirin","text":"<p>This guide shows you how to configure Kirin to work with AWS S3, Google Cloud Storage, and Azure Blob Storage. Each section covers authentication setup and creating your first catalog with cloud storage.</p>"},{"location":"how-to/setup-cloud-storage/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS account with S3 access (for S3 setup)</li> <li>Google Cloud project with GCS access (for GCS setup)</li> <li>Azure account with Blob Storage access (for Azure setup)</li> <li>Basic familiarity with Kirin datasets and catalogs</li> </ul>"},{"location":"how-to/setup-cloud-storage/#aws-s3-setup","title":"AWS S3 Setup","text":"<p>Configure Kirin to use AWS S3 as your storage backend.</p>"},{"location":"how-to/setup-cloud-storage/#step-1-configure-aws-credentials","title":"Step 1: Configure AWS Credentials","text":"<p>Set up AWS credentials using one of these methods:</p>"},{"location":"how-to/setup-cloud-storage/#option-a-aws-cli-configuration-recommended","title":"Option A: AWS CLI Configuration (Recommended)","text":"<pre><code>aws configure --profile {{ aws_profile }}\n# Enter your AWS Access Key ID\n# Enter your AWS Secret Access Key\n# Enter your default region (e.g., us-east-1)\n</code></pre>"},{"location":"how-to/setup-cloud-storage/#option-b-environment-variables","title":"Option B: Environment Variables","text":"<pre><code>export AWS_ACCESS_KEY_ID=\"{{ access_key_id }}\"\nexport AWS_SECRET_ACCESS_KEY=\"{{ secret_access_key }}\"\nexport AWS_DEFAULT_REGION=\"{{ region }}\"\n</code></pre>"},{"location":"how-to/setup-cloud-storage/#option-c-iam-role-for-ec2ecslambda","title":"Option C: IAM Role (for EC2/ECS/Lambda)","text":"<p>If running on AWS infrastructure, IAM roles are automatically used.</p>"},{"location":"how-to/setup-cloud-storage/#step-2-create-s3-bucket","title":"Step 2: Create S3 Bucket","text":"<p>Create an S3 bucket for your Kirin data:</p> <pre><code>aws s3 mb s3://{{ bucket_name }} --region {{ region }}\n</code></pre> <p>Or use the AWS Console to create a bucket with appropriate permissions.</p>"},{"location":"how-to/setup-cloud-storage/#step-3-create-catalog-with-s3","title":"Step 3: Create Catalog with S3","text":"<p>Use the S3 URL as your <code>root_dir</code>:</p> <pre><code>from kirin import Catalog\n\n# Using AWS profile\ncatalog = Catalog(\n    root_dir=\"s3://{{ bucket_name }}/data\",\n    aws_profile=\"{{ aws_profile }}\"\n)\n\n# Using environment variables (no profile needed)\ncatalog = Catalog(root_dir=\"s3://{{ bucket_name }}/data\")\n\n# Create a dataset\ndataset = catalog.create_dataset(\n    name=\"{{ dataset_name }}\",\n    description=\"My cloud dataset\"\n)\n</code></pre>"},{"location":"how-to/setup-cloud-storage/#step-4-verify-s3-setup","title":"Step 4: Verify S3 Setup","text":"<p>Test that your setup works by creating a commit:</p> <pre><code>from pathlib import Path\n\n# Create a test file\ntest_file = Path(\"test.txt\")\ntest_file.write_text(\"Hello from S3!\")\n\n# Commit to dataset\ncommit_hash = dataset.commit(\n    message=\"Test commit\",\n    add_files=[str(test_file)]\n)\n\nprint(f\"\u2705 Successfully committed to S3: {commit_hash}\")\n</code></pre>"},{"location":"how-to/setup-cloud-storage/#what-just-happened-s3","title":"What just happened? (S3)","text":"<ul> <li>Kirin authenticated with AWS using your credentials</li> <li>Created dataset metadata in S3</li> <li>Stored your file using content-addressed storage in S3</li> <li>All future operations will use S3 as the backend</li> </ul>"},{"location":"how-to/setup-cloud-storage/#google-cloud-storage-setup","title":"Google Cloud Storage Setup","text":"<p>Configure Kirin to use Google Cloud Storage as your storage backend.</p>"},{"location":"how-to/setup-cloud-storage/#step-1-create-service-account","title":"Step 1: Create Service Account","text":"<p>Create a service account with Storage Object Admin permissions:</p> <pre><code># Create service account\ngcloud iam service-accounts create {{ service_account_name }} \\\n    --display-name=\"Kirin Storage Service Account\"\n\n# Grant Storage Object Admin role\ngcloud projects add-iam-policy-binding {{ project_id }} \\\n    --member=\"serviceAccount:{{ service_account_name }}@{{ project_id }}.iam.gserviceaccount.com\" \\\n    --role=\"roles/storage.objectAdmin\"\n\n# Create and download key\ngcloud iam service-accounts keys create {{ service_account_name }}-key.json \\\n    --iam-account={{ service_account_name }}@{{ project_id }}.iam.gserviceaccount.com\n</code></pre>"},{"location":"how-to/setup-cloud-storage/#step-2-create-gcs-bucket","title":"Step 2: Create GCS Bucket","text":"<p>Create a GCS bucket for your Kirin data:</p> <pre><code>gsutil mb -p {{ project_id }} -l {{ region }} gs://{{ bucket_name }}\n</code></pre> <p>Or use the Google Cloud Console to create a bucket.</p>"},{"location":"how-to/setup-cloud-storage/#step-3-create-catalog-with-gcs","title":"Step 3: Create Catalog with GCS","text":"<p>Use the GCS URL as your <code>root_dir</code> and provide the service account key:</p> <pre><code>from kirin import Catalog\n\ncatalog = Catalog(\n    root_dir=\"gs://{{ bucket_name }}/data\",\n    gcs_token=\"/path/to/{{ service_account_name }}-key.json\",\n    gcs_project=\"{{ project_id }}\"\n)\n\n# Create a dataset\ndataset = catalog.create_dataset(\n    name=\"{{ dataset_name }}\",\n    description=\"My cloud dataset\"\n)\n</code></pre>"},{"location":"how-to/setup-cloud-storage/#alternative-application-default-credentials","title":"Alternative: Application Default Credentials","text":"<p>If you're running on Google Cloud infrastructure (Compute Engine, Cloud Run, etc.), you can use Application Default Credentials:</p> <pre><code># No token needed - uses default credentials\ncatalog = Catalog(\n    root_dir=\"gs://{{ bucket_name }}/data\",\n    gcs_project=\"{{ project_id }}\"\n)\n</code></pre>"},{"location":"how-to/setup-cloud-storage/#step-4-verify-gcs-setup","title":"Step 4: Verify GCS Setup","text":"<p>Test that your setup works:</p> <pre><code>from pathlib import Path\n\n# Create a test file\ntest_file = Path(\"test.txt\")\ntest_file.write_text(\"Hello from GCS!\")\n\n# Commit to dataset\ncommit_hash = dataset.commit(\n    message=\"Test commit\",\n    add_files=[str(test_file)]\n)\n\nprint(f\"\u2705 Successfully committed to GCS: {commit_hash}\")\n</code></pre>"},{"location":"how-to/setup-cloud-storage/#what-just-happened-gcs","title":"What just happened? (GCS)","text":"<ul> <li>Kirin authenticated with GCS using your service account</li> <li>Created dataset metadata in GCS</li> <li>Stored your file using content-addressed storage in GCS</li> <li>All future operations will use GCS as the backend</li> </ul>"},{"location":"how-to/setup-cloud-storage/#azure-blob-storage-setup","title":"Azure Blob Storage Setup","text":"<p>Configure Kirin to use Azure Blob Storage as your storage backend.</p>"},{"location":"how-to/setup-cloud-storage/#step-1-create-storage-account","title":"Step 1: Create Storage Account","text":"<p>Create an Azure Storage Account and container:</p> <pre><code># Create resource group\naz group create --name {{ resource_group }} --location {{ location }}\n\n# Create storage account\naz storage account create \\\n    --name {{ storage_account_name }} \\\n    --resource-group {{ resource_group }} \\\n    --location {{ location }} \\\n    --sku Standard_LRS\n\n# Create container\naz storage container create \\\n    --name {{ container_name }} \\\n    --account-name {{ storage_account_name }} \\\n    --auth-mode login\n</code></pre>"},{"location":"how-to/setup-cloud-storage/#step-2-get-connection-string","title":"Step 2: Get Connection String","text":"<p>Retrieve the connection string for authentication:</p> <pre><code>az storage account show-connection-string \\\n    --name {{ storage_account_name }} \\\n    --resource-group {{ resource_group }} \\\n    --output tsv\n</code></pre> <p>The connection string looks like: <code>DefaultEndpointsProtocol=https;AccountName=...;AccountKey=...;EndpointSuffix=core.windows.net</code></p>"},{"location":"how-to/setup-cloud-storage/#step-3-create-catalog-with-azure","title":"Step 3: Create Catalog with Azure","text":"<p>Use the Azure URL as your <code>root_dir</code> and provide the connection string:</p> <pre><code>from kirin import Catalog\n\ncatalog = Catalog(\n    root_dir=\"az://{{ container_name }}/data\",\n    azure_connection_string=\"{{ connection_string }}\"\n)\n\n# Create a dataset\ndataset = catalog.create_dataset(\n    name=\"{{ dataset_name }}\",\n    description=\"My cloud dataset\"\n)\n</code></pre>"},{"location":"how-to/setup-cloud-storage/#alternative-environment-variable","title":"Alternative: Environment Variable","text":"<p>You can also set the connection string as an environment variable:</p> <pre><code>export AZURE_STORAGE_CONNECTION_STRING=\"{{ connection_string }}\"\n</code></pre> <p>Then create the catalog without the connection string parameter:</p> <pre><code>catalog = Catalog(root_dir=\"az://{{ container_name }}/data\")\n</code></pre>"},{"location":"how-to/setup-cloud-storage/#step-4-verify-azure-setup","title":"Step 4: Verify Azure Setup","text":"<p>Test that your setup works:</p> <pre><code>from pathlib import Path\n\n# Create a test file\ntest_file = Path(\"test.txt\")\ntest_file.write_text(\"Hello from Azure!\")\n\n# Commit to dataset\ncommit_hash = dataset.commit(\n    message=\"Test commit\",\n    add_files=[str(test_file)]\n)\n\nprint(f\"\u2705 Successfully committed to Azure: {commit_hash}\")\n</code></pre>"},{"location":"how-to/setup-cloud-storage/#what-just-happened-azure","title":"What just happened? (Azure)","text":"<ul> <li>Kirin authenticated with Azure using your connection string</li> <li>Created dataset metadata in Azure Blob Storage</li> <li>Stored your file using content-addressed storage in Azure</li> <li>All future operations will use Azure as the backend</li> </ul>"},{"location":"how-to/setup-cloud-storage/#next-steps","title":"Next Steps","text":"<p>Now that you have cloud storage configured, you can:</p> <ul> <li>Create multiple datasets in your catalog</li> <li>Commit files to version control your data</li> <li>Access files from anywhere using the same API</li> <li>Use cloud storage for production workflows</li> </ul> <p>See the Cloud Storage Overview tutorial for more details on working with remote files.</p>"},{"location":"how-to/track-model-data/","title":"Track Model Data","text":"<p>To run this notebook, click on the molab shield above or run the following command at the terminal:</p> <pre><code>uvx marimo edit --sandbox --mcp --no-token --watch https://github.com/nll-ai/kirin/blob/main/docs/how-to/track-model-data.py\n</code></pre> <pre><code>import marimo as mo\n</code></pre>"},{"location":"how-to/track-model-data/#track-model-training-data","title":"Track Model Training Data","text":"<p>This guide shows you how to version control your machine learning models and training data using Kirin. You'll learn to create a model registry, commit models with metadata, query models by performance metrics, and compare different versions.</p> <pre><code>import tempfile\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom loguru import logger\n\nfrom kirin import Dataset\n\ntemp_dir = tempfile.mkdtemp(prefix=\"kirin_model_demo_\")\nmodel_registry = Dataset(root_dir=temp_dir, name=\"sentiment_classifier\")\n\nprint(f\"\u2705 Model registry created at: {temp_dir}\")\n</code></pre>"},{"location":"how-to/track-model-data/#create-a-model-registry","title":"Create a Model Registry","text":"<p>Start by creating a Dataset to serve as your model registry. In production, use cloud storage like <code>s3://my-bucket/models</code> instead of a temporary directory.</p>"},{"location":"how-to/track-model-data/#create-a-model-for-demonstration","title":"Create a Model for Demonstration","text":"<p>We'll create a simple sentiment classifier model to demonstrate the workflow.</p> <pre><code>class SimpleSentimentClassifier(torch.nn.Module):\n    def __init__(self, vocab_size=1000, embedding_dim=128, hidden_dim=64):\n        super().__init__()\n        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.classifier = torch.nn.Linear(hidden_dim, 2)\n        self.dropout = torch.nn.Dropout(0.2)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, _ = self.lstm(embedded)\n        last_output = lstm_out[:, -1, :]\n        dropped = self.dropout(last_output)\n        return self.classifier(dropped)\n\nmodel = SimpleSentimentClassifier()\nparam_count = sum(p.numel() for p in model.parameters())\nprint(f\"\u2705 Created model with {param_count:,} parameters\")\n</code></pre>"},{"location":"how-to/track-model-data/#save-model-files","title":"Save Model Files","text":"<p>Save your model weights, configuration, and training information as separate files that will be versioned together.</p> <pre><code>model_dir = Path(temp_dir) / \"models\"\nmodel_dir.mkdir(exist_ok=True)\n\nmodel_path = model_dir / \"model_weights.pt\"\ntorch.save(model.state_dict(), model_path)\n\nconfig_path = model_dir / \"config.json\"\nconfig_path.write_text(\"\"\"{\n\"model_type\": \"SimpleSentimentClassifier\",\n\"vocab_size\": 1000,\n\"embedding_dim\": 128,\n\"hidden_dim\": 64,\n\"num_classes\": 2\n}\"\"\")\n\ntraining_info_path = model_dir / \"training_info.json\"\ntraining_info_path.write_text(\"\"\"{\n\"dataset\": \"sentiment_analysis_v1\",\n\"train_samples\": 10000,\n\"val_samples\": 2000,\n\"test_samples\": 2000,\n\"batch_size\": 32,\n\"learning_rate\": 0.001,\n\"epochs\": 10\n}\"\"\")\n\nprint(\"\u2705 Created model files:\")\nprint(f\"   - {model_path.name}\")\nprint(f\"   - {config_path.name}\")\nprint(f\"   - {training_info_path.name}\")\n</code></pre>"},{"location":"how-to/track-model-data/#commit-your-first-model","title":"Commit Your First Model","text":"<p>Commit your model with comprehensive metadata including performance metrics, hyperparameters, and tags for easy discovery.</p> <pre><code>metadata = {\n    \"framework\": \"pytorch\",\n    \"model_type\": \"SimpleSentimentClassifier\",\n    \"version\": \"1.0.0\",\n    \"accuracy\": 0.87,\n    \"f1_score\": 0.85,\n    \"precision\": 0.88,\n    \"recall\": 0.82,\n    \"hyperparameters\": {\n        \"vocab_size\": 1000,\n        \"embedding_dim\": 128,\n        \"hidden_dim\": 64,\n        \"learning_rate\": 0.001,\n        \"epochs\": 10,\n        \"batch_size\": 32,\n    },\n    \"training_data\": {\n        \"dataset\": \"sentiment_analysis_v1\",\n        \"train_samples\": 10000,\n        \"val_samples\": 2000,\n        \"test_samples\": 2000,\n    },\n    \"training_time_seconds\": 1200,\n    \"model_size_mb\": 0.5,\n}\n\ntags = [\"baseline\", \"v1.0\"]\n\ncommit_hash = model_registry.commit(\n    message=\"Initial baseline model - SimpleSentimentClassifier v1.0\",\n    add_files=[str(model_path), str(config_path), str(training_info_path)],\n    metadata=metadata,\n    tags=tags,\n)\n\nprint(\"\u2705 Committed initial model version\")\nprint(f\"   Commit: {commit_hash[:8]}\")\nprint(f\"   Tags: {tags}\")\nprint(f\"   Accuracy: {metadata['accuracy']}\")\n</code></pre>"},{"location":"how-to/track-model-data/#create-an-improved-model-version","title":"Create an Improved Model Version","text":"<p>Train or create an improved version of your model with better performance.</p> <pre><code>improved_model = SimpleSentimentClassifier()\nwith torch.no_grad():\n    for param in improved_model.parameters():\n        param.add_(torch.randn_like(param) * 0.01)\n\nimproved_model_path = Path(temp_dir) / \"models\" / \"model_weights.pt\"\ntorch.save(improved_model.state_dict(), improved_model_path)\n\nimproved_config_path = Path(temp_dir) / \"models\" / \"config.json\"\nimproved_config_path.write_text(\"\"\"{\n\"model_type\": \"SimpleSentimentClassifier\",\n\"vocab_size\": 1000,\n\"embedding_dim\": 128,\n\"hidden_dim\": 64,\n\"num_classes\": 2,\n\"improvements\": [\"better_regularization\", \"learning_rate_schedule\"]\n}\"\"\")\n\nprint(\n    \"\u2705 Created improved model files \"\n    \"(same filenames - versioning handled by commits)\"\n)\n</code></pre>"},{"location":"how-to/track-model-data/#commit-the-improved-model","title":"Commit the Improved Model","text":"<p>Commit the improved model with updated metadata reflecting the better performance and new hyperparameters.</p> <pre><code>improved_metadata = {\n    \"framework\": \"pytorch\",\n    \"model_type\": \"SimpleSentimentClassifier\",\n    \"version\": \"2.0.0\",\n    \"accuracy\": 0.92,\n    \"f1_score\": 0.90,\n    \"precision\": 0.91,\n    \"recall\": 0.89,\n    \"hyperparameters\": {\n        \"vocab_size\": 1000,\n        \"embedding_dim\": 128,\n        \"hidden_dim\": 64,\n        \"learning_rate\": 0.0005,\n        \"epochs\": 15,\n        \"batch_size\": 32,\n        \"weight_decay\": 0.01,\n    },\n    \"training_data\": {\n        \"dataset\": \"sentiment_analysis_v2\",\n        \"train_samples\": 15000,\n        \"val_samples\": 3000,\n        \"test_samples\": 3000,\n    },\n    \"training_time_seconds\": 1800,\n    \"model_size_mb\": 0.5,\n    \"improvements\": [\n        \"Better regularization\",\n        \"Learning rate scheduling\",\n        \"More training data\",\n        \"Longer training time\",\n    ],\n}\n\nimproved_tags = [\"improved\", \"v2.0\", \"production\"]\n\nimproved_commit_hash = model_registry.commit(\n    message=\"Improved model v2.0 - Better regularization and more data\",\n    add_files=[str(improved_model_path), str(improved_config_path)],\n    metadata=improved_metadata,\n    tags=improved_tags,\n)\n\nprint(\"\u2705 Committed improved model version\")\nprint(f\"   Commit: {improved_commit_hash[:8]}\")\nprint(f\"   Tags: {improved_tags}\")\nprint(f\"   Accuracy: {improved_metadata['accuracy']} (\u2191 from 0.87)\")\n</code></pre>"},{"location":"how-to/track-model-data/#discover-models-by-tags-and-metadata","title":"Discover Models by Tags and Metadata","text":"<p>Use <code>find_commits()</code> to discover models by tags, metadata filters, or custom filter functions.</p> <pre><code>print(\"\ud83d\udd0d Model Discovery Examples\")\nprint(\"=\" * 50)\n\nproduction_models = model_registry.find_commits(tags=[\"production\"])\nprint(f\"\\n\ud83d\udce6 Production models: {len(production_models)}\")\nfor prod_commit in production_models:\n    print(f\"   {prod_commit.short_hash}: {prod_commit.message}\")\n    print(f\"      Accuracy: {prod_commit.metadata.get('accuracy', 'N/A')}\")\n\nhigh_accuracy_models = model_registry.find_commits(\n    metadata_filter=lambda m: m.get(\"accuracy\", 0) &gt; 0.9\n)\nprint(f\"\\n\ud83c\udfaf High accuracy models (&gt;0.9): {len(high_accuracy_models)}\")\nfor acc_commit in high_accuracy_models:\n    print(f\"   {acc_commit.short_hash}: {acc_commit.message}\")\n    print(f\"      Accuracy: {acc_commit.metadata.get('accuracy', 'N/A')}\")\n</code></pre>"},{"location":"how-to/track-model-data/#compare-model-versions","title":"Compare Model Versions","text":"<p>Use <code>compare_commits()</code> to see what changed between versions including metadata differences, tag changes, and file changes.</p> <pre><code>if len(production_models) &gt;= 2:\n    print(\"\ud83d\udd04 Model Comparison\")\n    print(\"=\" * 30)\n\n    commit1 = production_models[0]\n    commit2 = production_models[1]\n\n    comparison = model_registry.compare_commits(commit1.hash, commit2.hash)\n\n    print(\"Comparing:\")\n    print(f\"  {comparison['commit1']['hash']}: {comparison['commit1']['message']}\")\n    print(f\"  {comparison['commit2']['hash']}: {comparison['commit2']['message']}\")\n\n    print(\"\\n\ud83d\udcca Metadata Changes:\")\n    metadata_diff = comparison[\"metadata_diff\"]\n\n    if metadata_diff[\"changed\"]:\n        print(\"  \ud83d\udd04 Changed:\")\n        for diff_key, change in metadata_diff[\"changed\"].items():\n            print(f\"     {diff_key}: {change['old']} \u2192 {change['new']}\")\n\n    print(\"\\n\ud83c\udff7\ufe0f  Tag Changes:\")\n    tags_diff = comparison[\"tags_diff\"]\n    if tags_diff[\"added\"]:\n        print(f\"  \u2795 Added tags: {tags_diff['added']}\")\nelse:\n    print(\"Not enough models to compare\")\n</code></pre>"},{"location":"how-to/track-model-data/#visualize-model-performance-over-time","title":"Visualize Model Performance Over Time","text":"<p>Track how your models improve over time by plotting metrics from commit history.</p> <pre><code>print(\"\ud83d\udcc8 Model Performance Over Time\")\nprint(\"=\" * 40)\n\ncommits = model_registry.history()\nmetrics_data = []\n\nfor hist_commit in commits:\n    if hist_commit.metadata:\n        metrics_data.append(\n            {\n                \"commit\": hist_commit.short_hash,\n                \"message\": hist_commit.message[:30] + \"...\"\n                if len(hist_commit.message) &gt; 30\n                else hist_commit.message,\n                \"accuracy\": hist_commit.metadata.get(\"accuracy\", 0),\n                \"f1_score\": hist_commit.metadata.get(\"f1_score\", 0),\n                \"version\": hist_commit.metadata.get(\"version\", \"unknown\"),\n            }\n        )\n\nif metrics_data:\n    df = pd.DataFrame(metrics_data)\n\n    print(\"\\nModel Performance Summary:\")\n    print(df[[\"commit\", \"version\", \"accuracy\", \"f1_score\"]].to_string(index=False))\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    ax1.plot(range(len(df)), df[\"accuracy\"], \"o-\", linewidth=2, markersize=8)\n    ax1.set_title(\"Model Accuracy Over Time\")\n    ax1.set_xlabel(\"Commit Order\")\n    ax1.set_ylabel(\"Accuracy\")\n    ax1.grid(True, alpha=0.3)\n    ax1.set_ylim(0.8, 1.0)\n\n    ax2.plot(\n        range(len(df)),\n        df[\"f1_score\"],\n        \"s-\",\n        color=\"orange\",\n        linewidth=2,\n        markersize=8,\n    )\n    ax2.set_title(\"Model F1 Score Over Time\")\n    ax2.set_xlabel(\"Commit Order\")\n    ax2.set_ylabel(\"F1 Score\")\n    ax2.grid(True, alpha=0.3)\n    ax2.set_ylim(0.8, 1.0)\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No metrics data found\")\n    df = None\n    fig = None\n    ax1 = None\n    ax2 = None\n</code></pre>"},{"location":"how-to/track-model-data/#load-specific-model-versions","title":"Load Specific Model Versions","text":"<p>Checkout a specific commit to access files from that version. Files are lazily downloaded when accessed.</p> <pre><code>print(\"\ud83d\ude80 Loading Specific Model Versions\")\nprint(\"=\" * 40)\n\nprod_models = model_registry.find_commits(tags=[\"production\"])\n\nif prod_models:\n    latest_model = prod_models[0]\n    print(f\"Loading latest production model: {latest_model.short_hash}\")\n\n    model_registry.checkout(latest_model.hash)\n\n    print(\"\\n\ud83d\udcc1 Files in this commit:\")\n    for filename in model_registry.list_files():\n        file_obj = model_registry.get_file(filename)\n        print(f\"   {filename}: {file_obj.size} bytes\")\n\n    print(\"\\n\ud83d\udcbe Accessing files (lazy loading):\")\n    with model_registry.local_files() as local_files:\n        for filename in local_files.keys():\n            local_path = local_files[filename]\n            print(f\"   {filename} \u2192 {local_path}\")\n            print(f\"      Exists: {Path(local_path).exists()}\")\n\n    print(\"\\n\ud83d\udccb Model metadata:\")\n    for key, value in latest_model.metadata.items():\n        if isinstance(value, dict):\n            print(f\"   {key}:\")\n            for sub_key, sub_value in value.items():\n                print(f\"     {sub_key}: {sub_value}\")\n        else:\n            print(f\"   {key}: {value}\")\nelse:\n    print(\"No production models found\")\n    latest_model = None\n    local_files = None\n    local_path = None\n</code></pre>"},{"location":"how-to/track-model-data/#view-registry-statistics","title":"View Registry Statistics","text":"<p>Get an overview of your model registry including total commits, files, and tag distribution.</p> <pre><code>all_commits = model_registry.history()\n\nprint(\"\ud83c\udfaf Summary\")\nprint(\"=\" * 50)\nprint(\"\\n\ud83d\udcca Registry Statistics:\")\nprint(f\"   Total commits: {len(all_commits)}\")\nprint(f\"   Total files: {sum(len(c.files) for c in all_commits)}\")\n\ntag_counts = {}\nfor summary_commit in all_commits:\n    for tag in summary_commit.tags:\n        tag_counts[tag] = tag_counts.get(tag, 0) + 1\n\nprint(\"\\n\ud83c\udff7\ufe0f  Tag Distribution:\")\nfor tag, count in sorted(tag_counts.items()):\n    print(f\"   {tag}: {count}\")\n</code></pre>"},{"location":"how-to/track-model-data/#summary","title":"Summary","text":"<p>Your model registry now tracks:</p> <ul> <li>\u2705 Content-addressed storage (automatic deduplication)</li> <li>\u2705 Lazy loading (files only downloaded when needed)</li> <li>\u2705 Rich metadata tracking (hyperparameters, metrics, etc.)</li> <li>\u2705 Flexible tagging system (staging, versions, domains)</li> <li>\u2705 Powerful querying (by metadata, tags, or custom filters)</li> <li>\u2705 Model comparison and diffing</li> <li>\u2705 Linear history (simple, no branching complexity)</li> <li>\u2705 Cloud storage support (works with S3, GCS, Azure)</li> </ul> <p>Use Cases: - Model experiment tracking - A/B testing different model versions - Model deployment staging (dev \u2192 staging \u2192 production) - Reproducible ML workflows</p>"},{"location":"how-to/version-control-pipeline/","title":"Version Control Pipeline","text":"<p>To run this notebook, click on the molab shield above or run the following command at the terminal:</p> <pre><code>uvx marimo edit --sandbox --mcp --no-token --watch https://github.com/nll-ai/kirin/blob/main/docs/how-to/version-control-pipeline.py\n</code></pre> <pre><code>import marimo as mo\n</code></pre>"},{"location":"how-to/version-control-pipeline/#version-control-your-data-pipeline","title":"Version Control Your Data Pipeline","text":"<p>This guide shows you how to version control your ETL/data pipeline outputs using Kirin. You'll learn to track pipeline runs, commit transformed data, store pipeline metadata, and compare outputs across different runs.</p> <p>Key Benefit: With Kirin, you don't need to organize files in directory hierarchies like <code>data/v1/</code>, <code>data/v2/</code>, or <code>runs/run_001/</code>. Instead, use the same filenames across runs and let Kirin's commit system handle versioning. Metadata is stored with commits, not in directory structures.</p> <pre><code>import json\nimport tempfile\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport polars as pl\n\nfrom kirin import Dataset\n\ntemp_dir = Path(tempfile.mkdtemp(prefix=\"kirin_pipeline_demo_\"))\npipeline_registry = Dataset(root_dir=temp_dir, name=\"etl_pipeline\")\n\nprint(f\"\u2705 Pipeline registry created at: {temp_dir}\")\n</code></pre>"},{"location":"how-to/version-control-pipeline/#why-not-use-directory-hierarchies","title":"Why Not Use Directory Hierarchies?","text":"<p>Traditional approach (what you might do without Kirin):</p> <pre><code>data/\n\u251c\u2500\u2500 v1/\n\u2502   \u251c\u2500\u2500 transformed_sales.csv\n\u2502   \u251c\u2500\u2500 product_summary.csv\n\u2502   \u2514\u2500\u2500 pipeline_metadata.json\n\u251c\u2500\u2500 v2/\n\u2502   \u251c\u2500\u2500 transformed_sales.csv\n\u2502   \u251c\u2500\u2500 product_summary.csv\n\u2502   \u2514\u2500\u2500 pipeline_metadata.json\n\u2514\u2500\u2500 runs/\n    \u251c\u2500\u2500 run_001/\n    \u2502   \u2514\u2500\u2500 transformed_sales.csv\n    \u2514\u2500\u2500 run_002/\n        \u2514\u2500\u2500 transformed_sales.csv\n</code></pre> <p>Problems with this approach: - Complex directory structures to maintain - Version info scattered across directory paths - Hard to query or compare versions - Manual organization required</p> <p>Kirin's approach (what we'll demonstrate):</p> <pre><code>dataset/\n\u251c\u2500\u2500 transformed_sales.csv  (versioned by commits)\n\u251c\u2500\u2500 product_summary.csv     (versioned by commits)\n\u2514\u2500\u2500 pipeline_metadata.json  (versioned by commits)\n</code></pre> <ul> <li>Same filenames across all runs</li> <li>Version info stored in commit metadata</li> <li>Easy to query and compare</li> <li>Automatic versioning via commits</li> </ul>"},{"location":"how-to/version-control-pipeline/#set-up-pipeline-input-data","title":"Set Up Pipeline Input Data","text":"<p>Create sample input data that your pipeline will process. In production, this might come from external sources like databases or APIs.</p> <pre><code>input_data_dir = temp_dir / \"input_data\"\ninput_data_dir.mkdir(exist_ok=True)\n\nraw_data = input_data_dir / \"raw_sales.csv\"\nraw_data.write_text(\"\"\"order_id,customer_id,product,quantity,price,order_date\n1001,C001,Widget A,2,29.99,2024-01-15\n1002,C002,Widget B,1,19.99,2024-01-16\n1003,C001,Widget C,3,39.99,2024-01-17\n1004,C003,Widget A,1,29.99,2024-01-18\n1005,C002,Widget B,2,19.99,2024-01-19\"\"\")\n\nprint(\"\u2705 Created sample input data\")\n</code></pre>"},{"location":"how-to/version-control-pipeline/#run-pipeline-and-generate-outputs","title":"Run Pipeline and Generate Outputs","text":"<p>Simulate a pipeline run that processes input data and generates transformed outputs, logs, and metadata.</p> <p>Note: We use simple, consistent filenames (<code>transformed_sales.csv</code>, <code>product_summary.csv</code>, etc.) without version numbers or directory hierarchies. Versioning is handled by commits, not filenames.</p> <pre><code>def run_pipeline(run_id, input_file, output_dir):\n    \"\"\"Simulate a pipeline run that processes data.\"\"\"\n    df = pl.read_csv(input_file)\n\n    transformed_data = df.with_columns(\n        [\n            (pl.col(\"quantity\") * pl.col(\"price\")).alias(\"total_amount\"),\n            pl.col(\"order_date\").str.strptime(pl.Date, \"%Y-%m-%d\").alias(\"date\"),\n        ]\n    )\n\n    output_dir.mkdir(exist_ok=True, parents=True)\n\n    # Use simple, consistent filenames - no version numbers, run IDs,\n    # or directory hierarchies\n    transformed_path = output_dir / \"transformed_sales.csv\"\n    transformed_data.write_csv(transformed_path)\n\n    summary = transformed_data.group_by(\"product\").agg(\n        [\n            pl.col(\"total_amount\").sum().alias(\"total_revenue\"),\n            pl.col(\"quantity\").sum().alias(\"total_quantity\"),\n        ]\n    )\n\n    summary_path = output_dir / \"product_summary.csv\"\n    summary.write_csv(summary_path)\n\n    # Metadata stored separately, not in directory structure\n    pipeline_metadata = {\n        \"run_id\": run_id,\n        \"timestamp\": datetime.now().isoformat(),\n        \"input_file\": str(input_file),\n        \"records_processed\": len(transformed_data),\n        \"products_count\": len(summary),\n        \"total_revenue\": float(summary[\"total_revenue\"].sum()),\n        \"execution_time_seconds\": 2.5,\n        \"pipeline_version\": \"1.0.0\",\n    }\n\n    metadata_path = output_dir / \"pipeline_metadata.json\"\n    metadata_path.write_text(json.dumps(pipeline_metadata, indent=2))\n\n    log_path = output_dir / \"pipeline.log\"\n    log_path.write_text(\n        f\"\"\"Pipeline Run {run_id}\nStarted: {pipeline_metadata[\"timestamp\"]}\nProcessing {pipeline_metadata[\"records_processed\"]} records\nGenerated transformed data and summary\nCompleted successfully in {pipeline_metadata[\"execution_time_seconds\"]}s\n\"\"\"\n    )\n\n    return (\n        transformed_path,\n        summary_path,\n        metadata_path,\n        log_path,\n        pipeline_metadata,\n    )\n\n# Simple output directory - no run-specific subdirectories\noutput_dir = temp_dir / \"outputs\"\n(\n    transformed1,\n    summary1,\n    metadata1_path,\n    log1_path,\n    metadata1,\n) = run_pipeline(\"run_001\", input_data_dir / \"raw_sales.csv\", output_dir)\n\nprint(\"\u2705 Pipeline run 1 completed\")\nprint(f\"   Records processed: {metadata1['records_processed']}\")\nprint(f\"   Total revenue: ${metadata1['total_revenue']:.2f}\")\nprint(\n    \"   Files: transformed_sales.csv, product_summary.csv, \"\n    \"pipeline_metadata.json, pipeline.log\"\n)\n</code></pre>"},{"location":"how-to/version-control-pipeline/#commit-pipeline-run-outputs","title":"Commit Pipeline Run Outputs","text":"<p>Commit all pipeline outputs (transformed data, summaries, logs, metadata) together as a single commit. This creates a snapshot of the entire pipeline run.</p> <p>Key Point: Notice we're committing files with simple names like <code>transformed_sales.csv</code> - no version numbers in filenames. The version information (run ID, timestamp, metrics) is stored in commit metadata, not in directory structures or filenames.</p> <pre><code>commit1 = pipeline_registry.commit(\n    message=f\"Pipeline run {metadata1['run_id']} - {metadata1['pipeline_version']}\",\n    add_files=[\n        str(transformed1),\n        str(summary1),\n        str(metadata1_path),\n        str(log1_path),\n    ],\n    metadata={\n        \"pipeline_run_id\": metadata1[\"run_id\"],\n        \"pipeline_version\": metadata1[\"pipeline_version\"],\n        \"execution_time_seconds\": metadata1[\"execution_time_seconds\"],\n        \"records_processed\": metadata1[\"records_processed\"],\n        \"total_revenue\": metadata1[\"total_revenue\"],\n        \"products_count\": metadata1[\"products_count\"],\n    },\n    tags=[\"pipeline-run\", \"v1.0.0\"],\n)\n\nprint(f\"\u2705 Committed pipeline run 1: {commit1[:8]}\")\nprint(\"   Files: 4 (transformed data, summary, metadata, log)\")\n</code></pre>"},{"location":"how-to/version-control-pipeline/#run-pipeline-with-updated-input-data","title":"Run Pipeline with Updated Input Data","text":"<p>Simulate a second pipeline run with updated input data. Notice we use the same input filename - the source file has been updated with new data, but we don't need to create <code>raw_sales_v2.csv</code> or organize it in versioned directories.</p> <pre><code># Update the same input file with new data (simulating source data refresh)\nraw_data.write_text(\"\"\"order_id,customer_id,product,quantity,price,order_date\n1001,C001,Widget A,2,29.99,2024-01-15\n1002,C002,Widget B,1,19.99,2024-01-16\n1003,C001,Widget C,3,39.99,2024-01-17\n1004,C003,Widget A,1,29.99,2024-01-18\n1005,C002,Widget B,2,19.99,2024-01-19\n1006,C004,Widget A,5,29.99,2024-02-01\n1007,C005,Widget C,2,39.99,2024-02-02\n1008,C001,Widget B,3,19.99,2024-02-03\"\"\")\n\nprint(\"\u2705 Updated input data (same filename, new content - 2 additional orders)\")\n</code></pre> <pre><code># Run pipeline again with updated input data\n# Output to the same directory - no run-specific subdirectories\n(\n    transformed2,\n    summary2,\n    metadata2_path,\n    log2_path,\n    metadata2,\n) = run_pipeline(\"run_002\", raw_data, output_dir)\n\nprint(\"\u2705 Pipeline run 2 completed\")\nprint(f\"   Records processed: {metadata2['records_processed']}\")\nprint(f\"   Total revenue: ${metadata2['total_revenue']:.2f}\")\nprint(\"   Same output filenames - versioning handled by commits\")\n</code></pre>"},{"location":"how-to/version-control-pipeline/#commit-second-pipeline-run","title":"Commit Second Pipeline Run","text":"<p>Commit the second run outputs using the same filenames as the first run. Kirin's commit system handles versioning automatically.</p> <p>Traditional approach (not needed with Kirin): - <code>data/v1/transformed_sales.csv</code> - <code>data/v2/transformed_sales.csv</code> - <code>runs/run_001/transformed_sales.csv</code> - <code>runs/run_002/transformed_sales.csv</code></p> <p>Kirin approach (simpler): - <code>transformed_sales.csv</code> (versioned by commits) - Metadata stored with commits, not in directory structure</p> <pre><code>commit2 = pipeline_registry.commit(\n    message=f\"Pipeline run {metadata2['run_id']} - {metadata2['pipeline_version']}\",\n    add_files=[\n        str(transformed2),\n        str(summary2),\n        str(metadata2_path),\n        str(log2_path),\n    ],\n    metadata={\n        \"pipeline_run_id\": metadata2[\"run_id\"],\n        \"pipeline_version\": metadata2[\"pipeline_version\"],\n        \"execution_time_seconds\": metadata2[\"execution_time_seconds\"],\n        \"records_processed\": metadata2[\"records_processed\"],\n        \"total_revenue\": metadata2[\"total_revenue\"],\n        \"products_count\": metadata2[\"products_count\"],\n    },\n    tags=[\"pipeline-run\", \"v1.0.0\"],\n)\n\nprint(f\"\u2705 Committed pipeline run 2: {commit2[:8]}\")\nprint(f\"   Records: {metadata2['records_processed']} (\u2191 from 5)\")\nprint(\n    f\"   Revenue: ${metadata2['total_revenue']:.2f} \"\n    f\"(\u2191 from ${metadata1['total_revenue']:.2f})\"\n)\n</code></pre>"},{"location":"how-to/version-control-pipeline/#track-pipeline-runs-over-time","title":"Track Pipeline Runs Over Time","text":"<p>View all pipeline runs in your registry and track how metrics change across runs.</p> <pre><code>all_runs = pipeline_registry.history()\n\nprint(\"\ud83d\udcca Pipeline Run History\")\nprint(\"=\" * 50)\n\nfor run_commit in all_runs:\n    run_meta = run_commit.metadata or {}\n    print(f\"\\n\ud83d\udd39 {run_commit.short_hash}: {run_commit.message}\")\n    print(f\"   Records: {run_meta.get('records_processed', 'N/A')}\")\n    print(f\"   Revenue: ${run_meta.get('total_revenue', 0):.2f}\")\n    print(f\"   Execution time: {run_meta.get('execution_time_seconds', 'N/A')}s\")\n    print(f\"   Tags: {', '.join(run_commit.tags)}\")\n</code></pre>"},{"location":"how-to/version-control-pipeline/#compare-pipeline-runs","title":"Compare Pipeline Runs","text":"<p>Use <code>compare_commits()</code> to see what changed between pipeline runs including metadata differences and file changes.</p> <pre><code>if len(all_runs) &gt;= 2:\n    print(\"\ud83d\udd04 Pipeline Run Comparison\")\n    print(\"=\" * 40)\n\n    run1_commit = all_runs[-1]\n    run2_commit = all_runs[-2]\n\n    comparison = pipeline_registry.compare_commits(\n        run1_commit.hash, run2_commit.hash\n    )\n\n    print(\"Comparing:\")\n    print(f\"  {comparison['commit1']['hash']}: {comparison['commit1']['message']}\")\n    print(f\"  {comparison['commit2']['hash']}: {comparison['commit2']['message']}\")\n\n    print(\"\\n\ud83d\udcca Metadata Changes:\")\n    metadata_diff = comparison[\"metadata_diff\"]\n\n    if metadata_diff[\"changed\"]:\n        for diff_key, change in metadata_diff[\"changed\"].items():\n            print(f\"   {diff_key}: {change['old']} \u2192 {change['new']}\")\n\n    print(\"\\n\ud83d\udcc1 File Changes:\")\n    files_diff = comparison[\"files_diff\"]\n    if files_diff[\"added\"]:\n        print(f\"   \u2795 Added: {files_diff['added']}\")\n    if files_diff[\"removed\"]:\n        print(f\"   \u2796 Removed: {files_diff['removed']}\")\n    if files_diff[\"modified\"]:\n        print(f\"   \ud83d\udd04 Modified: {files_diff['modified']}\")\nelse:\n    print(\"Not enough runs to compare\")\n</code></pre>"},{"location":"how-to/version-control-pipeline/#query-pipeline-runs-by-metadata","title":"Query Pipeline Runs by Metadata","text":"<p>Use <code>find_commits()</code> to discover pipeline runs by metadata filters, such as finding runs with high revenue or specific pipeline versions.</p> <pre><code>print(\"\ud83d\udd0d Pipeline Run Discovery\")\nprint(\"=\" * 40)\n\nhigh_revenue_runs = pipeline_registry.find_commits(\n    metadata_filter=lambda m: m.get(\"total_revenue\", 0) &gt; 1000\n)\nprint(f\"\\n\ud83d\udcb0 High revenue runs (&gt;$1000): {len(high_revenue_runs)}\")\nfor high_rev_commit in high_revenue_runs:\n    print(\n        f\"   {high_rev_commit.short_hash}: \"\n        f\"${high_rev_commit.metadata.get('total_revenue', 0):.2f}\"\n    )\n\nv1_runs = pipeline_registry.find_commits(tags=[\"v1.0.0\"])\nprint(f\"\\n\ud83c\udff7\ufe0f  v1.0.0 runs: {len(v1_runs)}\")\nfor v1_commit in v1_runs:\n    print(f\"   {v1_commit.short_hash}: {v1_commit.message}\")\n</code></pre>"},{"location":"how-to/version-control-pipeline/#access-pipeline-outputs-from-specific-runs","title":"Access Pipeline Outputs from Specific Runs","text":"<p>Checkout a specific pipeline run commit to access its outputs. Files are lazily downloaded when accessed.</p> <pre><code>if v1_runs:\n    latest_run = v1_runs[0]\n    print(f\"\ud83d\udce6 Accessing pipeline run: {latest_run.short_hash}\")\n    print(\"=\" * 40)\n\n    pipeline_registry.checkout(latest_run.hash)\n\n    print(\"\\n\ud83d\udcc1 Files in this run:\")\n    for filename in pipeline_registry.list_files():\n        file_obj = pipeline_registry.get_file(filename)\n        print(f\"   {filename}: {file_obj.size} bytes\")\n\n    print(\"\\n\ud83d\udcbe Accessing transformed data (lazy loading):\")\n    with pipeline_registry.local_files() as local_files:\n        if \"transformed_sales.csv\" in local_files:\n            transformed_path = local_files[\"transformed_sales.csv\"]\n            print(f\"   transformed_sales.csv \u2192 {transformed_path}\")\n            print(f\"   File exists: {Path(transformed_path).exists()}\")\n\n            df = pl.read_csv(transformed_path)\n            print(\"\\n\ud83d\udcca Data preview:\")\n            print(df.head(3))\nelse:\n    print(\"No runs found\")\n    latest_run = None\n    transformed_path = None\n    df = None\n</code></pre>"},{"location":"how-to/version-control-pipeline/#track-pipeline-performance-metrics","title":"Track Pipeline Performance Metrics","text":"<p>Analyze pipeline performance over time by extracting metrics from commit history.</p> <pre><code>print(\"\ud83d\udcc8 Pipeline Performance Analysis\")\nprint(\"=\" * 40)\n\nperformance_data = []\nfor perf_commit in all_runs:\n    if perf_commit.metadata:\n        performance_data.append(\n            {\n                \"run_id\": perf_commit.metadata.get(\"pipeline_run_id\", \"unknown\"),\n                \"commit\": perf_commit.short_hash,\n                \"records\": perf_commit.metadata.get(\"records_processed\", 0),\n                \"revenue\": perf_commit.metadata.get(\"total_revenue\", 0),\n                \"execution_time\": perf_commit.metadata.get(\n                    \"execution_time_seconds\", 0\n                ),\n            }\n        )\n\nif performance_data:\n    perf_df = pl.DataFrame(performance_data)\n\n    print(\"\\nPerformance Summary:\")\n    print(perf_df)\n\n    print(\"\\n\ud83d\udcca Statistics:\")\n    print(f\"   Average records per run: {perf_df['records'].mean():.1f}\")\n    print(f\"   Total revenue across runs: ${perf_df['revenue'].sum():.2f}\")\n    print(f\"   Average execution time: {perf_df['execution_time'].mean():.2f}s\")\nelse:\n    print(\"No performance data found\")\n    perf_df = None\n</code></pre>"},{"location":"how-to/version-control-pipeline/#summary","title":"Summary","text":"<p>Your pipeline registry now tracks:</p> <ul> <li>\u2705 Complete pipeline run snapshots (data, logs, metadata)</li> <li>\u2705 Pipeline run history with linear commits</li> <li>\u2705 Metadata tracking (execution time, records processed, revenue)</li> <li>\u2705 Run comparison and diffing</li> <li>\u2705 Query runs by metadata filters</li> <li>\u2705 Lazy loading of pipeline outputs</li> <li>\u2705 Content-addressed storage (automatic deduplication)</li> <li>\u2705 Cloud storage support (works with S3, GCS, Azure)</li> </ul> <p>Key Benefits Over Traditional Approaches:</p> <ul> <li>No directory hierarchies: Use simple filenames, no need for   <code>data/v1/</code>, <code>runs/run_001/</code> structures</li> <li>Metadata with commits: Version info stored with commits, not in   directory paths</li> <li>Same filenames: Use consistent names like <code>transformed_sales.csv</code>   across all runs</li> <li>Automatic versioning: Commits handle versioning automatically</li> </ul> <p>Use Cases: - ETL pipeline versioning - Data transformation tracking - Pipeline output auditing - Reproducible data processing workflows - Pipeline performance monitoring</p>"},{"location":"reference/api/","title":"Kirin API Reference","text":""},{"location":"reference/api/#core-classes","title":"Core Classes","text":""},{"location":"reference/api/#dataset","title":"Dataset","text":"<p>The main class for working with Kirin datasets.</p> <pre><code>from kirin import Dataset\n\n# Create or load a dataset\ndataset = Dataset(root_dir=\"/path/to/data\", name=\"my-dataset\")\n</code></pre>"},{"location":"reference/api/#constructor-parameters","title":"Constructor Parameters","text":"<pre><code>Dataset(\n    root_dir: Union[str, Path],           # Root directory for the dataset\n    name: str,                           # Name of the dataset\n    description: str = \"\",               # Description of the dataset\n    fs: Optional[fsspec.AbstractFileSystem] = None,  # Filesystem to use\n    # Cloud authentication parameters\n    aws_profile: Optional[str] = None,   # AWS profile for S3 authentication\n    gcs_token: Optional[Union[str, Path]] = None,  # GCS service account token\n    gcs_project: Optional[str] = None,   # GCS project ID\n    azure_account_name: Optional[str] = None,  # Azure account name\n    azure_account_key: Optional[str] = None,    # Azure account key\n    azure_connection_string: Optional[str] = None,  # Azure connection string\n)\n</code></pre>"},{"location":"reference/api/#basic-operations","title":"Basic Operations","text":"<ul> <li><code>commit(message, add_files=None, remove_files=None, metadata=None,   tags=None)</code> - Commit changes to the dataset</li> <li><code>checkout(commit_hash=None)</code> - Switch to a specific commit (latest if None)</li> <li><code>files</code> - Dictionary of files in the current commit</li> <li><code>local_files()</code> - Context manager for accessing files as local paths</li> <li><code>history(limit=None)</code> - Get commit history</li> <li><code>get_file(filename)</code> - Get a file from the current commit</li> <li><code>read_file(filename)</code> - Read file content as text</li> <li><code>download_file(filename, target_path)</code> - Download file to local path</li> </ul>"},{"location":"reference/api/#model-versioning-operations","title":"Model Versioning Operations","text":"<ul> <li><code>find_commits(tags=None, metadata_filter=None, limit=None)</code> - Find commits   matching criteria</li> <li><code>compare_commits(hash1, hash2)</code> - Compare metadata between two commits</li> </ul>"},{"location":"reference/api/#notebook-integration","title":"Notebook Integration","text":"<p>Kirin provides rich HTML representations for datasets, commits, and catalogs that display beautifully in Jupyter and Marimo notebooks.</p>"},{"location":"reference/api/#html-representation","title":"HTML Representation","text":"<p>When you display a <code>Dataset</code>, <code>Commit</code>, or <code>Catalog</code> object in a notebook cell, Kirin automatically generates an interactive HTML view with:</p> <ul> <li>File Lists: Click on any file to preview its contents or reveal code snippets</li> <li>File Previews:</li> <li>CSV files: Displayed as interactive tables with proper formatting</li> <li>JSON files: Formatted JSON with syntax highlighting</li> <li>Text files: Displayed in code blocks with proper formatting</li> <li>Files larger than 100KB are not previewed for performance</li> <li>Copy Code to Access Button: Each file has a button that copies Python code   to your clipboard with the correct variable name</li> <li>Commit History: Visual display of commit history with metadata</li> <li>File Metadata: File sizes, content types, and icons</li> </ul> <p>Example:</p> <pre><code>from kirin import Dataset\n\ndataset = Dataset(root_dir=\"/path/to/data\", name=\"my_dataset\")\n\n# Display in notebook - shows interactive HTML\ndataset\n</code></pre>"},{"location":"reference/api/#variable-names-in-code-snippets","title":"Variable Names in Code Snippets","text":"<p>By default, code snippets use generic variable names (\"dataset\", \"commit\", or \"catalog\") based on the class type. You can customize the variable name used in code snippets by setting the <code>_repr_variable_name</code> attribute.</p> <p>Default Behavior:</p> <pre><code>dataset = Dataset(root_dir=\"/path/to/data\", name=\"my_dataset\")\n\n# Display in notebook - code snippets use \"dataset\" by default\ndataset\n</code></pre> <p>When you click \"Copy Code to Access\" on a file, the copied code will use the default variable name:</p> <pre><code># Get path to local clone of file\nwith dataset.local_files() as files:\n    file_path = files[\"data.csv\"]\n</code></pre> <p>Custom Variable Names:</p> <p>If you want code snippets to use a different variable name, set the <code>_repr_variable_name</code> attribute:</p> <pre><code>my_dataset = Dataset(root_dir=\"/path/to/data\", name=\"my_dataset\")\nmy_dataset._repr_variable_name = \"my_dataset\"\n\n# Now code snippets will use \"my_dataset\" instead of \"dataset\"\nmy_dataset  # Display in notebook\n</code></pre> <p>When you click \"Copy Code to Access\", the copied code will use your custom variable name:</p> <pre><code># Get path to local clone of file\nwith my_dataset.local_files() as files:\n    file_path = files[\"data.csv\"]\n</code></pre> <p>Note: The <code>_repr_variable_name</code> attribute is only used for HTML representation and doesn't affect the actual dataset object.</p> <p>Known Limitation (as of December 2025): The \"Copy Code to Access\" button does not work within Marimo notebooks running inside VSCode due to clipboard API restrictions. The button works correctly when viewing notebooks in a web browser.</p>"},{"location":"reference/api/#method-details","title":"Method Details","text":""},{"location":"reference/api/#commitmessage-add_filesnone-remove_filesnone-metadatanone-tagsnone","title":"<code>commit(message, add_files=None, remove_files=None, metadata=None, tags=None)</code>","text":"<p>Create a new commit with changes to the dataset.</p> <p>Enhanced for ML artifacts:</p> <ul> <li> <p>Model objects: If <code>add_files</code> contains scikit-learn model objects, they   are automatically serialized, and hyperparameters/metrics are extracted and   added to metadata.</p> </li> <li> <p>Plot objects: If <code>add_files</code> contains matplotlib or plotly figure objects,   they are automatically converted to files (SVG for vector plots, WebP for   raster plots) with format auto-detection.</p> </li> </ul> <p>Parameters:</p> <ul> <li><code>message</code> (str): Commit message describing the changes</li> <li><code>add_files</code> (List[Union[str, Path, Any]], optional): List of files (paths),   model objects, or plot objects to add. Can include:</li> <li>File paths (str or Path): Regular files</li> <li>scikit-learn model objects: Automatically serialized with hyperparameters     and metrics extracted</li> <li>matplotlib/plotly figure objects: Automatically converted to SVG/WebP with     format auto-detection</li> <li><code>remove_files</code> (List[str], optional): List of filenames to remove</li> <li><code>metadata</code> (Dict[str, Any], optional): Metadata dictionary (merged with   auto-extracted metadata). For model-specific metadata, use   <code>metadata[\"models\"][var_name]</code> structure.</li> <li><code>tags</code> (List[str], optional): List of tags for staging/versioning</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Hash of the new commit</li> </ul> <p>Examples:</p> <pre><code># Basic commit with file paths\ndataset.commit(\"Add new data\", add_files=[\"data.csv\"])\n\n# Commit with scikit-learn model object (automatic serialization)\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\ndataset.commit(\n    message=\"Initial model\",\n    add_files=[model],  # Auto-serialized as \"model.pkl\"\n    metadata={\"accuracy\": 0.95}  # Data-dependent metrics\n)\n\n# Multiple models with model-specific metadata\nrf_model = RandomForestClassifier(n_estimators=100)\nrf_model.fit(X_train, y_train)\nrf_accuracy = rf_model.score(X_test, y_test)\n\nlr_model = LogisticRegression()\nlr_model.fit(X_train, y_train)\nlr_accuracy = lr_model.score(X_test, y_test)\n\ndataset.commit(\n    message=\"Compare models\",\n    add_files=[rf_model, lr_model],\n    metadata={\n        \"models\": {\n            \"rf_model\": {\"accuracy\": rf_accuracy},  # Model-specific\n            \"lr_model\": {\"accuracy\": lr_accuracy},  # Model-specific\n        },\n        \"dataset\": \"iris\",  # Shared metadata\n    },\n)\n\n# Commit with matplotlib plot object (automatic conversion)\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.plot([1, 2, 3], [1, 4, 9])\nax.set_title(\"Training Loss\")\n\ndataset.commit(\n    message=\"Add training plot\",\n    add_files=[fig],  # Auto-converted to SVG\n)\n\n# Mixed: model objects, plot objects, and file paths\ndataset.commit(\n    message=\"Model with plots\",\n    add_files=[\n        model,  # Auto-serialized model\n        fig,  # Auto-converted plot\n        \"config.json\",  # Regular file path\n    ],\n)\n\n# Traditional model versioning (still works)\ndataset.commit(\n    message=\"Improved model v2.0\",\n    add_files=[\"model.pt\", \"config.json\"],\n    metadata={\n        \"framework\": \"pytorch\",\n        \"accuracy\": 0.92,\n        \"hyperparameters\": {\"lr\": 0.001, \"epochs\": 10}\n    },\n    tags=[\"production\", \"v2.0\"]\n)\n</code></pre> <p>Format Auto-Detection for Plots:</p> <p>When plot objects are committed, Kirin automatically detects the optimal format:</p> <ul> <li> <p>SVG (vector): Default for matplotlib and plotly figures. Best for line plots,   scatter plots, and other vector-based visualizations. Provides infinite   scalability without quality loss.</p> </li> <li> <p>WebP (raster): Used for plots with raster elements (e.g., images, heatmaps).   Provides good compression while maintaining quality.</p> </li> </ul> <p>The format is automatically chosen based on the plot type. Matplotlib and plotly figures default to SVG, which is optimal for most scientific visualizations.</p> <p>Metadata Structure:</p> <p>When model objects are committed, metadata is automatically structured as:</p> <pre><code>{\n    \"models\": {\n        \"model_name\": {\n            \"model_type\": \"RandomForestClassifier\",  # Auto-extracted\n            \"hyperparameters\": {...},  # Auto-extracted via get_params()\n            \"metrics\": {...},  # Auto-extracted (feature_importances_, etc.)\n            \"sklearn_version\": \"1.3.0\",  # Auto-extracted\n            \"accuracy\": 0.95,  # User-provided, model-specific\n            \"source_file\": \"ml-workflow.py\",  # Auto-detected\n            \"source_hash\": \"...\"  # Auto-detected\n        }\n    },\n    \"dataset\": \"iris\"  # Top-level metadata (shared)\n}\n</code></pre> <p>Metadata Merging:</p> <ul> <li>Auto-extracted metadata (hyperparameters, metrics, sklearn_version, source   info) is added to each model's entry</li> <li>User-provided model-specific metadata (via <code>metadata[\"models\"][var_name]</code>)   is merged into each model's entry</li> <li>Top-level metadata (outside <code>models</code> dict) applies to the entire commit</li> <li>User-provided metadata wins on conflicts</li> </ul>"},{"location":"reference/api/#find_commitstagsnone-metadata_filternone-limitnone","title":"<code>find_commits(tags=None, metadata_filter=None, limit=None)</code>","text":"<p>Find commits matching specified criteria.</p> <p>Parameters:</p> <ul> <li><code>tags</code> (List[str], optional): Filter by tags (commits must have ALL   specified tags)</li> <li><code>metadata_filter</code> (Callable[[Dict], bool], optional): Function that takes   metadata dict and returns bool</li> <li><code>limit</code> (int, optional): Maximum number of commits to return</li> </ul> <p>Returns:</p> <ul> <li><code>List[Commit]</code>: List of matching commits (newest first)</li> </ul> <p>Examples:</p> <pre><code># Find production models\nproduction_models = dataset.find_commits(tags=[\"production\"])\n\n# Find high-accuracy models\nhigh_accuracy = dataset.find_commits(\n    metadata_filter=lambda m: m.get(\"accuracy\", 0) &gt; 0.9\n)\n\n# Find PyTorch production models\npytorch_prod = dataset.find_commits(\n    tags=[\"production\"],\n    metadata_filter=lambda m: m.get(\"framework\") == \"pytorch\"\n)\n</code></pre>"},{"location":"reference/api/#compare_commitshash1-hash2","title":"<code>compare_commits(hash1, hash2)</code>","text":"<p>Compare metadata between two commits.</p> <p>Parameters:</p> <ul> <li><code>hash1</code> (str): First commit hash</li> <li><code>hash2</code> (str): Second commit hash</li> </ul> <p>Returns:</p> <ul> <li><code>dict</code>: Dictionary with comparison results including metadata and tag   differences</li> </ul> <p>Example:</p> <pre><code>comparison = dataset.compare_commits(\"abc123\", \"def456\")\nprint(\"Metadata changes:\", comparison[\"metadata_diff\"][\"changed\"])\nprint(\"Tag changes:\", comparison[\"tags_diff\"])\n</code></pre>"},{"location":"reference/api/#examples","title":"Examples","text":"<pre><code># Basic usage\ndataset = Dataset(root_dir=\"/data\", name=\"project\")\ndataset.commit(\"Initial commit\", add_files=[\"data.csv\"])\n\n# Cloud storage with authentication\ndataset = Dataset(\n    root_dir=\"s3://my-bucket/data\",\n    name=\"project\",\n    aws_profile=\"my-profile\"\n)\n\n# GCS with service account\ndataset = Dataset(\n    root_dir=\"gs://my-bucket/data\",\n    name=\"project\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"my-project\"\n)\n\n# Azure with connection string\ndataset = Dataset(\n    root_dir=\"az://my-container/data\",\n    name=\"project\",\n    azure_connection_string=os.getenv(\"AZURE_CONNECTION_STRING\")\n)\n</code></pre>"},{"location":"reference/api/#catalog","title":"Catalog","text":"<p>The main class for managing collections of datasets.</p> <pre><code>from kirin import Catalog\n\n# Create or load a catalog\ncatalog = Catalog(root_dir=\"/path/to/data\")\n</code></pre>"},{"location":"reference/api/#catalog-constructor-parameters","title":"Catalog Constructor Parameters","text":"<pre><code>Catalog(\n    root_dir: Union[str, fsspec.AbstractFileSystem],  # Root directory for the catalog\n    fs: Optional[fsspec.AbstractFileSystem] = None,  # Filesystem to use\n    # Cloud authentication parameters\n    aws_profile: Optional[str] = None,   # AWS profile for S3 authentication\n    gcs_token: Optional[Union[str, Path]] = None,  # GCS service account token\n    gcs_project: Optional[str] = None,   # GCS project ID\n    azure_account_name: Optional[str] = None,  # Azure account name\n    azure_account_key: Optional[str] = None,    # Azure account key\n    azure_connection_string: Optional[str] = None,  # Azure connection string\n)\n</code></pre>"},{"location":"reference/api/#catalog-basic-operations","title":"Catalog Basic Operations","text":"<ul> <li><code>datasets()</code> - List all datasets in the catalog</li> <li><code>get_dataset(name)</code> - Get a specific dataset</li> <li><code>create_dataset(name, description=\"\")</code> - Create a new dataset</li> <li><code>__len__()</code> - Number of datasets in the catalog</li> </ul>"},{"location":"reference/api/#catalog-examples","title":"Catalog Examples","text":"<pre><code># Basic usage\ncatalog = Catalog(root_dir=\"/data\")\ndatasets = catalog.datasets()\ndataset = catalog.get_dataset(\"my-dataset\")\n\n# Cloud storage with authentication\ncatalog = Catalog(\n    root_dir=\"s3://my-bucket/data\",\n    aws_profile=\"my-profile\"\n)\n\n# GCS with service account\ncatalog = Catalog(\n    root_dir=\"gs://my-bucket/data\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"my-project\"\n)\n</code></pre>"},{"location":"reference/api/#web-ui","title":"Web UI","text":"<p>The web UI provides a graphical interface for Kirin operations.</p>"},{"location":"reference/api/#routes","title":"Routes","text":"<ul> <li><code>/</code> - Home page for catalog management</li> <li><code>/catalogs/add</code> - Add new catalog</li> <li><code>/catalog/{catalog_id}</code> - View catalog and datasets</li> <li><code>/catalog/{catalog_id}/{dataset_name}</code> - View specific dataset</li> <li><code>/catalog/{catalog_id}/{dataset_name}/commit</code> - Commit interface</li> </ul>"},{"location":"reference/api/#catalog-management","title":"Catalog Management","text":"<p>The web UI supports cloud authentication through CatalogConfig:</p> <pre><code>from kirin.web.config import CatalogConfig\n\n# Create catalog config with cloud auth\nconfig = CatalogConfig(\n    id=\"my-catalog\",\n    name=\"My Catalog\",\n    root_dir=\"s3://my-bucket/data\",\n    aws_profile=\"my-profile\"\n)\n\n# Convert to runtime catalog\ncatalog = config.to_catalog()\n</code></pre>"},{"location":"reference/api/#cloud-authentication-in-web-ui","title":"Cloud Authentication in Web UI","text":"<p>The web UI automatically handles cloud authentication when you:</p> <ol> <li>Create a catalog with cloud storage URL (s3://, gs://, az://)</li> <li>The system will prompt for authentication parameters</li> <li>Credentials are stored securely in the catalog configuration</li> </ol>"},{"location":"reference/api/#storage-format","title":"Storage Format","text":"<p>Kirin uses a simplified Git-like storage format:</p> <pre><code>data/\n\u251c\u2500\u2500 data/                 # Content-addressed file storage\n\u2502   \u2514\u2500\u2500 {hash[:2]}/{hash[2:]}\n\u251c\u2500\u2500 datasets/\n\u2502   \u2514\u2500\u2500 my-dataset/\n\u2502       \u2514\u2500\u2500 commits.json  # Linear commit history\n</code></pre>"},{"location":"reference/api/#error-handling","title":"Error Handling","text":""},{"location":"reference/api/#common-exceptions","title":"Common Exceptions","text":"<ul> <li><code>ValueError</code> - Invalid operations (file not found, invalid commit hash, etc.)</li> <li><code>FileNotFoundError</code> - File not found in dataset</li> <li><code>HTTPException</code> - Web UI errors (catalog not found, validation errors)</li> </ul>"},{"location":"reference/api/#example-error-handling","title":"Example Error Handling","text":"<pre><code>try:\n    dataset.checkout(\"nonexistent-commit\")\nexcept ValueError as e:\n    print(f\"Checkout failed: {e}\")\n\ntry:\n    content = dataset.read_file(\"nonexistent.txt\")\nexcept FileNotFoundError as e:\n    print(f\"File not found: {e}\")\n</code></pre>"},{"location":"reference/api/#best-practices","title":"Best Practices","text":""},{"location":"reference/api/#dataset-naming","title":"Dataset Naming","text":"<ul> <li>Use descriptive names: <code>user-data</code>, <code>ml-experiments</code>, <code>production-models</code></li> <li>Avoid generic names: <code>test</code>, <code>data</code>, <code>temp</code></li> </ul>"},{"location":"reference/api/#workflow-patterns","title":"Workflow Patterns","text":"<ul> <li>Commit changes regularly with descriptive messages</li> <li>Use linear commit history for simplicity</li> <li>Keep datasets focused on specific use cases</li> <li>Use catalogs to organize related datasets</li> </ul>"},{"location":"reference/api/#file-management","title":"File Management","text":"<ul> <li>Use <code>local_files()</code> context manager for library compatibility</li> <li>Commit changes after adding/removing files</li> <li>Use descriptive commit messages</li> </ul>"},{"location":"reference/api/#advanced-features","title":"Advanced Features","text":""},{"location":"reference/api/#context-managers","title":"Context Managers","text":"<pre><code># Access files as local paths\nwith dataset.local_files() as local_files:\n    df = pd.read_csv(local_files[\"data.csv\"])\n    # Files automatically cleaned up\n</code></pre>"},{"location":"reference/api/#commit","title":"Commit","text":"<p>Represents an immutable snapshot of files at a point in time with optional metadata and tags.</p>"},{"location":"reference/api/#commit-notebook-integration","title":"Commit Notebook Integration","text":"<p>Commits also support rich HTML representation in notebooks. When you display a <code>Commit</code> object, you'll see:</p> <ul> <li>Commit Metadata: Hash, message, timestamp, and parent commit</li> <li>File List: All files in the commit with interactive access</li> <li>Copy Code to Access: Each file has a button that copies code including a   checkout step</li> </ul> <p>Example:</p> <pre><code>from kirin import Dataset\n\ndataset = Dataset(root_dir=\"/path/to/data\", name=\"my_dataset\")\ncommit = dataset.get_commit(commit_hash)\n\n# Display in notebook - shows interactive HTML\ncommit\n</code></pre> <p>Commit Code Snippets:</p> <p>When you click \"Copy Code to Access\" on a file in a commit, the code includes a checkout step:</p> <pre><code># Checkout this commit first\ndataset.checkout(\"commit_hash\")\n# Get path to local clone of file\nwith dataset.local_files() as files:\n    file_path = files[\"data.csv\"]\n</code></pre> <p>Note: Commits are frozen dataclasses, so you cannot set <code>_repr_variable_name</code> on them. Code snippets will use <code>\"dataset\"</code> as the default variable name.</p>"},{"location":"reference/api/#properties","title":"Properties","text":"<ul> <li><code>hash</code> (str): Unique commit identifier</li> <li><code>message</code> (str): Commit message</li> <li><code>timestamp</code> (datetime): When the commit was created</li> <li><code>parent_hash</code> (Optional[str]): Hash of the parent commit (None for initial commit)</li> <li><code>files</code> (Dict[str, File]): Dictionary of files in this commit</li> <li><code>metadata</code> (Dict[str, Any]): Metadata dictionary for model versioning</li> <li><code>tags</code> (List[str]): List of tags for staging/versioning</li> </ul>"},{"location":"reference/api/#methods","title":"Methods","text":"<ul> <li><code>get_file(name)</code> - Get a file by name</li> <li><code>list_files()</code> - List all file names</li> <li><code>has_file(name)</code> - Check if file exists</li> <li><code>get_file_count()</code> - Get number of files</li> <li><code>get_total_size()</code> - Get total size of all files</li> <li><code>to_dict()</code> - Convert to dictionary representation</li> <li><code>from_dict(data, storage)</code> - Create from dictionary</li> </ul>"},{"location":"reference/api/#commit-examples","title":"Commit Examples","text":"<pre><code># Access commit properties\ncommit = dataset.current_commit\nprint(f\"Commit: {commit.short_hash}\")\nprint(f\"Message: {commit.message}\")\nprint(f\"Files: {len(commit.files)}\")\nprint(f\"Metadata: {commit.metadata}\")\nprint(f\"Tags: {commit.tags}\")\n\n# Check if commit has specific metadata\nif commit.metadata.get(\"accuracy\", 0) &gt; 0.9:\n    print(\"High accuracy model!\")\n\n# Check if commit has specific tags\nif \"production\" in commit.tags:\n    print(\"Production model\")\n</code></pre>"},{"location":"reference/api/#commit-history","title":"Commit History","text":"<pre><code># Get commit history\nhistory = dataset.history(limit=10)\nfor commit in history:\n    print(f\"{commit.hash}: {commit.message}\")\n</code></pre>"},{"location":"reference/api/#file-operations","title":"File Operations","text":"<pre><code># Add files to commit\ndataset.commit(\"Add new data\", add_files=[\"new_data.csv\"])\n\n# Remove files from commit\ndataset.commit(\"Remove old data\", remove_files=[\"old_data.csv\"])\n\n# Combined operations\ndataset.commit(\"Update dataset\",\n              add_files=[\"new_data.csv\"],\n              remove_files=[\"old_data.csv\"])\n</code></pre>"},{"location":"reference/api/#cloud-storage-integration","title":"Cloud Storage Integration","text":"<pre><code># AWS S3 with profile\ndataset = Dataset(\n    root_dir=\"s3://my-bucket/data\",\n    name=\"my-dataset\",\n    aws_profile=\"production\"\n)\n\n# GCS with service account\ndataset = Dataset(\n    root_dir=\"gs://my-bucket/data\",\n    name=\"my-dataset\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"my-project\"\n)\n\n# Azure with connection string\ndataset = Dataset(\n    root_dir=\"az://my-container/data\",\n    name=\"my-dataset\",\n    azure_connection_string=os.getenv(\"AZURE_CONNECTION_STRING\")\n)\n</code></pre> <p>For detailed examples and cloud storage setup, see the Cloud Storage Authentication Guide.</p>"},{"location":"reference/storage-format/","title":"Storage Format","text":"<p>Technical details about Kirin's storage format and data structures.</p>"},{"location":"reference/storage-format/#overview","title":"Overview","text":"<p>Kirin uses a simplified Git-like storage format optimized for data versioning. The storage is organized into two main areas:</p> <ul> <li>Content Store: Content-addressed file storage</li> <li>Dataset Store: Commit history and metadata</li> </ul>"},{"location":"reference/storage-format/#storage-layout","title":"Storage Layout","text":"<pre><code>&lt;root&gt;/\n\u251c\u2500\u2500 data/                     # Content-addressed storage\n\u2502   \u251c\u2500\u2500 ab/                  # First two characters of hash\n\u2502   \u2502   \u2514\u2500\u2500 cdef1234...      # Rest of hash (no file extensions)\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 datasets/                 # Dataset storage\n    \u251c\u2500\u2500 dataset1/             # Dataset directory\n    \u2502   \u2514\u2500\u2500 commits.json       # Linear commit history\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"reference/storage-format/#content-store","title":"Content Store","text":""},{"location":"reference/storage-format/#file-storage","title":"File Storage","text":"<p>Files are stored in the content store using their content hash:</p> <p>Storage Path: <code>data/{hash[:2]}/{hash[2:]}</code></p> <p>Example:</p> <ul> <li>File hash: <code>abc123def456...</code></li> <li>Storage path: <code>data/ab/c123def456...</code></li> </ul>"},{"location":"reference/storage-format/#critical-design-extension-less-storage","title":"Critical Design: Extension-less Storage","text":"<p>Files are stored WITHOUT file extensions in the content store:</p> <ul> <li>Storage Path: <code>data/ab/cdef1234...</code> (no <code>.csv</code>, <code>.txt</code>, etc.)</li> <li>Original Extensions: Stored as metadata in the <code>File</code> entity's <code>name</code> attribute</li> <li>Extension Restoration: Original filenames restored when files are accessed</li> <li>Content Integrity: Files identified purely by content hash</li> <li>Deduplication: Identical content stored only once, regardless of original filename</li> </ul>"},{"location":"reference/storage-format/#benefits-of-extension-less-storage","title":"Benefits of Extension-less Storage","text":"<ol> <li>Content Integrity: Files identified by content, not filename</li> <li>Deduplication: Identical content stored once regardless of original name</li> <li>Tamper-proof: Any change to content changes the hash</li> <li>Efficient Storage: No duplicate storage for identical content</li> </ol>"},{"location":"reference/storage-format/#dataset-store","title":"Dataset Store","text":""},{"location":"reference/storage-format/#commit-history-format","title":"Commit History Format","text":"<p>Each dataset maintains a single JSON file with linear commit history:</p> <p>File: <code>datasets/{dataset_name}/commits.json</code></p> <pre><code>{\n  \"dataset_name\": \"my_dataset\",\n  \"commits\": [\n    {\n      \"hash\": \"abc123...\",\n      \"message\": \"Initial commit\",\n      \"timestamp\": \"2024-01-01T12:00:00\",\n      \"parent_hash\": null,\n      \"files\": {\n        \"data.csv\": {\n          \"hash\": \"def456...\",\n          \"name\": \"data.csv\",\n          \"size\": 1024,\n          \"content_type\": \"text/csv\"\n        }\n      }\n    },\n    {\n      \"hash\": \"ghi789...\",\n      \"message\": \"Add processed data\",\n      \"timestamp\": \"2024-01-01T13:00:00\",\n      \"parent_hash\": \"abc123...\",\n      \"files\": {\n        \"data.csv\": {\n          \"hash\": \"def456...\",\n          \"name\": \"data.csv\",\n          \"size\": 1024,\n          \"content_type\": \"text/csv\"\n        },\n        \"processed.csv\": {\n          \"hash\": \"jkl012...\",\n          \"name\": \"processed.csv\",\n          \"size\": 2048,\n          \"content_type\": \"text/csv\"\n        }\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"reference/storage-format/#commit-structure","title":"Commit Structure","text":"<p>Each commit contains:</p> <ul> <li>hash: SHA256 hash of the commit</li> <li>message: Human-readable commit message</li> <li>timestamp: ISO 8601 timestamp</li> <li>parent_hash: Hash of parent commit (null for first commit)</li> <li>files: Dictionary mapping filename to file metadata</li> </ul>"},{"location":"reference/storage-format/#file-metadata","title":"File Metadata","text":"<p>Each file entry contains:</p> <ul> <li>hash: Content hash of the file</li> <li>name: Original filename (including extension)</li> <li>size: File size in bytes</li> <li>content_type: MIME type of the file</li> </ul>"},{"location":"reference/storage-format/#data-structures","title":"Data Structures","text":""},{"location":"reference/storage-format/#file-entity","title":"File Entity","text":"<pre><code>@dataclass(frozen=True)\nclass File:\n    \"\"\"Represents a versioned file with content-addressed storage.\"\"\"\n\n    hash: str                    # Content hash (SHA256)\n    name: str                   # Original filename\n    size: int                   # File size in bytes\n    content_type: Optional[str] = None  # MIME type\n\n    def read_bytes(self) -&gt; bytes: ...\n    def open(self, mode: str = \"rb\") -&gt; Union[BinaryIO, TextIO]: ...\n    def download_to(self, path: Union[str, Path]) -&gt; str: ...\n    def exists(self) -&gt; bool: ...\n    def to_dict(self) -&gt; dict: ...\n</code></pre>"},{"location":"reference/storage-format/#commit-entity","title":"Commit Entity","text":"<pre><code>@dataclass(frozen=True)\nclass Commit:\n    \"\"\"Represents an immutable snapshot of files at a point in time.\"\"\"\n\n    hash: str                           # Commit hash\n    message: str                        # Commit message\n    timestamp: datetime                  # Creation timestamp\n    parent_hash: Optional[str]          # Parent commit hash\n    files: Dict[str, File]             # filename -&gt; File mapping\n\n    def get_file(self, name: str) -&gt; Optional[File]: ...\n    def list_files(self) -&gt; List[str]: ...\n    def has_file(self, name: str) -&gt; bool: ...\n    def get_total_size(self) -&gt; int: ...\n</code></pre>"},{"location":"reference/storage-format/#dataset-entity","title":"Dataset Entity","text":"<pre><code>class Dataset:\n    \"\"\"Represents a logical collection of files with linear history.\"\"\"\n\n    def __init__(self, root_dir: Union[str, Path], name: str,\n                 description: str = \"\",\n                 fs: Optional[fsspec.AbstractFileSystem] = None,\n                 # AWS/S3 authentication\n                 aws_profile: Optional[str] = None,\n                 # GCP/GCS authentication\n                 gcs_token: Optional[Union[str, Path]] = None,\n                 gcs_project: Optional[str] = None,\n                 # Azure authentication\n                 azure_account_name: Optional[str] = None,\n                 azure_account_key: Optional[str] = None,\n                 azure_connection_string: Optional[str] = None): ...\n\n    def commit(self, message: str, add_files: List[Union[str, Path]] = None,\n               remove_files: List[str] = None) -&gt; str: ...\n    def checkout(self, commit_hash: Optional[str] = None) -&gt; None: ...\n    def get_file(self, name: str) -&gt; Optional[File]: ...\n    def list_files(self) -&gt; List[str]: ...\n    def has_file(self, name: str) -&gt; bool: ...\n    def read_file(self, name: str, mode: str = \"r\") -&gt; Union[str, bytes]: ...\n    def download_file(self, name: str, target_path: Union[str, Path]) -&gt; str: ...\n    def open_file(self, name: str, mode: str = \"rb\") -&gt; Union[BinaryIO,\n                                                               TextIO]: ...\n    def local_files(self): ...  # Context manager for local file access\n    def history(self, limit: Optional[int] = None) -&gt; List[Commit]: ...\n    def get_commit(self, commit_hash: str) -&gt; Optional[Commit]: ...\n    def get_commits(self) -&gt; List[Commit]: ...\n    def is_empty(self) -&gt; bool: ...\n    def cleanup_orphaned_files(self) -&gt; int: ...\n    def get_info(self) -&gt; dict: ...\n    def to_dict(self) -&gt; dict: ...\n\n    # Properties\n    @property\n    def current_commit(self) -&gt; Optional[Commit]: ...\n    @property\n    def head(self) -&gt; Optional[Commit]: ...  # Alias for current_commit\n    @property\n    def files(self) -&gt; Dict[str, File]: ...  # Files from current commit\n</code></pre>"},{"location":"reference/storage-format/#catalog-entity","title":"Catalog Entity","text":"<pre><code>@dataclass\nclass Catalog:\n    \"\"\"Represents a collection of datasets.\"\"\"\n\n    root_dir: Union[str, fsspec.AbstractFileSystem]\n    fs: Optional[fsspec.AbstractFileSystem] = None\n    # AWS/S3 authentication\n    aws_profile: Optional[str] = None\n    # GCP/GCS authentication\n    gcs_token: Optional[Union[str, Path]] = None\n    gcs_project: Optional[str] = None\n    # Azure authentication\n    azure_account_name: Optional[str] = None\n    azure_account_key: Optional[str] = None\n    azure_connection_string: Optional[str] = None\n\n    def datasets(self) -&gt; List[str]: ...  # List dataset names\n    def get_dataset(self, dataset_name: str) -&gt; Dataset: ...  # Get existing dataset\n    def create_dataset(self, dataset_name: str,\n                       description: str = \"\") -&gt; Dataset: ...  # Create new dataset\n    def __len__(self) -&gt; int: ...  # Number of datasets\n</code></pre>"},{"location":"reference/storage-format/#content-addressing","title":"Content Addressing","text":""},{"location":"reference/storage-format/#hash-calculation","title":"Hash Calculation","text":"<p>Files are hashed using SHA256 directly on content bytes:</p> <pre><code>import hashlib\n\ndef calculate_hash(content: bytes) -&gt; str:\n    \"\"\"Calculate SHA256 hash of content bytes.\"\"\"\n    return hashlib.sha256(content).hexdigest()\n\n# Example usage in storage\ndef store_file(file_path: Path) -&gt; str:\n    with open(file_path, \"rb\") as f:\n        content = f.read()\n    return hashlib.sha256(content).hexdigest()\n</code></pre>"},{"location":"reference/storage-format/#deduplication","title":"Deduplication","text":"<p>Identical content is stored only once:</p> <pre><code># Two files with identical content\nfile1_content = b\"Hello, World!\"\nfile2_content = b\"Hello, World!\"\n\n# Both files get the same hash\nhash1 = hashlib.sha256(file1_content).hexdigest()\nhash2 = hashlib.sha256(file2_content).hexdigest()\n\nassert hash1 == hash2  # Same hash = same storage location\n</code></pre>"},{"location":"reference/storage-format/#content-integrity","title":"Content Integrity","text":"<p>Any change to file content changes the hash:</p> <pre><code># Original content\ncontent1 = b\"Hello, World!\"\nhash1 = hashlib.sha256(content1).hexdigest()\n\n# Modified content\ncontent2 = b\"Hello, World!\"  # Even a single character change\nhash2 = hashlib.sha256(content2).hexdigest()\n\nassert hash1 != hash2  # Different hash = different storage location\n</code></pre>"},{"location":"reference/storage-format/#commit-hash-generation","title":"Commit Hash Generation","text":"<p>Commit hashes are generated using file hashes, message, and timestamp:</p> <pre><code>def generate_commit_hash(files: Dict[str, File], message: str,\n                        parent_hash: Optional[str],\n                        timestamp: datetime) -&gt; str:\n    \"\"\"Generate commit hash from file hashes, message, and timestamp.\"\"\"\n    import hashlib\n\n    # Sort file hashes for consistency\n    file_hashes = sorted(file.hash for file in files.values())\n    parent_hash = parent_hash or \"\"\n\n    # Combine all components\n    content = (\n        \"\\n\".join(file_hashes) + \"\\n\" +\n        message + \"\\n\" +\n        parent_hash + \"\\n\" +\n        str(timestamp)\n    )\n\n    # Generate hash\n    hasher = hashlib.sha256()\n    hasher.update(content.encode(\"utf-8\"))\n    return hasher.hexdigest()\n</code></pre>"},{"location":"reference/storage-format/#backend-integration","title":"Backend Integration","text":""},{"location":"reference/storage-format/#fsspec-backends","title":"FSSpec Backends","text":"<p>Kirin supports any fsspec backend:</p> <pre><code># Local filesystem\nfs = fsspec.filesystem(\"file\")\n\n# S3\nfs = fsspec.filesystem(\"s3\", profile=\"my-profile\")\n\n# GCS\nfs = fsspec.filesystem(\"gcs\", token=\"/path/to/key.json\")\n\n# Azure\nfs = fsspec.filesystem(\"az\", connection_string=\"...\")\n</code></pre>"},{"location":"reference/storage-format/#storage-operations","title":"Storage Operations","text":"<pre><code># Store file content\ndef store_file(fs, file_path: Path) -&gt; str:\n    \"\"\"Store file and return content hash.\"\"\"\n    with open(file_path, \"rb\") as f:\n        content = f.read()\n\n    hash_value = hashlib.sha256(content).hexdigest()\n    storage_path = f\"data/{hash_value[:2]}/{hash_value[2:]}\"\n\n    fs.write_bytes(storage_path, content)\n    return hash_value\n\n# Retrieve file content\ndef retrieve_file(fs, hash_value: str) -&gt; bytes:\n    \"\"\"Retrieve file content by hash.\"\"\"\n    storage_path = f\"data/{hash_value[:2]}/{hash_value[2:]}\"\n    return fs.read_bytes(storage_path)\n</code></pre>"},{"location":"reference/storage-format/#performance-considerations","title":"Performance Considerations","text":""},{"location":"reference/storage-format/#zero-copy-operations","title":"Zero-Copy Operations","text":"<p>Kirin is designed with zero-copy philosophy:</p> <ul> <li>Reference-based operations: Use File objects as references instead of   copying content</li> <li>Lazy loading: File content is only downloaded when accessed</li> <li>Deduplication: Identical content is stored only once regardless of filename</li> </ul>"},{"location":"reference/storage-format/#caching","title":"Caching","text":"<p>Kirin implements commit-level caching for improved performance:</p> <pre><code># Commit objects are cached in memory\nclass CommitStore:\n    def __init__(self):\n        self._commits_cache: Dict[str, Commit] = {}\n\n    def get_commit(self, commit_hash: str) -&gt; Commit:\n        # Try cache first\n        if commit_hash in self._commits_cache:\n            return self._commits_cache[commit_hash]\n        # Load from storage if not cached\n        # ...\n</code></pre>"},{"location":"reference/storage-format/#lazy-loading","title":"Lazy Loading","text":"<p>Content loaded only when needed:</p> <pre><code># Lazy file loading\nclass LazyFile:\n    def __init__(self, fs, hash_value: str, name: str):\n        self.fs = fs\n        self.hash_value = hash_value\n        self.name = name\n        self._content = None\n\n    def read_bytes(self) -&gt; bytes:\n        if self._content is None:\n            self._content = retrieve_file(self.fs, self.hash_value)\n        return self._content\n</code></pre>"},{"location":"reference/storage-format/#migration-and-backup","title":"Migration and Backup","text":""},{"location":"reference/storage-format/#backup-strategies","title":"Backup Strategies","text":"<pre><code># Backup content store\nrsync -av data/ backup/data/\n\n# Backup dataset metadata\nrsync -av datasets/ backup/datasets/\n</code></pre>"},{"location":"reference/storage-format/#migration-between-backends","title":"Migration Between Backends","text":"<pre><code># Migrate from local to S3\nlocal_fs = fsspec.filesystem(\"file\")\ns3_fs = fsspec.filesystem(\"s3\", profile=\"my-profile\")\n\n# Copy content store\nfor root, dirs, files in os.walk(\"data\"):\n    for file in files:\n        local_path = os.path.join(root, file)\n        s3_path = f\"s3://my-bucket/{local_path}\"\n        s3_fs.put(local_path, s3_path)\n</code></pre>"},{"location":"reference/storage-format/#security-considerations","title":"Security Considerations","text":""},{"location":"reference/storage-format/#access-control","title":"Access Control","text":"<ul> <li>File permissions: Respect filesystem permissions</li> <li>Cloud IAM: Use appropriate cloud permissions</li> <li>Encryption: Support for encrypted storage backends</li> </ul>"},{"location":"reference/storage-format/#data-integrity","title":"Data Integrity","text":"<ul> <li>Hash verification: Verify content hashes on retrieval</li> <li>Tamper detection: Detect any content changes</li> <li>Audit trails: Track all storage operations</li> </ul>"},{"location":"reference/storage-format/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Architecture Overview - System architecture</li> </ul>"},{"location":"releases/v0.0.10/","title":"V0.0.10","text":""},{"location":"releases/v0.0.10/#version-0010","title":"Version 0.0.10","text":"<p>This release introduces a major improvement to the workflow for saving plots with datasets, making it easier and more flexible to commit visualizations. The deprecated save_plot() API has been removed in favor of a more streamlined approach. Additional improvements include enhanced documentation, updated tutorials, and code quality fixes.</p>"},{"location":"releases/v0.0.10/#new-features","title":"New Features","text":"<ul> <li>Added support for committing matplotlib and plotly plot objects directly via dataset.commit(), with automatic conversion to SVG or WebP formats and intelligent filename generation. Also includes new helper functions for plot serialization and comprehensive tests. Documentation and tutorials have been updated to reflect these changes. (67bcad0) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.10/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed line length issues and removed unused variables in dataset.py, plots.py, and documentation for improved code quality and readability. (d11239) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.10/#deprecations","title":"Deprecations","text":"<ul> <li>Removed the deprecated dataset.save_plot() method and its public API entry point. All plot saving functionality is now handled through dataset.commit() with plot objects. (67bcad0) (Eric Ma)</li> <li>Removed obsolete tests related to the deprecated save_plot() API. (730a0a) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.11/","title":"V0.0.11","text":""},{"location":"releases/v0.0.11/#version-0011","title":"Version 0.0.11","text":"<p>This release introduces major improvements to the widget system, including file previews, enhanced code snippet handling, and significant codebase refactoring for maintainability. Several bugs related to user interaction with widgets have also been fixed, and code formatting in tutorials has been improved for readability.</p>"},{"location":"releases/v0.0.11/#new-features","title":"New Features","text":"<ul> <li>Added file previews to widget HTML, supporting CSV (as tables), JSON, and text files, with interactive click handlers and comprehensive styling. Widget HTML generation now uses Jinja2 templates for maintainability. (344d2f, Eric Ma)</li> <li>Added support for variable names and a data-code attribute in widgets, allowing code snippets to use the correct variable name and improving test compatibility. Commit widgets now include the full hash in code snippets. (988a91, Eric Ma)</li> </ul>"},{"location":"releases/v0.0.11/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed copy button behavior by using event delegation, preventing file previews from opening when clicking the copy button and ensuring robust handling with dynamic content. (044b67, Eric Ma)</li> </ul>"},{"location":"releases/v0.0.11/#refactoring","title":"Refactoring","text":"<ul> <li>Extracted duplicated JavaScript and HTML code into shared modules and macros, introduced a BaseWidget class, and updated all templates and widgets to use these shared components, eliminating over 300 lines of duplication. (d9f23b, Eric Ma)</li> </ul>"},{"location":"releases/v0.0.11/#style-improvements","title":"Style Improvements","text":"<ul> <li>Reformatted code in the first-dataset.py tutorial for better readability and improved documentation presentation. (28bccd, Eric Ma)</li> </ul>"},{"location":"releases/v0.0.11/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.0.2/","title":"V0.0.2","text":""},{"location":"releases/v0.0.2/#version-002","title":"Version 0.0.2","text":"<p>This release introduces major new features for cloud storage support, a FastAPI-based web UI, improved authentication, and significant enhancements to dataset management and usability. The project now supports content-addressed storage with visible filenames, lazy file loading, and a more robust, cloud-agnostic architecture. Numerous improvements to documentation, testing, and CI/CD workflows are also included.</p>"},{"location":"releases/v0.0.2/#new-features","title":"New Features","text":"<ul> <li>Add FastAPI-based web UI for dataset, file, and commit management, with shadcn/ui-inspired design and HTMX partials. Includes CRUD operations, commit history visualization, and file preview. (ee5e7f, Eric Ma)</li> <li>Add cloud authentication helpers for S3, GCS, Azure, and S3-compatible services, enabling seamless local and cloud storage via fsspec. (ee5e7f, Eric Ma)</li> <li>Refactor Dataset and DatasetCommit to support fsspec filesystems, allowing all file operations to work with local and cloud storage. (ee5e7f, Eric Ma)</li> <li>Implement content-addressed storage with visible filenames, supporting deduplication and automatic migration from old formats. (89c920, Eric Ma)</li> <li>Add cloud-agnostic authentication parameters to Catalog, Dataset, and web UI, supporting S3, GCS, and Azure authentication. (9a2afb, Eric Ma)</li> <li>Add AWS profile selection and lazy directory creation for S3 compatibility, with improved error handling and unified catalog forms in the web UI. (b948ec, Eric Ma)</li> <li>Add secure cloud auth detection, keyring credential storage, and improved backend authentication UX, including CLI and web UI enhancements. (4dd1d2, Eric Ma)</li> <li>Implement lazy loading for local_files context manager in Dataset, introducing LazyLocalFiles for efficient file access and caching. (365b7d, Eric Ma)</li> <li>Add initial implementation of Data Catalog for managing collections of datasets, with methods for listing, retrieving, and creating datasets. (c1fc76, Eric Ma)</li> <li>Add dataset length functionality to Catalog and shared testing utilities for improved test coverage. (2bdbe3, Eric Ma)</li> <li>Add local file access context manager, partial hash resolution for Dataset.checkout, and improved file utilities for easier file operations. (637ac7, Eric Ma)</li> <li>Add search functionality to filter datasets by name or description in the web UI. (9acdeb, Eric Ma)</li> <li>Add tabbed interface for datasets and dataset creation, with improved responsive grid and utility classes in the web UI. (7c2750, Eric Ma)</li> <li>Add Python code snippets to web UI, including cloud authentication parameters and copy-to-clipboard functionality. (16857a, Eric Ma)</li> <li>Add rebase merge strategy for linear commit history, with updated visualization and Marimo notebook demo. (852f14, Eric Ma)</li> <li>Add interactive commit tree view with Mermaid.js and view toggle to dataset UI. (d1d845, Eric Ma)</li> <li>Add comprehensive regression tests for Git semantics and rebase merge workflow. (c5bce5, Eric Ma)</li> <li>Add debugging and testing scripts for commit structure and git semantics. (81e710, Eric Ma)</li> <li>Add detailed documentation for design, architecture, and best practices, including AGENTS.md, guides, and expanded README. (9ba556, Eric Ma; 8f2a62, Eric Ma)</li> <li>Add mkdocstrings and related dependencies for improved documentation generation. (4d5e32, Eric Ma)</li> </ul>"},{"location":"releases/v0.0.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fix file download endpoint in web UI and update integration tests to match new API routes. (c2083c, Eric Ma)</li> <li>Show correct commit counts for datasets on catalog landing page, with tests and documentation. (95082f, Eric Ma)</li> <li>Fix markdownlint compliance across all documentation files. (07b606, Eric Ma)</li> </ul>"},{"location":"releases/v0.0.2/#deprecations","title":"Deprecations","text":"<ul> <li>BREAKING CHANGE: Dataset and DatasetCommit now require fsspec filesystem compatibility; all file operations are routed through fsspec. Users must install appropriate cloud dependencies (s3fs, gcsfs, adlfs, etc.) for cloud storage support. (ee5e7f, Eric Ma)</li> <li>BREAKING CHANGE: The prototype notebook is replaced by new local and GCP demo notebooks. The new web UI is now the primary interface for dataset management. Old dataset creation patterns may need to be updated. (e80d04, Eric Ma)</li> <li>BREAKING CHANGE: Branching, merging, and web UI are no longer supported. All history is now linear-only. (5cdf91, Eric Ma)</li> <li>BREAKING CHANGE: All previous 'backend' routes, forms, and configuration are now replaced by 'catalog' equivalents in the web UI and API. (60c5cc, Eric Ma)</li> </ul> <p>This release marks a significant step forward in making Kirin a robust, cloud-ready, and user-friendly data versioning platform. Please review the breaking changes and update your usage patterns and dependencies as needed.</p>"},{"location":"releases/v0.0.3/","title":"V0.0.3","text":""},{"location":"releases/v0.0.3/#version-003","title":"Version 0.0.3","text":"<p>This release introduces a new CLI upload command, streamlines pre-commit hook configurations, and removes redundant tests for improved maintainability.</p>"},{"location":"releases/v0.0.3/#new-features","title":"New Features","text":"<ul> <li>Added a new upload command to the CLI, allowing users to upload files to a dataset in a catalog with a commit message. This includes argument parsing, validation, and comprehensive tests. (b73f84) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.3/#bug-fixes","title":"Bug Fixes","text":"<p>No bug fixes in this release.</p>"},{"location":"releases/v0.0.3/#deprecations","title":"Deprecations","text":"<p>No deprecations in this release.</p>"},{"location":"releases/v0.0.3/#other-changes","title":"Other Changes","text":"<ul> <li>Excluded release notes markdown files from pre-commit hooks to prevent unnecessary checks. (f7ffab) (Eric Ma)</li> <li>Updated and streamlined pre-commit hook exclude patterns for better maintainability. (016aa6) (Eric Ma)</li> <li>Removed a redundant test for the upload command help output. (ef0320) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.4/","title":"V0.0.4","text":""},{"location":"releases/v0.0.4/#version-004","title":"Version 0.0.4","text":"<p>This release includes a bug fix to the CLI entry point, ensuring the correct function is used when launching the application.</p>"},{"location":"releases/v0.0.4/#new-features","title":"New Features","text":"<p>No new features were added in this release.</p>"},{"location":"releases/v0.0.4/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated the CLI entry point in pyproject.toml to use kirin.cli:app instead of kirin.cli:main, ensuring the CLI launches correctly. (3ebdf7) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.4/#deprecations","title":"Deprecations","text":"<p>No deprecations were introduced in this release.</p>"},{"location":"releases/v0.0.5/","title":"V0.0.5","text":""},{"location":"releases/v0.0.5/#version-005","title":"Version 0.0.5","text":"<p>This release introduces automatic authentication and timeout protection for cloud catalog access in the web UI, along with improved documentation and testing for these new features. Users can now configure authentication commands for cloud providers, and the UI will automatically attempt to authenticate and retry operations if needed, providing a smoother and more secure experience.</p>"},{"location":"releases/v0.0.5/#new-features","title":"New Features","text":"<ul> <li>Added automatic authentication and timeout protection for cloud catalog access. If a catalog operation fails due to authentication or timeout, the system will attempt to authenticate using a configurable CLI command and retry the operation. This includes improved error handling and user feedback in the web UI, as well as timeout protection for dataset operations to prevent UI hangs. (68fd97) (Eric Ma)</li> <li>Documented the new web UI auto-authentication and timeout features, including configuration instructions for AWS, GCP, and Azure, security considerations, and troubleshooting guidance. Also added fast unit tests for authentication timeout and auto-execution behaviors. (bc4a92) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.5/#bug-fixes","title":"Bug Fixes","text":"<p>No bug fixes in this release.</p>"},{"location":"releases/v0.0.5/#deprecations","title":"Deprecations","text":"<p>No deprecations in this release.</p>"},{"location":"releases/v0.0.6/","title":"V0.0.6","text":""},{"location":"releases/v0.0.6/#version-006","title":"Version 0.0.6","text":"<p>This release introduces major new features for model and plot versioning, significant improvements to dependency management, and several bug fixes to enhance reliability and developer experience. The update also includes UI enhancements for authentication and code quality improvements throughout the codebase.</p>"},{"location":"releases/v0.0.6/#new-features","title":"New Features","text":"<ul> <li>Model Versioning Support: Added support for model versioning by extending commits and datasets to include metadata and tags. New methods for querying and comparing model versions are available, along with updated documentation and a demo notebook. All references from \"gitdata\" to \"kirin\" have been updated. (04b970) (Eric Ma)</li> <li>Plot Versioning with SVG/WebP and Thumbnails: Introduced plot saving functionality with automatic format detection (SVG for vector, WebP for raster), support for matplotlib and plotly, thumbnail generation and storage, and integration with the web UI for image previews. Comprehensive tests and documentation updates included. (29fbb4) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.6/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>/tmp/ File Filtering in CI: Fixed an issue where test files in <code>/tmp/</code> were incorrectly filtered out during CI runs, causing test failures. Now, only IPython-related temporary files are skipped. (a186a6) (Eric Ma)</li> <li>Pytest and Coverage in Pixi Environments: Added <code>pytest</code> and <code>pytest-cov</code> to the pixi test dependencies to fix failing CI jobs due to missing packages. (e08cda) (Eric Ma)</li> <li>Catalog Auto-Authentication: Improved catalog authentication by adding a dedicated endpoint, proactive authentication before listing datasets, and better error detection for expired tokens. The UI now allows users to manually trigger authentication. (a388ac) (Eric Ma)</li> <li>Erroneous Notebook Cell Removed: Deleted an unintended code cell from a prototype notebook to clean up the codebase. (2a698c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.6/#improvements-refactoring","title":"Improvements &amp; Refactoring","text":"<ul> <li>Pixi-First Dependency Management: Reorganized dependencies to follow a pixi-first approach, unpinning versions for flexibility, removing duplication, and improving documentation for dependency management. (1bd03f) (Eric Ma)</li> <li>Plot Source Linking Code Quality: Refactored plot source linking logic for better organization, type hints, error handling, and removed dead code. Tests and documentation were updated accordingly. (5c1245) (Eric Ma)</li> <li>Code Formatting: Cleaned up code formatting by removing unnecessary blank lines and condensing function calls for improved readability. (16740b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.6/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul> <p>Contributors: Eric Ma, dependabot[bot], github-actions</p>"},{"location":"releases/v0.0.7/","title":"V0.0.7","text":""},{"location":"releases/v0.0.7/#version-007","title":"Version 0.0.7","text":"<p>This release introduces major improvements to the file preview experience in the Web UI, including commit navigation for all file types, enhanced error handling, and comprehensive documentation updates. Several bugs affecting file previews and documentation formatting have also been addressed.</p>"},{"location":"releases/v0.0.7/#new-features","title":"New Features","text":"<ul> <li>Added commit navigation for all files in the preview modal, including Previous/Next buttons, contextual labels (Latest, Oldest, Commit X of Y), and support for all file types (not just images). This includes a new backend endpoint for file commit history, commit history caching, race condition protection, and comprehensive test coverage. (5275cd) (Eric Ma)</li> <li>Added comprehensive Web UI documentation, including new \"Getting Started\" and \"How-To Guide\" pages, expanded overview, and improved navigation and quickstart instructions. (5d5977) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.7/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed a bug where text file previews would incorrectly show an empty image; text files now preview correctly. (5275cd) (Eric Ma)</li> <li>Fixed commit navigation so it works for all files, not just images, and improved error handling for invalid checkout hashes. (5275cd) (Eric Ma)</li> <li>Fixed trailing whitespace in the contributors section of the v0.0.6 release notes. (c6621e) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.7/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.8/","title":"V0.0.8","text":""},{"location":"releases/v0.0.8/#version-008","title":"Version 0.0.8","text":"<p>This release brings major improvements to the interactive notebook experience, cloud storage documentation, and authentication performance. Users will find new HTML widgets for datasets and commits, a unified cloud storage setup guide, and enhanced authentication caching for smoother catalog browsing. Several tutorials and how-to guides have been added or improved, making it easier to get started and manage data with Kirin.</p>"},{"location":"releases/v0.0.8/#new-features","title":"New Features","text":"<ul> <li>Added interactive HTML widgets for datasets, commits, and catalogs in notebooks, including code snippet copy buttons and variable name customization. (3a42e0) (Eric Ma)</li> <li>Implemented interactive HTML representations with copy code functionality for Dataset, Commit, and Catalog classes, including file list previews and dynamic variable names. (65ee74) (Eric Ma)</li> <li>Added a comprehensive cloud storage tutorial and extended Marimo-to-Markdown conversion to cover tutorials. (cc691a) (Eric Ma)</li> <li>Added a how-to guide for managing multiple datasets using the Kirin Catalog feature. (c33dc2) (Eric Ma)</li> <li>Added a how-to guide for version controlling data pipelines with Kirin, demonstrating best practices for tracking and comparing pipeline outputs. (c33d2f) (Eric Ma)</li> <li>Added a step-by-step guide for setting up cloud storage backends (AWS S3, Google Cloud Storage, Azure Blob Storage). (b6696a) (Eric Ma)</li> <li>Enabled code copy button in the documentation theme for easier code reuse. (cad82d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.8/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Improved reliability of Marimo-to-Markdown conversion script, with better error handling and documentation summary updates. (320037) (Eric Ma)</li> <li>Improved modal spacing and layout for preview content and code snippets in the web UI. (3545ab) (Eric Ma)</li> <li>Fixed and clarified dataset modification steps and file operation examples in the first-dataset tutorial. (d9966c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.8/#performance-improvements","title":"Performance Improvements","text":"<ul> <li>Added authentication caching for catalogs, reducing redundant authentication calls and improving catalog listing performance. (5cb9d9) (Eric Ma)</li> <li>Extended authentication cache TTL from 5 to 30 minutes and reset TTL on use for longer, smoother sessions. (a3d564) (Eric Ma)</li> <li>Added in-memory caching for catalog dataset counts and improved status reporting in the UI. (67c6f0) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.8/#documentation-improvements","title":"Documentation Improvements","text":"<ul> <li>Restructured and clarified step explanations in the first-dataset tutorial for better learning flow. (b54ad7) (Eric Ma)</li> <li>Clarified use of Application Default Credentials with Workload Identity on GKE in cloud storage documentation and tutorials. (075c64, da54a3) (Eric Ma)</li> <li>Updated next steps in cloud storage tutorial to reference a unified setup guide. (ef9954) (Eric Ma)</li> <li>Simplified and consolidated navigation structure in documentation for a cleaner experience. (00cf92, 395cd4) (Eric Ma)</li> <li>Expanded and clarified Marimo notebook writing standards and best practices. (a65fd8) (Eric Ma)</li> <li>Improved documentation on versioning and updated examples to avoid version numbers in filenames. (4d94ea) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.8/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul> <p>Thank you to all contributors for making this release possible!</p>"},{"location":"releases/v0.0.9/","title":"V0.0.9","text":""},{"location":"releases/v0.0.9/#version-009","title":"Version 0.0.9","text":"<p>This release introduces major improvements to the web UI, enhanced support for machine learning artifact tracking, and several robustness and usability fixes. Users will benefit from richer commit metadata visualization, improved image preview controls, and more reliable model variable name detection, especially in CI environments.</p>"},{"location":"releases/v0.0.9/#new-features","title":"New Features","text":"<ul> <li>Display commit tags and metadata in dataset and file views, including badges for tags and a collapsible JSON metadata viewer with syntax highlighting. Added image preview mode toggle (fit/scroll) with persistent user preference, and improved UI consistency for tags and metadata sections. (9f3e46, Eric Ma)</li> <li>Enhance the commit() API to automatically handle scikit-learn models: auto-serialize models, extract hyperparameters and metrics, detect variable names for filenames, link source scripts, and store model-specific metadata. Added sklearn version tracking, comprehensive tests, and updated documentation. (98ce83, Eric Ma)</li> <li>Add ty type checker to pre-commit hooks for improved code quality. (1e65ba, Eric Ma)</li> </ul>"},{"location":"releases/v0.0.9/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fix detection of internal files in CI environments with duplicate /kirin/ segments, ensuring user code is correctly identified. Also, simplify and clean up model variable name detection logic. (3b5f64, Eric Ma)</li> <li>Improve robustness of model variable name detection in CI environments by handling frame inspection errors and avoiding source line loading. (a23f10, Eric Ma)</li> <li>Remove fallback to class names for variable name detection, raising clear errors when detection fails to ensure metadata consistency. (446706, Eric Ma)</li> <li>Fix end-of-file formatting to ensure exactly one newline. (5bb41d, Eric Ma)</li> <li>Fix linting issues related to line length and make the ty pre-commit hook optional. (a60d1e, Eric Ma)</li> </ul>"},{"location":"releases/v0.0.9/#deprecations","title":"Deprecations","text":"<ul> <li>Remove manual model saving and metadata creation steps from ml-workflow.py, as Kirin now auto-extracts and serializes models and metadata. Clean up redundant and unused code, and ensure commit() only passes essential metrics and tags. (08250a, Eric Ma)</li> <li>Remove fallback to class names for variable name detection in dataset and ML artifact code, requiring explicit variable names for metadata keys. (446706, Eric Ma)</li> </ul>"},{"location":"tutorials/cloud-storage/","title":"Cloud Storage","text":"<p>To run this notebook, click on the molab shield above or run the following command at the terminal:</p> <pre><code>uvx marimo edit --sandbox --mcp --no-token --watch https://github.com/nll-ai/kirin/blob/main/docs/tutorials/cloud-storage.py\n</code></pre> <pre><code>import marimo as mo\n</code></pre>"},{"location":"tutorials/cloud-storage/#cloud-storage-overview","title":"Cloud Storage Overview","text":"<p>This tutorial introduces you to using Kirin with cloud storage backends. You'll learn how Kirin works seamlessly with S3, GCS, Azure, and other cloud storage providers, and understand the key concepts for working with remote datasets.</p>"},{"location":"tutorials/cloud-storage/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Understanding cloud storage backends and how Kirin uses them</li> <li>How to create catalogs and datasets with cloud storage</li> <li>Authentication methods for different cloud providers</li> <li>Working with remote files using the same API as local files</li> <li>Key differences and considerations when using cloud storage</li> <li>Best practices for cloud storage workflows</li> </ul>"},{"location":"tutorials/cloud-storage/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Your First Dataset tutorial</li> <li>Basic understanding of datasets and commits</li> <li>Familiarity with cloud storage concepts (S3, GCS, Azure)</li> </ul> <pre><code>import tempfile\nfrom pathlib import Path\n\nfrom kirin import Catalog\n\n# For this tutorial, we'll use local storage to demonstrate concepts\n# In production, you would use: Catalog(root_dir=\"s3://my-bucket/data\")\ntemp_dir = Path(tempfile.mkdtemp(prefix=\"kirin_cloud_tutorial_\"))\ncatalog = Catalog(root_dir=temp_dir)\n\nprint(f\"\u2705 Created catalog with root: {catalog.root_dir}\")\nprint(\"   (In production, this would be a cloud URL like s3://bucket/data)\")\n</code></pre>"},{"location":"tutorials/cloud-storage/#understanding-cloud-storage-backends","title":"Understanding Cloud Storage Backends","text":"<p>Kirin supports multiple cloud storage backends through the fsspec library. This means you can use the same API whether you're working with local files or cloud storage.</p> <p>Supported Backends:</p> <ul> <li>AWS S3: <code>s3://bucket/path</code></li> <li>Google Cloud Storage: <code>gs://bucket/path</code></li> <li>Azure Blob Storage: <code>az://container/path</code></li> <li>And many more: Dropbox, Google Drive, FTP, etc.</li> </ul> <p>The key insight is that Kirin treats all storage backends the same way - you use the same methods and patterns regardless of where your data is stored.</p>"},{"location":"tutorials/cloud-storage/#step-1-creating-catalogs-with-cloud-storage","title":"Step 1: Creating Catalogs with Cloud Storage","text":"<p>A catalog is a collection of datasets. When you create a catalog with a cloud storage URL, all datasets in that catalog will be stored in the cloud.</p> <p>The API is identical whether you're using local or cloud storage - you just change the <code>root_dir</code> parameter.</p> <pre><code># Local storage (what we're using in this tutorial)\ncatalog = Catalog(root_dir=\"/path/to/local/data\")\n\n# AWS S3\ncatalog = Catalog(\n    root_dir=\"s3://{{ bucket_name }}/data\",\n    aws_profile=\"{{ aws_profile }}\"\n)\n\n# Google Cloud Storage\ncatalog = Catalog(\n    root_dir=\"gs://{{ bucket_name }}/data\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"{{ project_id }}\"\n)\n\n# Azure Blob Storage\ncatalog = Catalog(\n    root_dir=\"az://{{ container_name }}/data\",\n    azure_connection_string=\"{{ connection_string }}\"\n)\n</code></pre> <p>Key points:</p> <ul> <li>The URL scheme (<code>s3://</code>, <code>gs://</code>, <code>az://</code>) tells Kirin which backend to   use</li> <li>Authentication parameters are passed when creating the catalog</li> <li>Once created, you use the catalog the same way regardless of backend</li> </ul> <pre><code># Create a dataset in the catalog (works the same for local or cloud)\ndataset = catalog.create_dataset(\n    \"cloud_demo\", description=\"Demo dataset for cloud storage tutorial\"\n)\n\nprint(f\"\u2705 Created dataset: {dataset.name}\")\nprint(f\"   Dataset root: {dataset.root_dir}\")\nprint(\"   (This would be in cloud storage if using s3://, gs://, etc.)\")\n</code></pre>"},{"location":"tutorials/cloud-storage/#step-2-the-same-api-for-local-and-cloud","title":"Step 2: The Same API for Local and Cloud","text":"<p>One of Kirin's strengths is that the API is identical whether you're working with local files or cloud storage. This means:</p> <ul> <li>You can develop locally and deploy to cloud without code changes</li> <li>The same patterns work everywhere</li> <li>You don't need to learn different APIs for different backends</li> </ul> <pre><code># Create some sample files\ndata_dir = temp_dir / \"sample_data\"\ndata_dir.mkdir(exist_ok=True)\n\n# Create a CSV file\ncsv_file = data_dir / \"sales_data.csv\"\ncsv_file.write_text(\"\"\"date,product,revenue\n2024-01-01,Widget A,1000\n2024-01-02,Widget B,1500\n2024-01-03,Widget A,1200\n\"\"\")\n\n# Create a JSON config file\nconfig_file = data_dir / \"config.json\"\nconfig_file.write_text(\"\"\"{\n\"version\": \"1.0\",\n\"region\": \"us-east-1\",\n\"currency\": \"USD\"\n}\n\"\"\")\n\nprint(\"\u2705 Created sample files:\")\nprint(f\"   - {csv_file.name}\")\nprint(f\"   - {config_file.name}\")\n</code></pre> <pre><code># Commit files - same API whether local or cloud!\ncommit_hash = dataset.commit(\n    message=\"Initial commit: Add sales data and configuration\",\n    add_files=[str(csv_file), str(config_file)],\n)\n\nprint(f\"\u2705 Created commit: {commit_hash[:8]}\")\nprint(\"   (Files are now stored in content-addressed storage)\")\nprint(\"   (If using cloud, files are uploaded to cloud storage)\")\n</code></pre> <p>What just happened?</p> <ul> <li>Files were committed to the dataset</li> <li>If using cloud storage, files were uploaded to the cloud backend</li> <li>Files are stored using content-addressed storage (by content hash)</li> <li>The commit was created and stored in the dataset's history</li> </ul> <p>The process is identical whether you're using local or cloud storage!</p>"},{"location":"tutorials/cloud-storage/#step-3-working-with-remote-files","title":"Step 3: Working with Remote Files","text":"<p>When you access files from a cloud storage dataset, Kirin handles the complexity of downloading files on-demand. You use the same <code>local_files()</code> context manager as with local storage.</p> <pre><code># Access files - same API for local and cloud!\nwith dataset.local_files() as local_files:\n    print(\"\ud83d\udcc2 Files available locally:\")\n    for filename, local_path in local_files.items():\n        print(f\"   {filename} -&gt; {local_path}\")\n\n    # Read file content (works the same for local or cloud)\n    csv_path = local_files[\"sales_data.csv\"]\n    csv_content = Path(csv_path).read_text()\n    print(\"\\n\ud83d\udcdd CSV content:\")\n    print(csv_content[:200] + \"...\")\n</code></pre> <p>How it works with cloud storage:</p> <ul> <li>When you access a file, Kirin downloads it from cloud storage</li> <li>Files are cached locally in a temporary directory</li> <li>When you exit the context manager, temporary files are cleaned up</li> <li>This is called lazy loading - files are only downloaded when needed</li> </ul> <p>Benefits:</p> <ul> <li>Efficient: Only download what you use</li> <li>Automatic cleanup: Temporary files are managed for you</li> <li>Same API: Works identically for local and cloud storage</li> </ul>"},{"location":"tutorials/cloud-storage/#understanding-authentication","title":"Understanding Authentication","text":"<p>Different cloud providers use different authentication methods. Kirin supports the standard authentication patterns for each provider.</p> <p>Key concept: Authentication is configured when you create the catalog or dataset, not when you access files. Once authenticated, all operations use those credentials automatically.</p>"},{"location":"tutorials/cloud-storage/#aws-s3-authentication-methods","title":"AWS S3 Authentication Methods","text":"<p>1. AWS Profile (Recommended for Development)</p> <pre><code>catalog = Catalog(\n    root_dir=\"s3://{{ bucket_name }}/data\",\n    aws_profile=\"{{ aws_profile }}\"\n)\n</code></pre> <p>2. Environment Variables</p> <pre><code>export AWS_ACCESS_KEY_ID={{ access_key_id }}\nexport AWS_SECRET_ACCESS_KEY={{ secret_access_key }}\nexport AWS_DEFAULT_REGION={{ region }}\n</code></pre> <p>Then use without explicit credentials:</p> <pre><code>catalog = Catalog(root_dir=\"s3://{{ bucket_name }}/data\")\n</code></pre> <p>3. IAM Roles (Production)</p> <p>When running on EC2, ECS, or Lambda, IAM roles are used automatically:</p> <pre><code># No credentials needed - uses IAM role\ncatalog = Catalog(root_dir=\"s3://{{ bucket_name }}/data\")\n</code></pre>"},{"location":"tutorials/cloud-storage/#google-cloud-storage-authentication-methods","title":"Google Cloud Storage Authentication Methods","text":"<p>1. Service Account Key File</p> <pre><code>catalog = Catalog(\n    root_dir=\"gs://{{ bucket_name }}/data\",\n    gcs_token=\"/path/to/service-account.json\",\n    gcs_project=\"{{ project_id }}\"\n)\n</code></pre> <p>2. Application Default Credentials</p> <pre><code># One-time setup\ngcloud auth application-default login\n</code></pre> <p>Then use without explicit credentials:</p> <pre><code>catalog = Catalog(root_dir=\"gs://{{ bucket_name }}/data\")\n</code></pre> <p>3. Workload Identity (GKE/Kubernetes)</p> <p>When running on GKE with Workload Identity configured, Application Default Credentials automatically detect credentials from the metadata server (which is how Workload Identity works):</p> <pre><code># No credentials needed - uses ADC (which includes Workload Identity on GKE)\ncatalog = Catalog(root_dir=\"gs://{{ bucket_name }}/data\")\n</code></pre>"},{"location":"tutorials/cloud-storage/#azure-blob-storage-authentication-methods","title":"Azure Blob Storage Authentication Methods","text":"<p>1. Connection String</p> <pre><code>import os\ncatalog = Catalog(\n    root_dir=\"az://{{ container_name }}/data\",\n    azure_connection_string=os.getenv(\"AZURE_CONNECTION_STRING\")\n)\n</code></pre> <p>2. Account Name and Key</p> <pre><code>catalog = Catalog(\n    root_dir=\"az://{{ container_name }}/data\",\n    azure_account_name=\"{{ account_name }}\",\n    azure_account_key=\"{{ account_key }}\"\n)\n</code></pre> <p>3. Azure CLI Authentication</p> <pre><code># One-time setup\naz login\n</code></pre> <p>Then use without explicit credentials:</p> <pre><code>catalog = Catalog(root_dir=\"az://{{ container_name }}/data\")\n</code></pre>"},{"location":"tutorials/cloud-storage/#step-4-key-differences-from-local-storage","title":"Step 4: Key Differences from Local Storage","text":"<p>While the API is the same, there are some important differences to understand when working with cloud storage.</p>"},{"location":"tutorials/cloud-storage/#network-latency","title":"Network Latency","text":"<p>Cloud storage operations involve network requests, which adds latency:</p> <ul> <li>Local storage: Instant file access</li> <li>Cloud storage: Network round-trip time (typically 10-100ms)</li> </ul> <p>Impact: Operations like listing files or checking if a file exists take longer with cloud storage.</p> <p>Mitigation: Use batch operations when possible, and cache results when appropriate.</p>"},{"location":"tutorials/cloud-storage/#file-download-behavior","title":"File Download Behavior","text":"<p>With cloud storage, files are downloaded on-demand:</p> <ul> <li>Local storage: Files are already on disk</li> <li>Cloud storage: Files are downloaded when accessed via   <code>local_files()</code></li> </ul> <p>Impact: First access to a file takes longer (download time).</p> <p>Mitigation: Files are cached during the <code>local_files()</code> context, so multiple accesses to the same file don't re-download.</p>"},{"location":"tutorials/cloud-storage/#cost-considerations","title":"Cost Considerations","text":"<p>Cloud storage has usage-based costs:</p> <ul> <li>Storage costs: Pay for data stored</li> <li>Request costs: Pay for API requests (PUT, GET, LIST)</li> <li>Data transfer costs: Pay for data downloaded (in some cases)</li> </ul> <p>Best practices:</p> <ul> <li>Batch operations to reduce request counts</li> <li>Use appropriate storage classes (S3 Standard, IA, Glacier)</li> <li>Compress files before storing</li> <li>Monitor usage through cloud provider dashboards</li> </ul>"},{"location":"tutorials/cloud-storage/#authentication-requirements","title":"Authentication Requirements","text":"<p>Cloud storage requires authentication, while local storage does not:</p> <ul> <li>Local storage: No authentication needed</li> <li>Cloud storage: Must provide credentials (profile, keys, etc.)</li> </ul> <p>Best practices:</p> <ul> <li>Use IAM roles/service accounts in production (not access keys)</li> <li>Rotate credentials regularly</li> <li>Use least privilege (only grant necessary permissions)</li> <li>Never commit credentials to version control</li> </ul>"},{"location":"tutorials/cloud-storage/#step-5-best-practices-for-cloud-storage","title":"Step 5: Best Practices for Cloud Storage","text":"<p>Following best practices helps you build efficient, secure, and cost-effective cloud storage workflows.</p>"},{"location":"tutorials/cloud-storage/#security-best-practices","title":"Security Best Practices","text":"<p>1. Use IAM Roles/Service Accounts</p> <p>In production, use managed identities instead of access keys:</p> <pre><code># \u2705 Good: Uses IAM role automatically (on EC2/ECS/Lambda)\ncatalog = Catalog(root_dir=\"s3://{{ bucket_name }}/data\")\n\n# \u274c Avoid: Hardcoded credentials\ncatalog = Catalog(\n    root_dir=\"s3://{{ bucket_name }}/data\",\n    aws_access_key_id=\"AKIA...\",\n    aws_secret_access_key=\"...\"\n)\n</code></pre> <p>2. Least Privilege</p> <p>Grant only the permissions needed:</p> <ul> <li>Read-only access for datasets that won't be modified</li> <li>Write access only where necessary</li> <li>Use bucket policies to restrict access</li> </ul> <p>3. Monitor Access</p> <p>Enable audit logging in your cloud provider to track access patterns.</p>"},{"location":"tutorials/cloud-storage/#performance-best-practices","title":"Performance Best Practices","text":"<p>1. Batch Operations</p> <p>Group multiple file operations together:</p> <pre><code># \u2705 Good: Single commit with multiple files\ndataset.commit(\n    message=\"Add multiple files\",\n    add_files=[\"file1.csv\", \"file2.csv\", \"file3.csv\"]\n)\n\n# \u274c Avoid: Multiple separate commits\ndataset.commit(message=\"Add file1\", add_files=[\"file1.csv\"])\ndataset.commit(message=\"Add file2\", add_files=[\"file2.csv\"])\ndataset.commit(message=\"Add file3\", add_files=[\"file3.csv\"])\n</code></pre> <p>2. Use Appropriate Regions</p> <p>Store data in the same region as your compute resources to minimize latency.</p> <p>3. Process Files in Chunks</p> <p>For large files, use chunked processing:</p> <pre><code>with dataset.local_files() as local_files:\n    if \"large_data.csv\" in local_files:\n        local_path = local_files[\"large_data.csv\"]\n        # Process in chunks\n        for chunk in pd.read_csv(local_path, chunksize=10000):\n            process_chunk(chunk)\n</code></pre>"},{"location":"tutorials/cloud-storage/#cost-optimization-best-practices","title":"Cost Optimization Best Practices","text":"<p>1. Use Appropriate Storage Classes</p> <p>Different storage classes have different costs:</p> <ul> <li>Standard: Fast access, higher cost</li> <li>Infrequent Access (IA): Lower cost, slightly slower</li> <li>Glacier/Archive: Lowest cost, slow retrieval</li> </ul> <p>2. Enable Lifecycle Policies</p> <p>Automatically move old data to cheaper storage classes:</p> <pre><code># Example: Move files older than 90 days to IA\n# (Configured in cloud provider console, not in Kirin)\n</code></pre> <p>3. Compress Files</p> <p>Compress text files before storing to reduce storage costs:</p> <pre><code>import gzip\nimport shutil\n\n# Compress before committing\nwith open(\"data.csv\", \"rb\") as f_in:\n    with gzip.open(\"data.csv.gz\", \"wb\") as f_out:\n        shutil.copyfileobj(f_in, f_out)\n\ndataset.commit(message=\"Add compressed data\", add_files=[\"data.csv.gz\"])\n</code></pre>"},{"location":"tutorials/cloud-storage/#step-6-common-workflows","title":"Step 6: Common Workflows","text":"<p>Here are some common patterns for working with cloud storage in Kirin.</p>"},{"location":"tutorials/cloud-storage/#development-to-production-workflow","title":"Development to Production Workflow","text":"<p>Pattern: Develop locally, deploy to cloud</p> <pre><code># Development (local)\ndev_catalog = Catalog(root_dir=\"./local_data\")\ndev_dataset = dev_catalog.create_dataset(\"my_dataset\")\n\n# Production (cloud)\nprod_catalog = Catalog(\n    root_dir=\"s3://{{ bucket_name }}/data\",\n    aws_profile=\"{{ aws_profile }}\"\n)\nprod_dataset = prod_catalog.create_dataset(\"my_dataset\")\n\n# Same code works for both!\ncommit_hash = dataset.commit(\n    message=\"Add data\",\n    add_files=[\"data.csv\"]\n)\n</code></pre> <p>The API is identical, so you can develop locally and deploy to cloud without code changes.</p>"},{"location":"tutorials/cloud-storage/#multi-environment-workflow","title":"Multi-Environment Workflow","text":"<p>Pattern: Use different catalogs for different environments</p> <pre><code># Environment-specific catalogs\ncatalogs = {\n    \"dev\": Catalog(root_dir=\"./local_data\"),\n    \"staging\": Catalog(root_dir=\"s3://staging-bucket/data\"),\n    \"prod\": Catalog(root_dir=\"s3://prod-bucket/data\")\n}\n\n# Use appropriate catalog for environment\nenv = os.getenv(\"ENVIRONMENT\", \"dev\")\ncatalog = catalogs[env]\ndataset = catalog.get_dataset(\"my_dataset\")\n</code></pre> <p>This pattern allows you to use the same code across environments while keeping data isolated.</p>"},{"location":"tutorials/cloud-storage/#hybrid-workflow","title":"Hybrid Workflow","text":"<p>Pattern: Mix local and cloud storage</p> <pre><code># Local catalog for development\nlocal_catalog = Catalog(root_dir=\"./local_data\")\n\n# Cloud catalog for shared/production data\ncloud_catalog = Catalog(\n    root_dir=\"s3://{{ bucket_name }}/data\",\n    aws_profile=\"{{ aws_profile }}\"\n)\n\n# Use local for development, cloud for production\nif os.getenv(\"ENVIRONMENT\") == \"production\":\n    catalog = cloud_catalog\nelse:\n    catalog = local_catalog\n</code></pre> <p>This allows you to develop locally while using cloud storage for production workloads.</p>"},{"location":"tutorials/cloud-storage/#step-7-troubleshooting-common-issues","title":"Step 7: Troubleshooting Common Issues","text":"<p>Here are solutions to common problems when working with cloud storage.</p>"},{"location":"tutorials/cloud-storage/#ssl-certificate-errors","title":"SSL Certificate Errors","text":"<p>Problem: <code>SSLCertVerificationError</code> when connecting to cloud storage</p> <p>Solution: Set up SSL certificates for isolated Python environments:</p> <pre><code>python -m kirin.setup_ssl\n</code></pre> <p>This is especially important when using pixi, uv, or other isolated Python environments.</p>"},{"location":"tutorials/cloud-storage/#authentication-failures","title":"Authentication Failures","text":"<p>Problem: \"Access Denied\" or \"Anonymous caller\" errors</p> <p>Solutions:</p> <ol> <li> <p>Check credentials are configured: <pre><code># AWS\naws configure list\n\n# GCP\ngcloud auth list\n\n# Azure\naz account show\n</code></pre></p> </li> <li> <p>Verify credentials have correct permissions:</p> </li> <li>Check IAM policies for your user/role</li> <li> <p>Ensure bucket/container permissions are correct</p> </li> <li> <p>Test credentials directly: <pre><code># AWS\nimport boto3\ns3 = boto3.client('s3')\ns3.list_buckets()  # Should work without errors\n</code></pre></p> </li> </ol>"},{"location":"tutorials/cloud-storage/#performance-issues","title":"Performance Issues","text":"<p>Problem: Slow operations with cloud storage</p> <p>Solutions:</p> <ol> <li>Use same region: Store data in the same region as compute</li> <li>Batch operations: Group multiple operations together</li> <li>Cache results: Cache file lists and metadata when appropriate</li> <li>Use appropriate storage class: Standard for frequently accessed data</li> </ol> <p>Problem: Large file downloads are slow</p> <p>Solutions:</p> <ol> <li>Process in chunks: Don't download entire large files at once</li> <li>Use streaming: For very large files, consider streaming approaches</li> <li>Compress files: Smaller files download faster</li> </ol>"},{"location":"tutorials/cloud-storage/#summary","title":"Summary","text":"<p>Congratulations! You've learned the fundamentals of using Kirin with cloud storage:</p> <ul> <li>\u2705 Understanding cloud backends - S3, GCS, Azure, and more</li> <li>\u2705 Creating cloud catalogs - Same API for local and cloud</li> <li>\u2705 Authentication methods - Different approaches for each provider</li> <li>\u2705 Working with remote files - Lazy loading and automatic cleanup</li> <li>\u2705 Key differences - Network latency, costs, authentication</li> <li>\u2705 Best practices - Security, performance, cost optimization</li> <li>\u2705 Common workflows - Development to production patterns</li> <li>\u2705 Troubleshooting - Solutions to common issues</li> </ul>"},{"location":"tutorials/cloud-storage/#key-concepts","title":"Key Concepts","text":"<ul> <li>Unified API: Same methods work for local and cloud storage</li> <li>Content-addressed storage: Files stored by hash, works the same   everywhere</li> <li>Lazy loading: Files downloaded on-demand when accessed</li> <li>Authentication: Configured at catalog/dataset creation, not per   operation</li> <li>Backend-agnostic: Switch between backends by changing the URL</li> </ul>"},{"location":"tutorials/cloud-storage/#next-steps","title":"Next Steps","text":"<ul> <li>Setup Cloud Storage - Complete   guide for setting up AWS S3, Google Cloud Storage, and Azure Blob   Storage</li> <li>Track Model Training Data - See   cloud storage in action with ML workflows</li> </ul>"},{"location":"tutorials/commits/","title":"Commits","text":"<p>To run this notebook, click on the molab shield above or run the following command at the terminal:</p> <pre><code>uvx marimo edit --sandbox --mcp --no-token --watch https://github.com/nll-ai/kirin/blob/main/docs/tutorials/commits.py\n</code></pre> <pre><code>import marimo as mo\n</code></pre>"},{"location":"tutorials/commits/#working-with-commits","title":"Working with Commits","text":"<p>This tutorial deep dives into Kirin's commit system. You'll learn how commits work, how to navigate commit history, compare commits, and use commits effectively in your data science workflows.</p>"},{"location":"tutorials/commits/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Understanding commit structure and properties</li> <li>Navigating linear commit history</li> <li>Comparing commits to see what changed</li> <li>Working with specific commits</li> <li>Best practices for commit messages and workflows</li> </ul>"},{"location":"tutorials/commits/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Your First Dataset tutorial</li> <li>Basic understanding of datasets and files</li> </ul> <pre><code>import tempfile\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom kirin import Catalog\n\n# Create a temporary directory for our tutorial\n# In production, you might use: Catalog(root_dir=\"s3://my-bucket/data\")\ntemp_dir = Path(tempfile.mkdtemp(prefix=\"kirin_commits_tutorial_\"))\ncatalog = Catalog(root_dir=temp_dir)\n\n# Create a new dataset\ncommit_demo_dataset = catalog.create_dataset(\n    \"commit_demo\", description=\"Demo dataset for commit tutorial\"\n)\n\n# Create a directory for our data files\ndata_dir = temp_dir / \"sample_data\"\ndata_dir.mkdir(exist_ok=True)\n\nprint(f\"\u2705 Created dataset: {commit_demo_dataset.name}\")\nprint(f\"   Dataset root: {commit_demo_dataset.root_dir}\")\n</code></pre>"},{"location":"tutorials/commits/#step-1-understanding-commit-structure","title":"Step 1: Understanding Commit Structure","text":"<p>A commit in Kirin is an immutable snapshot of files at a specific point in time. Unlike Git, Kirin uses a linear commit history - each commit has exactly one parent, creating a simple chain:</p> <pre><code>Initial Commit \u2192 Commit 2 \u2192 Commit 3 \u2192 Commit 4\n</code></pre> <p>Let's create some commits and explore what makes up a commit.</p> <pre><code># Create first commit\nfile1 = data_dir / \"data.csv\"\nfile1.write_text(\"name,value\\nA,10\\nB,20\\n\")\n\ncommit_msg1 = f\"Initial data - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\ncommit_hash1 = commit_demo_dataset.commit(\n    message=commit_msg1, add_files=[str(file1)]\n)\n\nprint(f\"\u2705 Created first commit: {commit_hash1[:8]}\")\n</code></pre> <pre><code># Create second commit with updated data (same filename - versioning!)\nfile2 = data_dir / \"data.csv\"\nfile2.write_text(\"name,value\\nA,10\\nB,20\\nC,30\\n\")\n\ncommit_msg2 = f\"Add more data - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\ncommit_hash2 = commit_demo_dataset.commit(\n    message=commit_msg2, add_files=[str(file2)]\n)\n\nprint(f\"\u2705 Created second commit: {commit_hash2[:8]}\")\nprint(\"   Note: Same filename - versioning handled by commits!\")\n</code></pre> <pre><code># Get the current commit and explore its properties\ncurrent_commit = commit_demo_dataset.current_commit\nif current_commit:\n    print(\"\ud83d\udcca Commit Properties:\")\n    print(f\"   Hash: {current_commit.hash}\")\n    print(f\"   Short hash: {current_commit.short_hash}\")\n    print(f\"   Message: {current_commit.message}\")\n    print(f\"   Timestamp: {current_commit.timestamp}\")\n    parent_hash_display = (\n        current_commit.parent_hash[:8]\n        if current_commit.parent_hash\n        else \"None (initial)\"\n    )\n    print(f\"   Parent hash: {parent_hash_display}\")\n    print(f\"   Files: {current_commit.list_files()}\")\n    print(f\"   File count: {current_commit.get_file_count()}\")\n    print(f\"   Total size: {current_commit.get_total_size()} bytes\")\n    print(f\"   Is initial: {current_commit.is_initial}\")\n</code></pre> <p>Key properties:</p> <ul> <li>Hash: Unique identifier (SHA256) for the commit</li> <li>Message: Human-readable description of what changed</li> <li>Timestamp: When the commit was created</li> <li>Parent hash: Reference to the previous commit (None for initial   commit)</li> <li>Files: Dictionary of File objects in this commit</li> </ul>"},{"location":"tutorials/commits/#step-2-viewing-commit-history","title":"Step 2: Viewing Commit History","text":"<p>The commit history is a linear sequence. Let's explore it to see how commits are organized.</p> <pre><code># Get all commits (newest to oldest)\nhistory = commit_demo_dataset.history()\n# Reverse to show oldest first for tutorial clarity\nhistory_oldest_first = list(reversed(history))\n\nprint(f\"\ud83d\udcca Total commits: {len(history)}\")\nprint(\"\\nCommit History (oldest \u2192 newest):\")\nprint(\"=\" * 50)\n\nfor step_num, history_commit in enumerate(history_oldest_first, 1):\n    print(f\"\\n{step_num}. {history_commit.short_hash}: {history_commit.message}\")\n    print(f\"   Date: {history_commit.timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(f\"   Files: {', '.join(history_commit.list_files())}\")\n    parent_display = (\n        history_commit.parent_hash[:8]\n        if history_commit.parent_hash\n        else \"None (initial)\"\n    )\n    print(f\"   Parent: {parent_display}\")\n</code></pre> <p>Understanding the history:</p> <ul> <li>History is returned from newest to oldest (latest commit first)</li> <li>Each commit (except the first) has a parent</li> <li>The latest commit is <code>history[0]</code> or use <code>dataset.current_commit</code></li> <li>To display oldest first, reverse the list: <code>list(reversed(history))</code></li> </ul>"},{"location":"tutorials/commits/#step-3-limiting-history","title":"Step 3: Limiting History","text":"<p>For large datasets with many commits, you can limit how many commits to retrieve. This is useful for performance and focusing on recent changes.</p> <pre><code># Get only the 5 most recent commits\nrecent_commits = commit_demo_dataset.history(limit=5)\n\nprint(f\"\ud83d\udcca Recent commits: {len(recent_commits)}\")\nfor recent_commit in recent_commits:\n    print(f\"   {recent_commit.short_hash}: {recent_commit.message}\")\n</code></pre>"},{"location":"tutorials/commits/#step-4-getting-specific-commits","title":"Step 4: Getting Specific Commits","text":"<p>You can retrieve a specific commit by its hash. This is useful when you know exactly which commit you want to work with.</p> <pre><code># Get a specific commit (using the oldest commit for demonstration)\noldest_commit_hash = history[-1].hash  # Oldest is last in newest-first list\nretrieved_commit = commit_demo_dataset.get_commit(oldest_commit_hash)\n\nif retrieved_commit:\n    print(f\"\u2705 Retrieved commit: {retrieved_commit.short_hash}\")\n    print(f\"   Message: {retrieved_commit.message}\")\n    print(f\"   Files: {retrieved_commit.list_files()}\")\nelse:\n    print(\"\u274c Commit not found\")\n</code></pre>"},{"location":"tutorials/commits/#step-5-checking-out-commits","title":"Step 5: Checking Out Commits","text":"<p>\"Checking out\" a commit means switching the dataset to that commit's state. This lets you see what files were available at that point in time.</p> <pre><code># Checkout the latest commit (default)\ncommit_demo_dataset.checkout()\ncurrent_commit_hash = (\n    commit_demo_dataset.current_commit.short_hash\n    if commit_demo_dataset.current_commit\n    else \"None\"\n)\nprint(f\"\ud83d\udcc2 Current commit: {current_commit_hash}\")\nprint(f\"   Files: {list(commit_demo_dataset.files.keys())}\")\n</code></pre> <pre><code># Checkout a specific commit (using the oldest commit)\noldest_commit_for_checkout = history[-1]  # Oldest is last in newest-first list\ncommit_demo_dataset.checkout(oldest_commit_for_checkout.hash)\n\nprint(\"\\n\ud83d\udcc2 After checkout:\")\nprint(f\"   Current commit: {commit_demo_dataset.current_commit.short_hash}\")\nprint(f\"   Files: {list(commit_demo_dataset.files.keys())}\")\n</code></pre> <pre><code># Checkout latest again\ncommit_demo_dataset.checkout()  # No argument = latest\nprint(\"\\n\ud83d\udcc2 Back to latest:\")\nprint(f\"   Current commit: {commit_demo_dataset.current_commit.short_hash}\")\nprint(f\"   Files: {list(commit_demo_dataset.files.keys())}\")\n</code></pre> <p>Important: Checking out a commit doesn't delete anything - it just changes which files are \"current\" in the dataset. All commits and their files remain accessible.</p>"},{"location":"tutorials/commits/#step-6-comparing-commits","title":"Step 6: Comparing Commits","text":"<p>One of the most powerful features is comparing commits to see what changed between them. This helps you understand how your dataset evolved over time.</p> <pre><code># Get two commits to compare (oldest vs newest)\noldest_commit = history[-1]  # Oldest is last in newest-first list\nnewest_commit = history[0]  # Newest is first in newest-first list\n\n# Compare them (oldest first, then newest)\ncomparison = commit_demo_dataset.compare_commits(\n    oldest_commit.hash, newest_commit.hash\n)\n\nprint(\"\ud83d\udcca Commit Comparison:\")\nprint(\"=\" * 50)\ncommit1_info = comparison[\"commit1\"]\ncommit2_info = comparison[\"commit2\"]\nprint(f\"Commit 1: {commit1_info['hash'][:8]} - {commit1_info['message']}\")\nprint(f\"Commit 2: {commit2_info['hash'][:8]} - {commit2_info['message']}\")\n\n# File changes - compute manually by comparing file lists\nprint(\"\\n\ud83d\udcc1 File Changes:\")\nfiles1 = set(oldest_commit.list_files())\nfiles2 = set(newest_commit.list_files())\n\nadded_files = list(files2 - files1)\nremoved_files = list(files1 - files2)\nunchanged_files = list(files1 &amp; files2)\n\nif added_files:\n    print(f\"   Added: {added_files}\")\nif removed_files:\n    print(f\"   Removed: {removed_files}\")\nif unchanged_files:\n    print(f\"   Unchanged: {unchanged_files}\")\n\n# Metadata changes (if any)\nif comparison.get(\"metadata_diff\"):\n    metadata_diff = comparison[\"metadata_diff\"]\n    if metadata_diff.get(\"added\"):\n        print(f\"\\n\ud83d\udccb Metadata Added: {metadata_diff['added']}\")\n    if metadata_diff.get(\"changed\"):\n        print(f\"\ud83d\udccb Metadata Changed: {metadata_diff['changed']}\")\n\n# Tag changes (if any)\nif comparison.get(\"tags_diff\"):\n    tags_diff = comparison[\"tags_diff\"]\n    if tags_diff.get(\"added\"):\n        print(f\"\\n\ud83c\udff7\ufe0f  Tags Added: {tags_diff['added']}\")\n    if tags_diff.get(\"removed\"):\n        print(f\"\ud83c\udff7\ufe0f  Tags Removed: {tags_diff['removed']}\")\n</code></pre>"},{"location":"tutorials/commits/#step-7-understanding-file-changes","title":"Step 7: Understanding File Changes","text":"<p>Let's see how files change between commits by creating more commits and comparing them step by step.</p> <pre><code># Create commits with file changes\nfile3 = data_dir / \"data_v3.csv\"\nfile3.write_text(\"name,value\\nA,10\\nB,20\\nC,30\\nD,40\\n\")\n\ncommit_msg3 = f\"Add more rows - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\ncommit_demo_dataset.commit(message=commit_msg3, add_files=[str(file3)])\n\n# Remove a file\ncommit_msg4 = f\"Remove old file - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\ncommit_demo_dataset.commit(message=commit_msg4, remove_files=[\"data_v1.csv\"])\n\nprint(\"\u2705 Created additional commits with file changes\")\n</code></pre> <pre><code># Update history\nupdated_history = commit_demo_dataset.history()\n\n# Compare adjacent commits (going from newest to oldest)\nprint(\"\ud83d\udcca Comparing adjacent commits (newest \u2192 oldest):\")\nprint(\"=\" * 50)\n\nfor step_index in range(len(updated_history) - 1):\n    commit_a = updated_history[step_index]  # Newer commit\n    commit_b = updated_history[step_index + 1]  # Older commit\n\n    # Compute file differences manually\n    files_a = set(commit_a.list_files())\n    files_b = set(commit_b.list_files())\n\n    added_files_result = list(files_b - files_a)\n    removed_files_result = list(files_a - files_b)\n\n    print(f\"\\n{commit_a.short_hash} \u2192 {commit_b.short_hash}:\")\n    if added_files_result:\n        print(f\"   + Added: {added_files_result}\")\n    if removed_files_result:\n        print(f\"   - Removed: {removed_files_result}\")\n</code></pre>"},{"location":"tutorials/commits/#step-8-commit-metadata-and-tags","title":"Step 8: Commit Metadata and Tags","text":"<p>Commits can have metadata and tags for better organization. This is especially useful for tracking experiments, model versions, or data releases.</p> <pre><code># Create a commit with metadata and tags\nfile4 = data_dir / \"model_v1.pkl\"\nfile4.write_text(\"fake model data\")\n\ncommit_msg5 = f\"Add trained model - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\ncommit_demo_dataset.commit(\n    message=commit_msg5,\n    add_files=[str(file4)],\n    metadata={\n        \"accuracy\": 0.92,\n        \"framework\": \"sklearn\",\n        \"model_type\": \"RandomForest\",\n    },\n    tags=[\"model\", \"v1.0\", \"production\"],\n)\n\nprint(\"\u2705 Created commit with metadata and tags\")\n</code></pre> <pre><code># Access metadata and tags\nlatest_commit = commit_demo_dataset.current_commit\nif latest_commit:\n    print(f\"\ud83d\udcca Commit: {latest_commit.short_hash}\")\n    print(f\"   Metadata: {latest_commit.metadata}\")\n    print(f\"   Tags: {latest_commit.tags}\")\n</code></pre>"},{"location":"tutorials/commits/#step-9-finding-commits","title":"Step 9: Finding Commits","text":"<p>You can search for commits using various criteria: tags, metadata filters, or message content. This makes it easy to find specific commits in large datasets.</p> <pre><code># Find commits by tags\nproduction_commits = commit_demo_dataset.find_commits(tags=[\"production\"])\nprint(f\"\ud83c\udff7\ufe0f  Production commits: {len(production_commits)}\")\nfor prod_commit in production_commits:\n    print(f\"   {prod_commit.short_hash}: {prod_commit.message}\")\n</code></pre> <pre><code># Find commits by metadata filter\nhigh_accuracy_commits = commit_demo_dataset.find_commits(\n    metadata_filter=lambda m: m.get(\"accuracy\", 0) &gt; 0.9\n)\nprint(f\"\\n\ud83d\udcca High accuracy commits: {len(high_accuracy_commits)}\")\nfor acc_commit in high_accuracy_commits:\n    accuracy_value = acc_commit.metadata.get(\"accuracy\")\n    commit_info = f\"{acc_commit.short_hash}: {acc_commit.message}\"\n    print(f\"   {commit_info} (accuracy: {accuracy_value})\")\n</code></pre> <pre><code># Find commits by message content\ndef find_by_message(commit_demo_dataset, search_term):\n    history = commit_demo_dataset.history()\n    return [c for c in history if search_term.lower() in c.message.lower()]\n\nmodel_commits = find_by_message(commit_demo_dataset, \"model\")\nprint(f\"\\n\ud83d\udd0d Commits with 'model' in message: {len(model_commits)}\")\nfor msg_commit in model_commits:\n    print(f\"   {msg_commit.short_hash}: {msg_commit.message}\")\n</code></pre>"},{"location":"tutorials/commits/#step-10-commit-statistics","title":"Step 10: Commit Statistics","text":"<p>Let's analyze commit patterns to understand how your dataset has evolved over time.</p> <pre><code>def analyze_commits(commit_demo_dataset):\n    history = commit_demo_dataset.history()\n\n    if not history:\n        print(\"No commits found\")\n        return\n\n    print(\"\ud83d\udcca Commit Statistics:\")\n    print(\"=\" * 50)\n\n    # Basic stats\n    total_commits = len(history)\n    total_size = sum(c.get_total_size() for c in history)\n    avg_size = total_size / total_commits if total_commits &gt; 0 else 0\n\n    print(f\"Total commits: {total_commits}\")\n    print(f\"Total size: {total_size / (1024 * 1024):.2f} MB\")\n    print(f\"Average commit size: {avg_size / 1024:.2f} KB\")\n\n    # File frequency\n    file_counts = {}\n    for commit in history:\n        for filename in commit.list_files():\n            file_counts[filename] = file_counts.get(filename, 0) + 1\n\n    print(\"\\nMost frequently changed files:\")\n    sorted_files = sorted(file_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n    for filename, count in sorted_files:\n        print(f\"   {filename}: appears in {count} commits\")\n\n    # Time span\n    if len(history) &gt; 1:\n        first_date = history[0].timestamp\n        last_date = history[-1].timestamp\n        time_span = last_date - first_date\n        print(f\"\\nTime span: {time_span.days} days\")\n        print(f\"   First commit: {first_date.strftime('%Y-%m-%d %H:%M')}\")\n        print(f\"   Last commit: {last_date.strftime('%Y-%m-%d %H:%M')}\")\n\nanalyze_commits(commit_demo_dataset)\n</code></pre>"},{"location":"tutorials/commits/#step-11-working-with-commit-files","title":"Step 11: Working with Commit Files","text":"<p>You can access files from specific commits by checking out that commit and then using the standard file access methods.</p> <pre><code># Get files from a specific commit (using oldest for demonstration)\ntarget_commit = updated_history[-1]  # Oldest is last in newest-first list\ncommit_demo_dataset.checkout(target_commit.hash)\n\nprint(f\"\ud83d\udcc1 Files in commit {target_commit.short_hash}:\")\nwith dataset.local_files() as local_files:\n    for filename, local_path in local_files.items():\n        file_obj = dataset.get_file(filename)\n        print(f\"   {filename}:\")\n        print(f\"      Size: {file_obj.size} bytes\")\n        print(f\"      Hash: {file_obj.hash[:16]}...\")\n        print(f\"      Local path: {local_path}\")\n</code></pre>"},{"location":"tutorials/commits/#step-12-commit-workflows","title":"Step 12: Commit Workflows","text":"<p>Here are common commit workflow patterns that you can use in your data science projects.</p>"},{"location":"tutorials/commits/#pattern-1-linear-data-processing","title":"Pattern 1: Linear Data Processing","text":"<p>Sequential data processing pipeline where each step commits its output.</p> <pre><code># Sequential data processing pipeline\ncommit_demo_dataset.commit(\n    message=\"Add raw data\", add_files=[\"raw_data.csv\"]\n)\n# ... process data ...\ncommit_demo_dataset.commit(\n    message=\"Add cleaned data\", add_files=[\"cleaned_data.csv\"]\n)\n# ... analyze data ...\ncommit_demo_dataset.commit(\n    message=\"Add analysis results\", add_files=[\"results.csv\"]\n)\n</code></pre>"},{"location":"tutorials/commits/#pattern-2-experiment-tracking","title":"Pattern 2: Experiment Tracking","text":"<p>Track different experiments with metadata and tags for easy comparison.</p> <pre><code># Track different experiments\ncommit_demo_dataset.commit(\n    message=\"Experiment 1: Random Forest\",\n    add_files=[\"rf_model.pkl\", \"rf_results.csv\"],\n    metadata={\"model\": \"RandomForest\", \"accuracy\": 0.85},\n    tags=[\"experiment\", \"rf\"]\n)\n\ncommit_demo_dataset.commit(\n    message=\"Experiment 2: Gradient Boosting\",\n    add_files=[\"gb_model.pkl\", \"gb_results.csv\"],\n    metadata={\"model\": \"GradientBoosting\", \"accuracy\": 0.90},\n    tags=[\"experiment\", \"gb\"]\n)\n</code></pre>"},{"location":"tutorials/commits/#pattern-3-versioned-releases","title":"Pattern 3: Versioned Releases","text":"<p>Version your data releases with tags for easy reference.</p> <pre><code># Version your data releases\ncommit_demo_dataset.commit(\n    message=\"Release v1.0: Initial dataset\",\n    add_files=[\"dataset_v1.csv\"],\n    tags=[\"release\", \"v1.0\"]\n)\n\ncommit_demo_dataset.commit(\n    message=\"Release v1.1: Added features\",\n    add_files=[\"dataset_v1.1.csv\"],\n    tags=[\"release\", \"v1.1\"]\n)\n</code></pre>"},{"location":"tutorials/commits/#step-13-best-practices","title":"Step 13: Best Practices","text":"<p>Following best practices helps you maintain a clean, understandable commit history that makes it easy to track changes and understand your dataset's evolution.</p>"},{"location":"tutorials/commits/#write-clear-commit-messages","title":"Write Clear Commit Messages","text":"<p>Good commit messages are descriptive and specific. They explain what changed and why, making it easy to understand the dataset's history.</p> <pre><code># \u2705 Good: Descriptive and specific\ncommit_demo_dataset.commit(\n    message=\"Add Q1 2024 sales data with customer demographics\",\n    add_files=[\"sales_q1_2024.csv\"]\n)\n\n# \u2705 Good: Explains the change\ncommit_demo_dataset.commit(\n    message=\"Fix data quality issues: remove duplicates and handle missing values\",\n    add_files=[\"customers_cleaned.csv\"]\n)\n\n# \u274c Bad: Vague and unhelpful\ncommit_demo_dataset.commit(message=\"Update\", add_files=[\"data.csv\"])\ncommit_demo_dataset.commit(message=\"Fix\", add_files=[\"file.csv\"])\n</code></pre>"},{"location":"tutorials/commits/#make-atomic-commits","title":"Make Atomic Commits","text":"<p>Each commit should represent a single logical change. This makes it easier to understand what changed and to revert specific changes if needed.</p> <pre><code># \u2705 Good: Single logical change\ncommit_demo_dataset.commit(message=\"Add customer data\", add_files=[\"customers.csv\"])\n\n# \u2705 Good: Related changes together\ncommit_demo_dataset.commit(\n    message=\"Update customer data and add validation rules\",\n    add_files=[\"customers_updated.csv\", \"validation_rules.json\"]\n)\n\n# \u274c Bad: Unrelated changes\ncommit_demo_dataset.commit(\n    message=\"Add customer data and fix bug\",\n    add_files=[\"customers.csv\", \"bug_fix.py\"]\n)\n</code></pre>"},{"location":"tutorials/commits/#commit-regularly","title":"Commit Regularly","text":"<p>Commit after each logical step in your workflow. This creates a clear history of how your dataset evolved and makes it easier to track changes.</p> <pre><code># \u2705 Good: Commit after each logical step\ncommit_demo_dataset.commit(\n    message=\"Add raw data\", add_files=[\"raw_data.csv\"]\n)\n# ... process data ...\ncommit_demo_dataset.commit(\n    message=\"Add cleaned data\", add_files=[\"cleaned_data.csv\"]\n)\n\n# \u274c Bad: Too many changes in one commit\n# ... many processing steps ...\ncommit_demo_dataset.commit(\n    message=\"All changes\",\n    add_files=[\"file1.csv\", \"file2.csv\", \"file3.csv\", ...],\n)\n</code></pre>"},{"location":"tutorials/commits/#step-14-troubleshooting","title":"Step 14: Troubleshooting","text":"<p>Sometimes you need to find a specific commit or recover to a previous state. Here are some helpful techniques.</p>"},{"location":"tutorials/commits/#finding-lost-commits","title":"Finding Lost Commits","text":"<p>If you have a partial commit hash, you can search for the full commit.</p> <pre><code>def find_commit_by_hash(commit_demo_dataset, partial_hash):\n    history = commit_demo_dataset.history()\n    for commit in history:\n        if commit.hash.startswith(partial_hash):\n            return commit\n    return None\n\n# Find commit using a partial hash (using oldest for demonstration)\nfound_commit = None\npartial_hash = None\nif updated_history:\n    partial_hash = updated_history[-1].hash[:8]  # Oldest is last\n    found_commit = find_commit_by_hash(commit_demo_dataset, partial_hash)\n    if found_commit:\n        print(f\"\u2705 Found: {found_commit.short_hash} - {found_commit.message}\")\n</code></pre>"},{"location":"tutorials/commits/#recovering-from-mistakes","title":"Recovering from Mistakes","text":"<p>You can recover your dataset to a specific commit state by checking out that commit.</p> <pre><code>def recover_to_commit(commit_demo_dataset, commit_hash):\n    commit = commit_demo_dataset.get_commit(commit_hash)\n    if not commit:\n        print(f\"\u274c Commit {commit_hash} not found\")\n        return False\n\n    # Checkout the commit\n    commit_demo_dataset.checkout(commit_hash)\n\n    # Verify\n    if (\n        commit_demo_dataset.current_commit\n        and commit_demo_dataset.current_commit.hash == commit_hash\n    ):\n        print(f\"\u2705 Successfully recovered to commit {commit_hash[:8]}\")\n        print(f\"   Message: {commit_demo_dataset.current_commit.message}\")\n        return True\n    else:\n        print(\"\u274c Failed to recover\")\n        return False\n\n# Recover to a known good commit (using oldest for demonstration)\nrecover_to_commit(dataset, updated_history[-1].hash)\n</code></pre>"},{"location":"tutorials/commits/#summary","title":"Summary","text":"<p>Congratulations! You've learned how to work with commits in Kirin:</p> <ul> <li>\u2705 Understanding commit structure - Hash, message, timestamp,   parent, files</li> <li>\u2705 Navigating history - Viewing, limiting, and finding commits</li> <li>\u2705 Checking out commits - Switching between different commit   states</li> <li>\u2705 Comparing commits - Seeing what changed between commits</li> <li>\u2705 Using metadata and tags - Organizing commits with additional   information</li> <li>\u2705 Finding commits - Searching by tags, metadata, or message</li> <li>\u2705 Best practices - Writing good commit messages and workflows</li> </ul>"},{"location":"tutorials/commits/#key-concepts","title":"Key Concepts","text":"<ul> <li>Linear History: Each commit has one parent, creating a simple   chain</li> <li>Immutable Snapshots: Commits are immutable - they never change</li> <li>Content-Addressed Files: Files are stored by content hash, not   filename</li> <li>Checkout: Switching the dataset to a specific commit's state</li> </ul>"},{"location":"tutorials/commits/#next-steps","title":"Next Steps","text":"<ul> <li>Cloud Storage Overview - Learn about using   cloud storage backends</li> <li>Web UI Basics - Use the web interface to   browse commits</li> <li>Track Model Training Data - See   commits in action with ML workflows</li> </ul>"},{"location":"tutorials/first-dataset/","title":"First Dataset","text":"<p>To run this notebook, click on the molab shield above or run the following command at the terminal:</p> <pre><code>uvx marimo edit --sandbox --mcp --no-token --watch https://github.com/nll-ai/kirin/blob/main/docs/tutorials/first-dataset.py\n</code></pre> <pre><code>import marimo as mo\n</code></pre>"},{"location":"tutorials/first-dataset/#your-first-dataset","title":"Your First Dataset","text":"<p>This tutorial will guide you through creating and working with your first Kirin dataset. By the end, you'll understand the core concepts of datasets, commits, and how to work with versioned files.</p>"},{"location":"tutorials/first-dataset/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>How to create a dataset</li> <li>How to add files to a dataset</li> <li>How to view commit history</li> <li>How to access files from different commits</li> <li>How to update your dataset with new files</li> </ul>"},{"location":"tutorials/first-dataset/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.13 or higher</li> <li>Kirin installed (see Installation Guide)</li> </ul>"},{"location":"tutorials/first-dataset/#step-1-understanding-datasets","title":"Step 1: Understanding Datasets","text":"<p>A dataset in Kirin is a collection of versioned files. Think of it like a Git repository, but specifically designed for data files. Each dataset has:</p> <ul> <li>A name that identifies it</li> <li>A linear commit history that tracks changes over time</li> <li>Files that are stored using content-addressed storage</li> </ul> <pre><code>import tempfile\nfrom pathlib import Path\n\nfrom kirin import Catalog\n\n# Create a temporary directory for our tutorial\n# In production, you might use: Catalog(root_dir=\"s3://my-bucket/data\")\ntemp_dir = Path(tempfile.mkdtemp(prefix=\"kirin_tutorial_\"))\ncatalog = Catalog(root_dir=temp_dir)\n\n# Create a new dataset\nmy_dataset = catalog.create_dataset(\n    \"my_first_dataset\", description=\"My first Kirin dataset for learning\"\n)\nmy_dataset\n</code></pre> <p>Tip: When you display a dataset in a notebook cell (like <code>my_dataset</code> above), Kirin shows an interactive HTML view with a \"Copy Code to Access\" button for each file. The copied code uses \"dataset\" by default, but you can customize it by setting <code>my_dataset._repr_variable_name = \"my_dataset\"</code>.</p>"},{"location":"tutorials/first-dataset/#step-2-preparing-your-first-files","title":"Step 2: Preparing Your First Files","text":"<p>Before we can commit files, let's create some sample data files to work with.</p> <pre><code># Create a directory for our data files\ndata_dir = temp_dir / \"sample_data\"\ndata_dir.mkdir(exist_ok=True)\n\n# Create a simple CSV file\ncsv_file = data_dir / \"data.csv\"\ncsv_file.write_text(\"\"\"name,age,city\nAlice,28,New York\nBob,35,San Francisco\nCarol,42,Chicago\n\"\"\")\n\n# Create a JSON configuration file\nconfig_file = data_dir / \"config.json\"\nconfig_file.write_text(\"\"\"{\n\"version\": \"1.0\",\n\"description\": \"Sample dataset configuration\",\n\"columns\": [\"name\", \"age\", \"city\"]\n}\n\"\"\")\n\nprint(\"\u2705 Created files:\")\nprint(f\"   - {csv_file.name}\")\nprint(f\"   - {config_file.name}\")\n</code></pre>"},{"location":"tutorials/first-dataset/#step-3-making-your-first-commit","title":"Step 3: Making Your First Commit","text":"<p>Now let's add these files to our dataset. This creates your first commit.</p> <pre><code># Commit files to the dataset\nmy_dataset.commit(\n    message=\"Initial commit: Add sample data and configuration\",\n    add_files=[str(csv_file), str(config_file)],\n)\n\n# Display the current commit with rich HTML\nmy_dataset\n</code></pre> <p>What just happened?</p> <ul> <li>Kirin calculated content hashes for each file</li> <li>Files were stored in content-addressed storage</li> <li>A commit was created that references these files</li> <li>The commit was added to the dataset's linear history</li> </ul>"},{"location":"tutorials/first-dataset/#step-4-viewing-your-commit-history","title":"Step 4: Viewing Your Commit History","text":"<p>Let's see what we've created. You should see your first commit listed with the files you added.</p> <pre><code># Display the dataset which shows files in the current commit\nmy_dataset\n</code></pre>"},{"location":"tutorials/first-dataset/#step-5-accessing-files-from-a-commit","title":"Step 5: Accessing Files from a Commit","text":"<p>Now let's access the files from the current commit. The recommended way to work with files is using the <code>local_files()</code> context manager. This downloads files on-demand and cleans them up automatically.</p> <p>Key points:</p> <ul> <li>Files are only downloaded when you access them (lazy loading)</li> <li>Files are automatically cleaned up when you exit the context manager</li> <li>You can use standard Python libraries (pandas, polars, etc.) with the   local paths</li> </ul> <pre><code># Access files as local paths\nwith my_dataset.local_files() as local_files:\n    # Files are lazily downloaded when accessed\n    csv_path = local_files[\"data.csv\"]\n    config_path = local_files[\"config.json\"]\n\n    # Now you can use standard Python file operations\n    print(\"\ud83d\udcc2 Local file paths:\")\n    print(f\"   CSV: {csv_path}\")\n    print(f\"   Config: {config_path}\")\n\n    # Read file content\n    csv_content = Path(csv_path).read_text()\n    print(\"\\n\ud83d\udcdd CSV content:\")\n    print(csv_content)\n\n    # Or use with data science libraries\n    import pandas as pd\n\n    df = pd.read_csv(csv_path)\n    print(\"\\n\ud83d\udcca DataFrame:\")\n    print(f\"   Shape: {df.shape}\")\n    print(df)\n</code></pre>"},{"location":"tutorials/first-dataset/#step-6-adding-more-files","title":"Step 6: Adding More Files","text":"<p>Let's add another file to see how the commit history grows.</p> <pre><code>from datetime import datetime\n\n# Create a new file\nresults_file = data_dir / \"results.txt\"\nresults_file.write_text(\"\"\"Analysis Results\n================\nTotal records: 3\nAverage age: 35.0\nCities: New York, San Francisco, Chicago\n\"\"\")\n\n# Commit the new file\ncommit_msg = (\n    f\"Add analysis results - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n)\nmy_dataset.commit(\n    message=commit_msg,\n    add_files=[str(results_file)],\n)\n\n# Display the current commit with rich HTML\nmy_dataset.current_commit\n</code></pre>"},{"location":"tutorials/first-dataset/#step-7-viewing-the-updated-history","title":"Step 7: Viewing the Updated History","text":"<p>Let's see how the history has changed. Notice how the commit history is linear - each commit builds on the previous one.</p> <pre><code># Display the dataset which shows updated commit history\nupdated_history = my_dataset.history()\nmy_dataset\n</code></pre>"},{"location":"tutorials/first-dataset/#step-8-checking-out-different-commits","title":"Step 8: Checking Out Different Commits","text":"<p>You can checkout any commit to see what files were available at that point.</p> <pre><code># Get the first commit\nfirst_commit = updated_history[-1]  # Oldest commit is last in history\n\n# Checkout the first commit and display it\nmy_dataset.checkout(first_commit.hash)\nfirst_commit\n\n# Checkout the latest commit and display the dataset\nmy_dataset.checkout()  # No argument = latest commit\nmy_dataset\n</code></pre>"},{"location":"tutorials/first-dataset/#step-9-understanding-content-addressed-storage","title":"Step 9: Understanding Content-Addressed Storage","text":"<p>One of Kirin's key features is content-addressed storage. This means:</p> <ul> <li>Files are stored by their content hash, not by filename</li> <li>Identical files are automatically deduplicated</li> <li>File integrity is guaranteed by the hash</li> </ul> <p>Let's demonstrate this by creating a duplicate file with the same content. Even though the files have different names, they have the same content hash, so Kirin stores them only once!</p> <pre><code># Create a file with the same content as data.csv\nduplicate_file = data_dir / \"data_copy.csv\"\nduplicate_file.write_text(csv_file.read_text())\n\n# Commit the duplicate\nmy_dataset.commit(\n    message=\"Add duplicate file\", add_files=[str(duplicate_file)]\n)\n\n# Check the file objects\noriginal = my_dataset.get_file(\"data.csv\")\nduplicate = my_dataset.get_file(\"data_copy.csv\")\n\nprint(\"\ud83d\udd0d Content-Addressed Storage Demo:\")\nprint(f\"   Original file hash: {original.hash}\")\nprint(f\"   Duplicate file hash: {duplicate.hash}\")\nprint(f\"   Same content = Same hash: {original.hash == duplicate.hash}\")\n</code></pre>"},{"location":"tutorials/first-dataset/#step-10-removing-files","title":"Step 10: Removing Files","text":"<p>You can also remove files from a dataset.</p> <pre><code># Remove a file\nmy_dataset.commit(\n    message=\"Remove duplicate file\", remove_files=[\"data_copy.csv\"]\n)\n\n# Display the dataset to see updated state\nmy_dataset\n</code></pre>"},{"location":"tutorials/first-dataset/#step-11-combining-operations","title":"Step 11: Combining Operations","text":"<p>You can add and remove files in the same commit.</p> <pre><code># Create a summary report file\nsummary_report = data_dir / \"monthly_summary.json\"\nsummary_report.write_text(\"\"\"{\n\"period\": \"2024-01\",\n\"total_records\": 3,\n\"average_age\": 35.0,\n\"cities\": [\"New York\", \"San Francisco\", \"Chicago\"],\n\"generated_at\": \"2024-01-15T10:00:00Z\"\n}\n\"\"\")\n\n# Add summary report and remove detailed processing log\n# These are different types of files: summary vs detailed logs\nmy_dataset.commit(\n    message=\"Add monthly summary, remove detailed processing logs\",\n    add_files=[str(summary_report)],\n    remove_files=[\"results.txt\"],\n)\n\n# Display the dataset to see updated state\nmy_dataset\n</code></pre>"},{"location":"tutorials/first-dataset/#summary","title":"Summary","text":"<p>Congratulations! You've learned the fundamentals of working with Kirin datasets:</p> <ul> <li>\u2705 Created a dataset using a catalog</li> <li>\u2705 Made commits to track file changes</li> <li>\u2705 Viewed commit history to see how your dataset evolved</li> <li>\u2705 Accessed files from different commits</li> <li>\u2705 Worked with files locally using the context manager</li> <li>\u2705 Understood content-addressed storage and deduplication</li> <li>\u2705 Updated datasets by adding and removing files</li> </ul>"},{"location":"tutorials/first-dataset/#key-concepts","title":"Key Concepts","text":"<ul> <li>Dataset: A collection of versioned files with linear commit history</li> <li>Commit: A snapshot of files at a point in time</li> <li>Content-addressed storage: Files stored by content hash for   integrity and deduplication</li> <li>Linear history: Simple, sequential commits without branching   complexity</li> </ul>"},{"location":"tutorials/first-dataset/#next-steps","title":"Next Steps","text":"<ul> <li>Working with Commits - Deep dive into commit   operations and history</li> <li>Cloud Storage Overview - Learn about using   cloud storage backends</li> <li>Track Model Training Data - See   a real-world example with ML models</li> </ul>"},{"location":"tutorials/ml-workflow-script/","title":"Ml workflow script","text":"<p>title: Ml Workflow Script marimo-version: 0.19.7 header: \"# /// script\\n# requires-python = \\\"&gt;=3.13\\\"\\n# dependencies = [\\n#     \\\"\\   kirin\\\",\\n#     \\\"scikit-learn\\\",\\n#     \\\"matplotlib\\\",\\n#     \\\"seaborn\\\",\\n#\\   \\     \\\"numpy\\\",\\n#     \\\"joblib\\\",\\n# ]\\n#\\n# [tool.uv.sources]\\n# kirin = { path\\   \\ = \\\"../../\\\", editable = true }\\n# ///\\n\\n\\\"\\\"\\\"ML workflow script for Kirin tutorial.\\n\\   \\nThis script trains models, creates plots, and commits them to a Kirin dataset.\\n\\   It accepts a directory path where the dataset should be stored.\\n\\\"\\\"\\\"\\n\\nimport\\   \\ argparse\\nfrom pathlib import Path\\n\\nimport matplotlib.pyplot as plt\\nimport\\   \\ numpy as np\\nimport seaborn as sns\\nfrom sklearn.datasets import load_iris\\nfrom\\   \\ sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import accuracy_score,\\   \\ classification_report, confusion_matrix\\nfrom sklearn.model_selection import train_test_split\\n\\   \\nfrom kirin import Dataset\\n\\n# Set style for plots\\nsns.set_style(\\\"whitegrid\\\"\\   )\\nplt.rcParams[\\\"figure.figsize\\\"] = (10, 6)\\n\\n\\ndef train_initial_model(dataset_dir:\\   \\ Path):\\n    \\\"\\\"\\\"Train initial model and commit to dataset.\\\"\\\"\\\"\\n    # Load\\   \\ data\\n    iris_data = load_iris()\\n    X = iris_data.data\\n    y = iris_data.target\\n\\   \\    feature_names = iris_data.feature_names\\n    target_names = iris_data.target_names\\n\\   \\n    # Split data\\n    X_train, X_test, y_train, y_test = train_test_split(\\n \\   \\       X, y, test_size=0.25, random_state=42, stratify=y\\n    )\\n\\n    # Train\\   \\ initial model\\n    initial_model = RandomForestClassifier(\\n        n_estimators=3,\\n\\   \\        max_depth=1,\\n        random_state=1,\\n    )\\n    initial_model.fit(X_train,\\   \\ y_train)\\n\\n    # Evaluate\\n    y_pred = initial_model.predict(X_test)\\n    accuracy\\   \\ = accuracy_score(y_test, y_pred)\\n\\n    # Create plots\\n    # Plot 1: Confusion\\   \\ Matrix\\n    cm = confusion_matrix(y_test, y_pred)\\n    confusion_matrix_fig, ax\\   \\ = plt.subplots(figsize=(8, 6))\\n    sns.heatmap(\\n        cm,\\n        annot=True,\\n\\   \\        fmt=\\\"d\\\",\\n        cmap=\\\"Blues\\\",\\n        xticklabels=target_names,\\n\\   \\        yticklabels=target_names,\\n        ax=ax,\\n    )\\n    ax.set_title(\\\"Confusion\\   \\ Matrix - Initial Model\\\")\\n    ax.set_ylabel(\\\"True Label\\\")\\n    ax.set_xlabel(\\\"\\   Predicted Label\\\")\\n\\n    # Plot 2: Feature Importance\\n    feature_importance =\\   \\ initial_model.feature_importances_\\n    feature_importance_fig, ax = plt.subplots(figsize=(10,\\   \\ 6))\\n    indices = np.argsort(feature_importance)[::-1]\\n    ax.bar(range(len(feature_importance)),\\   \\ feature_importance[indices])\\n    ax.set_xticks(range(len(feature_importance)))\\n\\   \\    ax.set_xticklabels([feature_names[i] for i in indices], rotation=45, ha=\\\"\\   right\\\")\\n    ax.set_title(\\\"Feature Importance - Initial Model\\\")\\n    ax.set_xlabel(\\\"\\   Features\\\")\\n    ax.set_ylabel(\\\"Importance\\\")\\n\\n    # Classification Report\\n\\   \\    report = classification_report(y_test, y_pred, target_names=target_names)\\n\\   \\    reports_dir = dataset_dir / \\\"reports\\\"\\n    reports_dir.mkdir(exist_ok=True)\\n\\   \\    report_path = reports_dir / \\\"classification_report.txt\\\"\\n    report_path.write_text(report)\\n\\   \\n    # Commit to dataset\\n    dataset = Dataset(root_dir=dataset_dir, name=\\\"iris_classifier\\\"\\   )\\n    commit_hash = dataset.commit(\\n        message=\\\"Initial model v1.0 - Simple\\   \\ Random Forest baseline (3 trees, depth 1)\\\",\\n        add_files=[\\n          \\   \\  initial_model,  # Model object - auto-serialized\\n            confusion_matrix_fig,\\   \\  # Plot object - auto-converted to SVG\\n            feature_importance_fig,  #\\   \\ Plot object - auto-converted to SVG\\n            str(report_path),  # Text file\\n\\   \\        ],\\n        metadata={\\n            \\\"accuracy\\\": accuracy,\\n         \\   \\   \\\"model_version\\\": \\\"1.0.0\\\",\\n        },\\n    )\\n\\n    plt.close(confusion_matrix_fig)\\n\\   \\    plt.close(feature_importance_fig)\\n\\n    print(f\\\"\\u2705 Initial model committed:\\   \\ {commit_hash[:8]}\\\")\\n    print(f\\\"   Accuracy: {accuracy:.4f}\\\")\\n    return\\   \\ commit_hash, accuracy\\n\\n\\ndef train_improved_model(dataset_dir: Path):\\n    \\\"\\   \\\"\\\"Train improved model and commit to dataset.\\\"\\\"\\\"\\n    # Load data\\n    iris_data\\   \\ = load_iris()\\n    X = iris_data.data\\n    y = iris_data.target\\n    feature_names\\   \\ = iris_data.feature_names\\n    target_names = iris_data.target_names\\n\\n    #\\   \\ Split data\\n    X_train, X_test, y_train, y_test = train_test_split(\\n       \\   \\ X, y, test_size=0.25, random_state=42, stratify=y\\n    )\\n\\n    # Train improved\\   \\ model\\n    improved_model = RandomForestClassifier(\\n        n_estimators=100,\\n\\   \\        max_depth=None,\\n        min_samples_split=2,\\n        min_samples_leaf=1,\\n\\   \\        max_features=\\\"sqrt\\\",\\n        random_state=42,\\n    )\\n    improved_model.fit(X_train,\\   \\ y_train)\\n\\n    # Evaluate\\n    y_pred_improved = improved_model.predict(X_test)\\n\\   \\    improved_accuracy = accuracy_score(y_test, y_pred_improved)\\n\\n    # Create\\   \\ plots\\n    # Plot 1: Confusion Matrix\\n    cm_improved = confusion_matrix(y_test,\\   \\ y_pred_improved)\\n    confusion_matrix_fig, ax = plt.subplots(figsize=(8, 6))\\n\\   \\    sns.heatmap(\\n        cm_improved,\\n        annot=True,\\n        fmt=\\\"d\\\"\\   ,\\n        cmap=\\\"Greens\\\",\\n        xticklabels=target_names,\\n        yticklabels=target_names,\\n\\   \\        ax=ax,\\n    )\\n    ax.set_title(\\\"Confusion Matrix - Improved Model\\\")\\n\\   \\    ax.set_ylabel(\\\"True Label\\\")\\n    ax.set_xlabel(\\\"Predicted Label\\\")\\n\\n \\   \\   # Plot 2: Feature Importance\\n    feature_importance_improved = improved_model.feature_importances_\\n\\   \\    feature_importance_fig, ax = plt.subplots(figsize=(10, 6))\\n    indices_improved\\   \\ = np.argsort(feature_importance_improved)[::-1]\\n    ax.bar(\\n        range(len(feature_importance_improved)),\\n\\   \\        feature_importance_improved[indices_improved],\\n    )\\n    ax.set_xticks(range(len(feature_importance_improved)))\\n\\   \\    ax.set_xticklabels(\\n        [feature_names[i] for i in indices_improved],\\   \\ rotation=45, ha=\\\"right\\\"\\n    )\\n    ax.set_title(\\\"Feature Importance - Improved\\   \\ Model\\\")\\n    ax.set_xlabel(\\\"Features\\\")\\n    ax.set_ylabel(\\\"Importance\\\")\\n\\   \\n    # Classification Report\\n    report_improved = classification_report(\\n  \\   \\      y_test, y_pred_improved, target_names=target_names\\n    )\\n    reports_dir\\   \\ = dataset_dir / \\\"reports\\\"\\n    reports_dir.mkdir(exist_ok=True)\\n    report_improved_path\\   \\ = reports_dir / \\\"classification_report.txt\\\"\\n    report_improved_path.write_text(report_improved)\\n\\   \\n    # Commit to dataset\\n    dataset = Dataset(root_dir=dataset_dir, name=\\\"iris_classifier\\\"\\   )\\n    commit_hash = dataset.commit(\\n        message=(\\n            \\\"Improved\\   \\ model v2.0 - Better hyperparameters (100 trees, unlimited depth)\\\"\\n        ),\\n\\   \\        add_files=[\\n            improved_model,  # Model object - auto-serialized\\n\\   \\            confusion_matrix_fig,  # Plot object - auto-converted to SVG\\n    \\   \\        feature_importance_fig,  # Plot object - auto-converted to SVG\\n      \\   \\      str(report_improved_path),  # Text file\\n        ],\\n        metadata={\\n\\   \\            \\\"accuracy\\\": improved_accuracy,\\n            \\\"model_version\\\": \\\"\\   2.0.0\\\",\\n        },\\n    )\\n\\n    plt.close(confusion_matrix_fig)\\n    plt.close(feature_importance_fig)\\n\\   \\n    print(f\\\"\\u2705 Improved model committed: {commit_hash[:8]}\\\")\\n    print(f\\\"\\   \\   Accuracy: {improved_accuracy:.4f}\\\")\\n    return commit_hash, improved_accuracy\\n\\   \\n\\ndef main():\\n    parser = argparse.ArgumentParser(description=\\\"ML workflow\\   \\ script for Kirin\\\")\\n    parser.add_argument(\\n        \\\"dataset_dir\\\",\\n    \\   \\    type=Path,\\n        help=\\\"Directory where the dataset should be stored\\\",\\n\\   \\    )\\n    parser.add_argument(\\n        \\\"--step\\\",\\n        choices=[\\\"initial\\\"\\   , \\\"improved\\\", \\\"both\\\"],\\n        default=\\\"both\\\",\\n        help=\\\"Which step\\   \\ to run (default: both)\\\",\\n    )\\n    args = parser.parse_args()\\n\\n    dataset_dir\\   \\ = args.dataset_dir\\n    dataset_dir.mkdir(parents=True, exist_ok=True)\\n\\n   \\   \\ if args.step in (\\\"initial\\\", \\\"both\\\"):\\n        train_initial_model(dataset_dir)\\n\\   \\n    if args.step in (\\\"improved\\\", \\\"both\\\"):\\n        train_improved_model(dataset_dir)\\n\\   \\n    # Print summary\\n    if args.step == \\\"both\\\":\\n        dataset = Dataset(root_dir=dataset_dir,\\   \\ name=\\\"iris_classifier\\\")\\n        history = dataset.history()\\n        print(f\\\"\\   \\n\\U0001F4CA Commit History ({len(history)} commits):\\\")\\n        for i, commit\\   \\ in enumerate(history, 1):\\n            print(f\\\"{i}. {commit.message}\\\")\\n   \\   \\         if commit.metadata and \\\"accuracy\\\" in commit.metadata:\\n            \\   \\    print(f\\\"   Accuracy: {commit.metadata['accuracy']:.4f}\\\")\\n\\n\\nif name\\   \\ == \\\"main\\\":\\n    main()\" ---</p> <p>To run this notebook, click on the molab shield above or run the following command at the terminal:</p> <pre><code>uvx marimo edit --sandbox --mcp --no-token --watch https://github.com/nll-ai/kirin/blob/main/docs/tutorials/ml-workflow-script.py\n</code></pre>"},{"location":"tutorials/ml-workflow/","title":"Ml Workflow","text":"<p>To run this notebook, click on the molab shield above or run the following command at the terminal:</p> <pre><code>uvx marimo edit --sandbox --mcp --no-token --watch https://github.com/nll-ai/kirin/blob/main/docs/tutorials/ml-workflow.py\n</code></pre> <pre><code>import marimo as mo\n</code></pre>"},{"location":"tutorials/ml-workflow/#machine-learning-workflow-with-kirin","title":"Machine Learning Workflow with Kirin","text":"<p>This tutorial will guide you through a complete machine learning workflow using Kirin for version control. You'll learn how to train a model, create diagnostic plots, archive everything together, and track different model versions over time.</p>"},{"location":"tutorials/ml-workflow/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>How to train a machine learning model with scikit-learn</li> <li>How to create diagnostic plots for model evaluation</li> <li>How to archive model artifacts, metrics, and plots together in Kirin</li> <li>How to version control your ML workflow with commits</li> <li>How to track improvements across different model versions</li> </ul>"},{"location":"tutorials/ml-workflow/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.13 or higher</li> <li>Kirin installed (see Installation Guide)</li> <li>Basic familiarity with machine learning concepts</li> </ul>"},{"location":"tutorials/ml-workflow/#understanding-ml-workflow-versioning","title":"Understanding ML Workflow Versioning","text":"<p>When working with machine learning models, you need to track multiple artifacts together:</p> <ul> <li>Model artifacts: The trained model files (weights, parameters)</li> <li>Metrics: Performance measurements (accuracy, precision, recall)</li> <li>Diagnostic plots: Visualizations that help understand model behavior</li> <li>Metadata: Hyperparameters, training configuration, dataset info</li> </ul> <p>Kirin lets you commit all of these together, creating a complete snapshot of your model at a point in time. When you improve your model, you create a new commit with updated artifacts, showing the evolution of your work.</p> <pre><code>import subprocess\nimport tempfile\nfrom pathlib import Path\n\nfrom kirin import Dataset\n\n# Create temporary directory for our workflow\ntemp_dir = Path(tempfile.mkdtemp(prefix=\"kirin_ml_workflow_\"))\nprint(f\"Created dataset directory: {temp_dir}\")\n</code></pre>"},{"location":"tutorials/ml-workflow/#step-1-train-and-commit-initial-model","title":"Step 1: Train and Commit Initial Model","text":"<p>We'll use the classic Iris dataset from scikit-learn, which contains measurements of iris flowers for three different species. This is a classification problem where we predict the species based on flower measurements.</p> <p>We'll train an initial Random Forest model with simple hyperparameters to establish a baseline, then commit it along with diagnostic plots.</p> <pre><code># Get the script path (same directory as this notebook)\nscript_path = Path(__file__).parent / \"ml-workflow-script.py\"\n</code></pre> <pre><code># Run the script to train initial model\n_result = subprocess.run(\n    [\"uv\", \"run\", str(script_path), str(temp_dir), \"--step\", \"initial\"],\n    capture_output=True,\n    text=True,\n)\n\nprint(_result.stdout)\nif _result.stderr:\n    print(\"STDERR:\", _result.stderr)\nif _result.returncode != 0:\n    print(f\"Error: Script exited with code {_result.returncode}\")\n</code></pre> <p>What just happened?</p> <p>We've created our first model version commit! All the artifacts are now stored together in Kirin:</p> <ul> <li>The trained model was automatically serialized (no manual joblib.dump needed!)</li> <li>Hyperparameters were automatically extracted from the model</li> <li>Metrics were automatically extracted (feature_importances_, etc.)</li> <li>The plots were automatically converted to SVG files</li> <li>Source file linking connects the model to the script that created it</li> <li>Everything is versioned together, so you can always see what went into   this model version</li> </ul> <p>Notice how much simpler this is compared to the traditional workflow where you'd need to manually: save the model with <code>joblib.dump()</code>, extract hyperparameters with <code>model.get_params()</code>, save plots with <code>plt.savefig()</code>, and manually track all these files. Here, we just passed the model and plot objects directly, and Kirin handled serialization, metadata extraction, and file conversion automatically!</p> <p>Now let's see how to create an improved version.</p>"},{"location":"tutorials/ml-workflow/#step-2-train-and-commit-improved-model","title":"Step 2: Train and Commit Improved Model","text":"<p>Now let's train an improved version with better hyperparameters. We'll increase the number of trees, allow deeper trees, and tune other parameters to improve performance.</p> <pre><code># Run the script to train improved model\n_result = subprocess.run(\n    [\"uv\", \"run\", str(script_path), str(temp_dir), \"--step\", \"improved\"],\n    capture_output=True,\n    text=True,\n)\n\nprint(_result.stdout)\nif _result.stderr:\n    print(\"STDERR:\", _result.stderr)\nif _result.returncode != 0:\n    print(f\"Error: Script exited with code {_result.returncode}\")\n</code></pre> <p>What just happened?</p> <p>We've successfully created two model versions in Kirin:</p> <ol> <li>Initial Model (v1.0): Random Forest with 3 trees, max depth 1</li> <li>Improved Model (v2.0): Random Forest with 100 trees, unlimited depth</li> </ol> <p>Each commit contains: - The complete model artifact (auto-serialized) - Hyperparameters (auto-extracted via get_params()) - Metrics (auto-extracted: feature_importances_, n_features_in_, etc.) - All diagnostic plots (auto-converted to SVG) - Source file linking (connects model to script)</p> <p>This workflow is much simpler than manually managing files: instead of saving models with <code>joblib.dump()</code>, extracting metadata manually, and saving plots with <code>plt.savefig()</code>, we just passed the model and plot objects directly. Kirin handled all the serialization, metadata extraction, and file conversion automatically!</p> <p>The commit history shows the linear progression of your model development, making it easy to track improvements and understand what changed between versions.</p>"},{"location":"tutorials/ml-workflow/#step-3-view-the-commit-history","title":"Step 3: View the Commit History","text":"<p>Let's look at the commit history to see how our model has evolved. This shows the linear progression of our model versions.</p> <pre><code># Load the dataset and view commit history\ndataset = Dataset(root_dir=temp_dir, name=\"iris_classifier\")\nhistory = dataset.history()\n\nprint(\"\u2705 Commit history retrieved\")\nprint(f\"\\n\ud83d\udcca Model Version History ({len(history)} commits):\\n\")\nfor i, commit in enumerate(history, 1):\n    print(f\"{i}. {commit.message}\")\n    print(f\"   Hash: {commit.hash[:8]}\")\n    if commit.metadata and \"accuracy\" in commit.metadata:\n        print(f\"   Accuracy: {commit.metadata['accuracy']:.4f}\")\n    print()\n</code></pre> <pre><code># Display the dataset\ndataset\n</code></pre>"},{"location":"tutorials/ml-workflow/#step-4-access-files-from-different-versions","title":"Step 4: Access Files from Different Versions","text":"<p>One of the powerful features of Kirin is the ability to access files from any commit in the history. We'll use <code>checkout()</code> to switch to a specific commit, then use the <code>local_files()</code> context manager to access files as local paths.</p> <pre><code>import joblib\n\n# Get the first commit (initial model)\ninitial_commit = history[-1]  # History is in reverse chronological order\n\n# Checkout the initial commit\ndataset.checkout(initial_commit.hash)\n\n# Access files as local paths using the context manager\nwith dataset.local_files() as local_files:\n    # Find the model file\n    model_files = [f for f in local_files.keys() if f.endswith(\".pkl\")]\n    if model_files:\n        initial_model_path = local_files[model_files[0]]\n        initial_model_loaded = joblib.load(initial_model_path)\n\n        print(\"\u2705 Model loaded from initial commit\")\n        print(f\"   Commit: {initial_commit.hash[:8]}\")\n        print(f\"   Message: {initial_commit.message}\")\n        print(f\"   Model type: {type(initial_model_loaded).__name__}\")\n        print(f\"   Number of trees: {initial_model_loaded.n_estimators}\")\n</code></pre>"},{"location":"tutorials/ml-workflow/#summary","title":"Summary","text":"<p>In this tutorial, you've learned how to:</p> <ul> <li>Train machine learning models using scikit-learn</li> <li>Create diagnostic plots to visualize model performance</li> <li>Commit model objects directly - Kirin automatically handles   serialization, hyperparameter extraction, and metrics extraction</li> <li>Commit plot objects directly - Kirin automatically converts them to   SVG files with format auto-detection</li> <li>Archive complete model snapshots in Kirin, including:</li> <li>Model artifacts (auto-serialized)</li> <li>Hyperparameters (auto-extracted)</li> <li>Metrics (auto-extracted)</li> <li>Diagnostic plots (auto-converted to SVG)</li> <li>Source file linking</li> <li>Version control your ML workflow by committing different model   versions</li> <li>Track improvements across model versions using commit history</li> <li>Access historical versions of your models and artifacts</li> </ul> <p>Key Takeaways:</p> <ul> <li>Kirin lets you commit model objects directly - no manual serialization   needed!</li> <li>Kirin lets you commit plot objects directly - no manual file saving needed!</li> <li>Hyperparameters and metrics are automatically extracted from scikit-learn   models</li> <li>Plots are automatically converted to SVG with format auto-detection</li> <li>Source file linking connects models to the scripts that created them</li> <li>The linear commit history makes it easy to track model evolution</li> <li>You can always access any version of your models and artifacts from the   commit history</li> </ul> <p>Next Steps:</p> <ul> <li>Try this workflow with your own models and datasets</li> <li>Explore the Model Versioning Guide for   more advanced features</li> <li>Check out the How-To Guide for   additional ML workflow patterns</li> </ul>"},{"location":"web-ui/catalog-management/","title":"Catalog Management","text":"<p>Configuration and management of data catalogs in the web UI.</p>"},{"location":"web-ui/catalog-management/#catalog-configuration","title":"Catalog Configuration","text":""},{"location":"web-ui/catalog-management/#local-storage-catalogs","title":"Local Storage Catalogs","text":"<p>For local filesystem storage:</p> <p>Benefits:</p> <ul> <li>Simple setup: No authentication required</li> <li>Fast access: Direct filesystem access</li> <li>Easy backup: Standard file operations</li> <li>No costs: No cloud storage fees</li> </ul>"},{"location":"web-ui/catalog-management/#cloud-storage-catalogs","title":"Cloud Storage Catalogs","text":""},{"location":"web-ui/catalog-management/#aws-s3-configuration","title":"AWS S3 Configuration","text":"<p>Setup Steps:</p> <ol> <li>Configure AWS credentials in your environment</li> <li>Create S3 bucket with appropriate permissions</li> <li>Set up IAM policies for bucket access</li> <li>Test connection in the web UI</li> </ol> <p>Web UI Configuration:</p> <ul> <li>Catalog Name: Friendly name for the catalog</li> <li>Root Directory: S3 URL (e.g., <code>s3://my-bucket/data</code>)</li> <li>AWS Profile: Optional AWS profile name (auto-detected from environment)</li> <li>Authentication Command: Optional CLI command for automatic authentication</li> </ul>"},{"location":"web-ui/catalog-management/#google-cloud-storage-configuration","title":"Google Cloud Storage Configuration","text":"<p>Setup Steps:</p> <ol> <li>Authenticate with GCP using <code>gcloud auth application-default login</code></li> <li>Create bucket in GCP console</li> <li>Set up bucket permissions</li> <li>Test connection in the web UI</li> </ol> <p>Web UI Configuration:</p> <ul> <li>Catalog Name: Friendly name for the catalog</li> <li>Root Directory: GCS URL (e.g., <code>gs://my-bucket/data</code>)</li> <li>Authentication Command: Optional CLI command for automatic authentication</li> </ul>"},{"location":"web-ui/catalog-management/#azure-blob-storage-configuration","title":"Azure Blob Storage Configuration","text":"<p>Setup Steps:</p> <ol> <li>Authenticate with Azure using <code>az login</code></li> <li>Create storage account in Azure portal</li> <li>Create container for data storage</li> <li>Test connection in the web UI</li> </ol> <p>Web UI Configuration:</p> <ul> <li>Catalog Name: Friendly name for the catalog</li> <li>Root Directory: Azure URL (e.g., <code>az://my-container/data</code>)</li> <li>Authentication Command: Optional CLI command for automatic authentication</li> </ul>"},{"location":"web-ui/catalog-management/#authentication-command-auto-execution","title":"Authentication Command Auto-Execution","text":"<p>NEW: Kirin's web UI can automatically execute authentication commands when accessing catalogs, eliminating the need for manual CLI authentication.</p> <p>How It Works:</p> <ol> <li>Store your authentication command in the catalog configuration (e.g.,    <code>aws sso login --profile my-profile</code>)</li> <li>When authentication fails, Kirin automatically executes the command</li> <li>If successful, Kirin retries the operation and shows your datasets</li> <li>If it fails, you'll see clear error messages with manual instructions</li> </ol> <p>Setup:</p> <p>When adding or editing a catalog, fill in the \"Authentication Command (Optional)\" field:</p> <ul> <li>AWS S3: <code>aws sso login --profile {{ aws_profile }}</code></li> <li>GCP GCS: <code>gcloud auth login</code></li> <li>Azure: <code>az login</code></li> </ul> <p>Benefits:</p> <ul> <li>Automatic retry: No manual intervention needed</li> <li>Seamless UX: Authentication happens in the background</li> <li>Clear feedback: Success/failure messages shown in UI</li> <li>Manual fallback: You can still authenticate manually if auto-auth fails</li> </ul> <p>Example:</p> <pre><code>Catalog: production-s3\nRoot Directory: s3://my-production-bucket/data\nAWS Profile: production\nAuth Command: aws sso login --profile production\n\nResult: When you click on this catalog, Kirin will automatically\nrun the auth command if authentication is needed.\n</code></pre>"},{"location":"web-ui/catalog-management/#multi-catalog-workflows","title":"Multi-Catalog Workflows","text":""},{"location":"web-ui/catalog-management/#organizing-catalogs","title":"Organizing Catalogs","text":"<p>Structure your catalogs by purpose:</p> <pre><code>s3-production: s3://production-bucket/data\ngcs-production: gs://production-bucket/data\n\n# Analytics catalogs\ngcs-analytics: gs://analytics-bucket/data\nazure-ml: az://ml-container/data\n</code></pre>"},{"location":"web-ui/catalog-management/#working-with-multiple-catalogs","title":"Working with Multiple Catalogs","text":"<p>The web UI allows you to manage multiple catalogs from a single interface:</p> <ol> <li>Add multiple catalogs using the \"Add Catalog\" button</li> <li>Switch between catalogs by clicking on catalog cards</li> <li>View datasets in each catalog separately</li> <li>Edit or delete catalogs as needed</li> </ol> <p>Each catalog maintains its own datasets and commit history independently.</p>"},{"location":"web-ui/catalog-management/#authentication-management","title":"Authentication Management","text":""},{"location":"web-ui/catalog-management/#aws-authentication","title":"AWS Authentication","text":""},{"location":"web-ui/catalog-management/#using-aws-profiles","title":"Using AWS Profiles","text":"<pre><code># Configure AWS profile\naws configure --profile production\naws configure --profile development\n\n# Use in web UI: Select profile from dropdown when creating S3 catalog\n</code></pre>"},{"location":"web-ui/catalog-management/#using-environment-variables","title":"Using Environment Variables","text":"<pre><code># Set environment variables\nexport AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\nexport AWS_DEFAULT_REGION=\"us-west-2\"\n\n# Use in web UI: Leave AWS Profile empty to use environment variables\n</code></pre>"},{"location":"web-ui/catalog-management/#using-iam-roles","title":"Using IAM Roles","text":"<p>For AWS EC2, ECS, or Lambda instances that have an associated IAM role, no explicit credentials are required. The web UI will automatically use the IAM role provided to the instance for authentication.</p>"},{"location":"web-ui/catalog-management/#gcp-authentication","title":"GCP Authentication","text":""},{"location":"web-ui/catalog-management/#application-default-credentials-recommended","title":"Application Default Credentials (Recommended)","text":"<pre><code># Set up ADC (one-time setup)\ngcloud auth application-default login\n\n# Use in web UI: No additional configuration needed\n# Root Directory: gs://my-bucket/data\n</code></pre>"},{"location":"web-ui/catalog-management/#service-account-keys-advanced","title":"Service Account Keys (Advanced)","text":"<pre><code># Create service account\ngcloud iam service-accounts create kirin-service\n\n# Download key file\ngcloud iam service-accounts keys create key.json \\\n  --iam-account=kirin-service@project.iam.gserviceaccount.com\n\n# Set environment variable\nexport GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/key.json\"\n\n# Use in web UI: No additional configuration needed\n</code></pre>"},{"location":"web-ui/catalog-management/#azure-authentication","title":"Azure Authentication","text":""},{"location":"web-ui/catalog-management/#azure-cli-recommended","title":"Azure CLI (Recommended)","text":"<pre><code># Authenticate with Azure\naz login\n\n# Use in web UI: No additional configuration needed\n# Root Directory: az://my-container/data\n</code></pre>"},{"location":"web-ui/catalog-management/#connection-string-advanced","title":"Connection String (Advanced)","text":"<pre><code># Set environment variable\nexport AZURE_STORAGE_CONNECTION_STRING=\"DefaultEndpointsProtocol=https;AccountName=myaccount;AccountKey=mykey\"\n\n# Use in web UI: No additional configuration needed\n</code></pre>"},{"location":"web-ui/catalog-management/#security-best-practices","title":"Security Best Practices","text":""},{"location":"web-ui/catalog-management/#credential-management","title":"Credential Management","text":""},{"location":"web-ui/catalog-management/#secure-storage","title":"Secure Storage","text":"<ul> <li>Use IAM roles when possible instead of access keys</li> <li>Rotate credentials regularly</li> <li>Use least privilege - only grant necessary permissions</li> <li>Monitor access through cloud provider audit logs</li> </ul>"},{"location":"web-ui/catalog-management/#environment-variables","title":"Environment Variables","text":"<pre><code># Set environment variables for sensitive data\nexport AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\nexport GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/key.json\"\nexport AZURE_STORAGE_ACCOUNT_NAME=\"myaccount\"\nexport AZURE_STORAGE_ACCOUNT_KEY=\"mykey\"\n</code></pre>"},{"location":"web-ui/catalog-management/#access-control","title":"Access Control","text":""},{"location":"web-ui/catalog-management/#aws-s3-permissions","title":"AWS S3 Permissions","text":"<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::{{ account_id }}:user/{{ username }}\"\n      },\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::{{ bucket_name }}\",\n        \"arn:aws:s3:::{{ bucket_name }}/*\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"web-ui/catalog-management/#gcp-iam-permissions","title":"GCP IAM Permissions","text":"<p>For Google Cloud Storage, ensure your service account or user has the following roles:</p> <ul> <li>Storage Object Admin: For full read/write access to objects</li> <li>Storage Legacy Bucket Reader: For listing bucket contents</li> </ul>"},{"location":"web-ui/catalog-management/#troubleshooting","title":"Troubleshooting","text":""},{"location":"web-ui/catalog-management/#common-issues","title":"Common Issues","text":""},{"location":"web-ui/catalog-management/#authentication-failures","title":"Authentication Failures","text":"<pre><code># Test AWS credentials\naws s3 ls s3://{{ bucket_name }}\n\n# Test GCS credentials\ngsutil ls gs://{{ bucket_name }}\n\n# Test Azure credentials\naz storage blob list --container-name {{ container_name }}\n</code></pre>"},{"location":"web-ui/catalog-management/#permission-errors","title":"Permission Errors","text":"<pre><code># Check S3 bucket permissions\naws s3api get-bucket-policy --bucket my-bucket\n\n# Check GCS bucket permissions\ngsutil iam get gs://my-bucket\n\n# Check Azure container permissions\naz storage container show --name my-container\n</code></pre>"},{"location":"web-ui/catalog-management/#connection-issues","title":"Connection Issues","text":"<pre><code># Test network connectivity\nping s3.amazonaws.com\nping storage.googleapis.com\nping blob.core.windows.net\n\n# Check DNS resolution\nnslookup s3.amazonaws.com\nnslookup storage.googleapis.com\n</code></pre>"},{"location":"web-ui/catalog-management/#performance-issues","title":"Performance Issues","text":""},{"location":"web-ui/catalog-management/#slow-operations","title":"Slow Operations","text":"<pre><code># Use appropriate file sizes\n# Split large files into chunks\ndataset.commit(\n    message=\"Add chunked data\",\n    add_files=[\"chunk_001.csv\", \"chunk_002.csv\", \"chunk_003.csv\"]\n)\n\n# Use compression for text files\ndataset.commit(\n    message=\"Add compressed data\",\n    add_files=[\"data.csv.gz\"]\n)\n</code></pre>"},{"location":"web-ui/catalog-management/#memory-issues","title":"Memory Issues","text":"<pre><code># Use chunked processing for large files\nwith dataset.local_files() as local_files:\n    for filename, local_path in local_files.items():\n        if filename.endswith('.csv'):\n            # Process in chunks\n            for chunk in pd.read_csv(local_path, chunksize=10000):\n                process_chunk(chunk)\n</code></pre>"},{"location":"web-ui/catalog-management/#next-steps","title":"Next Steps","text":"<ul> <li>Web UI Overview - Getting started with the web interface</li> <li>Cloud Storage Guide - Detailed cloud setup</li> <li>Basic Usage Guide - Core dataset operations</li> </ul>"},{"location":"web-ui/getting-started/","title":"Getting Started with the Web UI","text":"<p>This tutorial walks you through creating your first catalog and dataset using the Kirin web interface. By the end, you'll have a working setup and understand the basic workflow.</p>"},{"location":"web-ui/getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kirin installed (see Installation Guide)</li> <li>A storage location ready (local directory or cloud bucket)</li> <li>For cloud storage: Appropriate authentication configured</li> </ul>"},{"location":"web-ui/getting-started/#step-1-start-the-web-ui","title":"Step 1: Start the Web UI","text":"<p>Open your terminal and run:</p> <pre><code># Development environment\npixi run kirin ui\n\n# Production environment\nuv run kirin ui\n\n# One-time use\nuvx kirin ui\n</code></pre> <p>The server starts and prints a URL like:</p> <pre><code>Starting Kirin Web UI...\nServer running at http://127.0.0.1:61581\n</code></pre> <p>Copy this URL and open it in your browser.</p>"},{"location":"web-ui/getting-started/#step-2-create-your-first-catalog","title":"Step 2: Create Your First Catalog","text":"<p>A catalog connects Kirin to a storage location. You need at least one catalog before you can work with datasets.</p> <p>On the home page:</p> <ol> <li>Click the blue \"+ Add Catalog\" button</li> <li>Fill in the form:</li> <li>Catalog Name: Enter a friendly name (e.g., \"My Data Catalog\")</li> <li>Root Directory: Enter your storage path:<ul> <li>Local: <code>/path/to/your/data</code></li> <li>S3: <code>s3://your-bucket-name/path</code></li> <li>GCS: <code>gs://your-bucket-name/path</code></li> <li>Azure: <code>az://your-container-name/path</code></li> </ul> </li> <li>For cloud storage (optional):</li> <li>AWS Profile: Select from dropdown if using S3</li> <li>Authentication Command: Enter CLI command (e.g., <code>aws sso login      --profile production</code>)</li> <li>Click \"Create Catalog\"</li> </ol> <p>What happens:</p> <ul> <li>The catalog appears on your home page</li> <li>You can now view datasets, edit settings, or delete it</li> <li>The catalog is ready to use immediately</li> </ul>"},{"location":"web-ui/getting-started/#step-3-create-your-first-dataset","title":"Step 3: Create Your First Dataset","text":"<p>Datasets are versioned collections of files within a catalog.</p> <p>Navigate to your catalog:</p> <ol> <li>Click \"View Datasets\" on your catalog card</li> <li>Click the \"Create Dataset\" tab</li> <li>Fill in the form:</li> <li>Dataset Name: Use lowercase letters, numbers, and hyphens only      (e.g., <code>my-first-dataset</code>)</li> <li>Description: Optional description of what this dataset contains</li> <li>Click \"Create Dataset\"</li> </ol> <p>What happens:</p> <ul> <li>You're taken to the dataset view</li> <li>The dataset is empty and ready for files</li> <li>You can see three tabs: Files, History, and Commit</li> </ul>"},{"location":"web-ui/getting-started/#step-4-add-files-to-your-dataset","title":"Step 4: Add Files to Your Dataset","text":"<p>Now let's add some files to your dataset.</p> <p>In the dataset view:</p> <ol> <li>Click the \"Commit\" tab</li> <li>Upload files:</li> <li>Drag and drop files into the upload area, or</li> <li>Click \"Choose Files\" to browse your computer</li> <li>Enter a commit message:</li> <li>Describe what you're adding (e.g., \"Add initial data files\")</li> <li>Be descriptive\u2014this helps you understand changes later</li> <li>Click \"Create Commit\"</li> </ol> <p>What happens:</p> <ul> <li>Files are uploaded and stored</li> <li>A new commit appears in the History tab</li> <li>Files appear in the Files tab</li> <li>You can preview or download files</li> </ul>"},{"location":"web-ui/getting-started/#step-5-explore-your-dataset","title":"Step 5: Explore Your Dataset","text":"<p>Take a moment to explore what you've created.</p> <p>Files Tab:</p> <ul> <li>See all files in your dataset</li> <li>Click \"Preview\" to view file contents</li> <li>Click \"Download\" to save files locally</li> </ul> <p>History Tab:</p> <ul> <li>See your commit history (should show one commit)</li> <li>Click \"Browse Files\" to see files at that point in time</li> <li>Notice the commit hash, message, and timestamp</li> </ul> <p>Commit Tab:</p> <ul> <li>Upload more files or remove existing ones</li> <li>See Python code snippets for accessing your dataset programmatically</li> </ul>"},{"location":"web-ui/getting-started/#step-6-make-another-commit","title":"Step 6: Make Another Commit","text":"<p>Let's practice the workflow by making a second commit.</p> <p>Add more files:</p> <ol> <li>Go to the \"Commit\" tab</li> <li>Upload additional files (or the same files with updates)</li> <li>Write a commit message (e.g., \"Add updated sales data\")</li> <li>Click \"Create Commit\"</li> </ol> <p>What happens:</p> <ul> <li>A second commit appears in History</li> <li>The Files tab shows the current state (latest commit)</li> <li>You can browse either commit to see differences</li> </ul>"},{"location":"web-ui/getting-started/#step-7-browse-commit-history","title":"Step 7: Browse Commit History","text":"<p>See how your dataset has changed over time.</p> <p>In the History tab:</p> <ol> <li>You should see two commits (newest first)</li> <li>Click \"Browse Files\" on the older commit</li> <li>Compare the file list with the current Files tab</li> <li>Notice how commit messages help you understand what changed</li> </ol> <p>Key concepts:</p> <ul> <li>HEAD: The current commit (marked with a blue badge)</li> <li>Commit hash: Unique identifier for each commit</li> <li>Linear history: Commits form a chain, one after another</li> </ul>"},{"location":"web-ui/getting-started/#next-steps","title":"Next Steps","text":"<p>You've completed the basic workflow! Here's what to explore next:</p> <p>Common workflows:</p> <ul> <li>Web UI How-To Guide - Learn common tasks and workflows</li> <li>Catalog Management - Configure cloud storage   and authentication</li> <li>Basic Usage Guide - Deep dive into dataset   operations</li> </ul> <p>Advanced topics:</p> <ul> <li>Set up cloud storage catalogs (S3, GCS, Azure)</li> <li>Share datasets with your team</li> <li>Use Python API alongside the web UI</li> <li>Organize multiple catalogs and datasets</li> </ul>"},{"location":"web-ui/getting-started/#troubleshooting","title":"Troubleshooting","text":"<p>Server won't start:</p> <ul> <li>Check if the port is already in use</li> <li>Kirin will automatically use a different port\u2014check the terminal output</li> </ul> <p>Can't create catalog:</p> <ul> <li>Verify your storage path is correct</li> <li>For cloud storage, ensure you're authenticated</li> <li>Check file permissions for local paths</li> </ul> <p>Files won't upload:</p> <ul> <li>Verify you have write access to the catalog's storage location</li> <li>Check file size\u2014very large files may take time</li> <li>For cloud storage, verify your authentication is working</li> </ul> <p>Can't see datasets:</p> <ul> <li>Wait a few seconds for cloud catalogs to connect</li> <li>Verify your catalog path is correct</li> <li>Check that you have read access to the storage location</li> </ul> <p>For more help, see the Troubleshooting section.</p>"},{"location":"web-ui/how-to/","title":"Web UI How-To Guide","text":"<p>Practical guides for common tasks in the Kirin web interface. Each section walks you through a specific workflow with step-by-step instructions.</p>"},{"location":"web-ui/how-to/#daily-workflow-adding-new-data","title":"Daily Workflow: Adding New Data","text":"<p>Goal: Add new files to an existing dataset.</p> <p>Steps:</p> <ol> <li>Navigate to your dataset (Home \u2192 Catalog \u2192 Dataset)</li> <li>Click the \"Commit\" tab</li> <li>Upload your files:</li> <li>Drag and drop files into the upload area, or</li> <li>Click \"Choose Files\" to browse</li> <li>Write a descriptive commit message (e.g., \"Add Q4 sales data from CRM\")</li> <li>Click \"Create Commit\"</li> </ol> <p>Tips:</p> <ul> <li>Upload multiple files in one commit for related changes</li> <li>Use clear commit messages to understand changes later</li> <li>Check the summary before committing to verify what will be added</li> </ul>"},{"location":"web-ui/how-to/#updating-existing-files","title":"Updating Existing Files","text":"<p>Goal: Replace old files with updated versions.</p> <p>Steps:</p> <ol> <li>Go to your dataset's \"Commit\" tab</li> <li>Upload files with the same names as existing files</li> <li>Write a commit message explaining the update (e.g., \"Update customer data    with latest export\")</li> <li>Click \"Create Commit\"</li> </ol> <p>What happens:</p> <ul> <li>New files replace old ones with the same names</li> <li>Old versions remain in commit history</li> <li>You can browse old commits to see previous versions</li> </ul>"},{"location":"web-ui/how-to/#removing-files","title":"Removing Files","text":"<p>Goal: Remove files you no longer need.</p> <p>Steps:</p> <ol> <li>Go to your dataset's \"Commit\" tab</li> <li>In the \"Remove Files\" section, check boxes next to files to remove</li> <li>Write a commit message explaining why (e.g., \"Remove deprecated v1 data    files\")</li> <li>Click \"Create Commit\"</li> </ol> <p>Important:</p> <ul> <li>Files are removed from the current commit but remain in history</li> <li>You can always browse old commits to see removed files</li> <li>Consider if you really want to remove files or just stop using them</li> </ul>"},{"location":"web-ui/how-to/#combining-add-and-remove-operations","title":"Combining Add and Remove Operations","text":"<p>Goal: Add new files and remove old ones in a single commit.</p> <p>Steps:</p> <ol> <li>Go to the \"Commit\" tab</li> <li>Upload new files in the upload section</li> <li>Check boxes to remove files in the remove section</li> <li>Write a commit message describing all changes</li> <li>Click \"Create Commit\"</li> </ol> <p>Use case example:</p> <ul> <li>Adding updated data files while removing outdated ones</li> <li>Reorganizing dataset structure</li> <li>Cleaning up while adding new content</li> </ul>"},{"location":"web-ui/how-to/#viewing-file-contents","title":"Viewing File Contents","text":"<p>Goal: Quickly check what's in a file without downloading it.</p> <p>Steps:</p> <ol> <li>In the \"Files\" tab, click \"Preview\" next to any file</li> <li>For text files: See syntax-highlighted content</li> <li>For images: View the image directly</li> <li>For code files: See formatted code with highlighting</li> </ol> <p>Features:</p> <ul> <li>Syntax highlighting for code files</li> <li>Line numbers for text files</li> <li>Inline image display</li> <li>Source file links for generated files (like plots)</li> </ul> <p>Limitations:</p> <ul> <li>Very large files (&gt;1000 lines) show first 1000 lines</li> <li>Binary files show a message instead of content</li> <li>Some file types may not preview perfectly</li> </ul>"},{"location":"web-ui/how-to/#comparing-versions","title":"Comparing Versions","text":"<p>Goal: See what changed between two commits.</p> <p>Steps:</p> <ol> <li>Go to the \"History\" tab</li> <li>Note the commit hashes of the two versions you want to compare</li> <li>Click \"Browse Files\" on the older commit</li> <li>Note the files and their sizes</li> <li>Navigate back and click \"Browse Files\" on the newer commit</li> <li>Compare the file lists, sizes, and contents</li> </ol> <p>What to look for:</p> <ul> <li>Files added (appear in newer but not older)</li> <li>Files removed (appear in older but not newer)</li> <li>Files changed (same name, different size or hash)</li> <li>Total size differences</li> </ul> <p>Tip: Good commit messages make it easier to understand why changes were made.</p>"},{"location":"web-ui/how-to/#setting-up-a-cloud-catalog","title":"Setting Up a Cloud Catalog","text":"<p>Goal: Connect to an S3 bucket for team collaboration.</p> <p>Prerequisites:</p> <ul> <li>AWS account with S3 access</li> <li>AWS CLI installed and configured</li> <li>Appropriate IAM permissions</li> </ul> <p>Steps:</p> <ol> <li>Ensure you're authenticated with AWS:</li> </ol> <pre><code>aws sso login --profile your-profile\n# or\naws configure\n</code></pre> <ol> <li> <p>In the web UI, click \"+ Add Catalog\"</p> </li> <li> <p>Fill in the form:</p> </li> <li>Catalog Name: Something descriptive (e.g., \"Production S3 Data\")</li> <li>Root Directory: <code>s3://your-bucket-name/data</code></li> <li>AWS Profile: Select your profile from the dropdown</li> <li> <p>Authentication Command: <code>aws sso login --profile your-profile</code>      (optional, enables auto-authentication)</p> </li> <li> <p>Click \"Create Catalog\"</p> </li> </ol> <p>What happens:</p> <ul> <li>The catalog connects to your S3 bucket</li> <li>You can now create datasets that store data in S3</li> <li>Auto-authentication runs the command when needed</li> </ul> <p>For GCS:</p> <ul> <li>Use path: <code>gs://your-bucket-name/data</code></li> <li>Auth command: <code>gcloud auth login</code> or <code>gcloud auth application-default login</code></li> </ul> <p>For Azure:</p> <ul> <li>Use path: <code>az://your-container-name/data</code></li> <li>Auth command: <code>az login</code></li> </ul>"},{"location":"web-ui/how-to/#sharing-a-dataset-with-your-team","title":"Sharing a Dataset with Your Team","text":"<p>Goal: Let teammates access a dataset you created.</p> <p>Steps:</p> <ol> <li>Ensure the catalog is accessible:</li> <li>For cloud storage: Ensure teammates have access to the bucket/container</li> <li> <p>For local storage: Use a shared directory or network drive</p> </li> <li> <p>Share catalog configuration:</p> </li> <li>Catalog name</li> <li>Root directory path</li> <li> <p>Authentication requirements (if any)</p> </li> <li> <p>Teammates set up the catalog:</p> </li> <li>They create the same catalog in their web UI</li> <li>They configure authentication if needed</li> <li> <p>They can now see all datasets in that catalog</p> </li> <li> <p>Share dataset information:</p> </li> <li>Dataset name</li> <li>Catalog it's in</li> <li>Any relevant commit hashes or descriptions</li> </ol> <p>Tips:</p> <ul> <li>Use the \"Python Access Code\" section to share exact code for   programmatic access</li> <li>Document catalog setup in your team's wiki or docs</li> <li>Consider using consistent catalog names across the team</li> </ul>"},{"location":"web-ui/how-to/#organizing-multiple-catalogs","title":"Organizing Multiple Catalogs","text":"<p>Goal: Structure your catalogs for easy management.</p> <p>Strategies:</p> <p>By environment:</p> <ul> <li><code>production-data</code> \u2192 Production S3 bucket</li> <li><code>staging-data</code> \u2192 Staging S3 bucket</li> <li><code>local-dev</code> \u2192 Local development directory</li> </ul> <p>By team:</p> <ul> <li><code>analytics-team</code> \u2192 Analytics team's shared storage</li> <li><code>ml-team</code> \u2192 Machine learning team's storage</li> <li><code>research-team</code> \u2192 Research team's storage</li> </ul> <p>By project:</p> <ul> <li><code>customer-segmentation</code> \u2192 Project-specific storage</li> <li><code>price-optimization</code> \u2192 Another project's storage</li> </ul> <p>Best practices:</p> <ul> <li>Use consistent naming conventions</li> <li>Document catalog purposes</li> <li>Keep related datasets in the same catalog</li> <li>Don't create too many catalogs\u2014group related work together</li> </ul>"},{"location":"web-ui/how-to/#finding-files-in-large-datasets","title":"Finding Files in Large Datasets","text":"<p>Goal: Quickly locate specific files when you have many files.</p> <p>Steps:</p> <ol> <li>Go to your dataset's \"Files\" tab</li> <li>Use browser search (Ctrl+F / Cmd+F) to search file names</li> <li>Look for file type icons to filter visually</li> <li>Use the dataset's search functionality if available</li> </ol> <p>Tips:</p> <ul> <li>Use descriptive filenames to make files easier to find</li> <li>Group related files with consistent naming patterns</li> <li>Consider splitting very large datasets into smaller, focused ones</li> </ul>"},{"location":"web-ui/how-to/#exporting-python-code","title":"Exporting Python Code","text":"<p>Goal: Get Python code to access your dataset programmatically.</p> <p>Steps:</p> <ol> <li>Navigate to your dataset</li> <li>Look for the \"Python Access Code\" section (collapsible, usually at    the top)</li> <li>Click to expand it</li> <li>Click \"Copy\" to copy the code</li> <li>Paste into your Python script or notebook</li> </ol> <p>What you get:</p> <ul> <li>Complete code to create a Dataset object</li> <li>Proper authentication configuration</li> <li>Ready to use in your scripts</li> </ul> <p>Example output:</p> <pre><code>from kirin import Dataset\n\ndataset = Dataset(\n    root_dir=\"s3://my-bucket/data\",\n    name=\"my-dataset\",\n    aws_profile=\"production\"\n)\n\n# Checkout to latest commit\ndataset.checkout()\n</code></pre>"},{"location":"web-ui/how-to/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"web-ui/how-to/#cant-connect-to-cloud-storage","title":"Can't Connect to Cloud Storage","text":"<p>Symptoms: Catalog shows \"Error\" status or can't list datasets.</p> <p>Solutions:</p> <ol> <li>Check authentication:</li> <li>Run the auth command manually: <code>aws sso login --profile your-profile</code></li> <li>Verify credentials are valid</li> <li> <p>Check expiration for temporary credentials</p> </li> <li> <p>Verify permissions:</p> </li> <li>Ensure your credentials have read/write access to the bucket</li> <li> <p>Check IAM policies or bucket permissions</p> </li> <li> <p>Check network:</p> </li> <li>Verify you can access the cloud storage from your network</li> <li> <p>Check for firewall or VPN issues</p> </li> <li> <p>Review error messages:</p> </li> <li>The web UI shows specific error messages</li> <li>Use these to diagnose the issue</li> </ol>"},{"location":"web-ui/how-to/#files-wont-upload","title":"Files Won't Upload","text":"<p>Symptoms: Upload fails or hangs indefinitely.</p> <p>Solutions:</p> <ol> <li>Check file size:</li> <li>Very large files (&gt;100MB) may take time</li> <li> <p>Consider splitting large files</p> </li> <li> <p>Verify permissions:</p> </li> <li>Ensure you have write access to the catalog's storage</li> <li> <p>Check cloud storage permissions</p> </li> <li> <p>Check network:</p> </li> <li>For cloud storage, verify connection is stable</li> <li> <p>Try smaller files first to test</p> </li> <li> <p>Review browser console:</p> </li> <li>Check for JavaScript errors</li> <li>Look for network request failures</li> </ol>"},{"location":"web-ui/how-to/#cant-see-datasets","title":"Can't See Datasets","text":"<p>Symptoms: Catalog shows \"Unknown\" dataset count or empty list.</p> <p>Solutions:</p> <ol> <li>Wait for connection:</li> <li>Cloud catalogs may take a few seconds to connect</li> <li> <p>Refresh the page if it seems stuck</p> </li> <li> <p>Verify path:</p> </li> <li>Ensure the catalog path is correct</li> <li> <p>Check that the storage location exists</p> </li> <li> <p>Check authentication:</p> </li> <li>Verify you're authenticated with the cloud provider</li> <li> <p>Run auth commands manually if needed</p> </li> <li> <p>Verify permissions:</p> </li> <li>Ensure you have read access to list datasets</li> <li>Check IAM policies or storage permissions</li> </ol>"},{"location":"web-ui/how-to/#best-practices","title":"Best Practices","text":""},{"location":"web-ui/how-to/#commit-messages","title":"Commit Messages","text":"<p>Write clear, descriptive messages:</p> <ul> <li>\u2705 Good: \"Add Q4 2024 sales data from CRM export\"</li> <li>\u2705 Good: \"Update customer segmentation model with new features\"</li> <li>\u274c Bad: \"update\"</li> <li>\u274c Bad: \"files\"</li> </ul> <p>Why it matters: Good commit messages help you and your team understand what changed and why, especially when looking at history weeks or months later.</p>"},{"location":"web-ui/how-to/#dataset-organization","title":"Dataset Organization","text":"<p>Keep datasets focused:</p> <ul> <li>\u2705 Good: One dataset per use case or analysis</li> <li>\u274c Bad: One giant dataset with everything</li> </ul> <p>Why it matters: Focused datasets are easier to understand, navigate, and share with others.</p>"},{"location":"web-ui/how-to/#file-naming","title":"File Naming","text":"<p>Use descriptive, consistent names:</p> <ul> <li>\u2705 Good: <code>customer-transactions-2024-q4.csv</code>, <code>model-training-features-v2.json</code></li> <li>\u274c Bad: <code>data.csv</code>, <code>file1.json</code>, <code>stuff.xlsx</code></li> </ul> <p>Why it matters: Clear names make files easier to find and understand, especially when working with multiple files.</p>"},{"location":"web-ui/how-to/#next-steps","title":"Next Steps","text":"<ul> <li>Web UI Overview - Understand the concepts and   architecture</li> <li>Getting Started Tutorial - Step-by-step first   setup</li> <li>Catalog Management - Advanced catalog   configuration</li> </ul>"},{"location":"web-ui/overview/","title":"Web UI Overview","text":"<p>The Kirin web interface provides a visual, browser-based way to manage your data versioning workflows. It's designed for users who prefer graphical interfaces over command-line tools, and for teams who need shared visibility into datasets and their history.</p>"},{"location":"web-ui/overview/#what-is-the-web-ui","title":"What Is the Web UI?","text":"<p>The web UI is a full-featured interface to Kirin's data versioning system, running as a local web server that you access through your browser. It provides the same core functionality as the Kirin CLI, but through an interactive, visual interface.</p> <p>Key characteristics:</p> <ul> <li>Self-hosted: Runs locally on your machine\u2014no external services or   accounts required</li> <li>Browser-based: Works in any modern web browser</li> <li>HTMX-powered: Fast, dynamic updates without full page reloads</li> <li>Cloud-integrated: Works seamlessly with S3, GCS, Azure, and local   storage</li> </ul>"},{"location":"web-ui/overview/#why-does-it-exist","title":"Why Does It Exist?","text":"<p>The web UI addresses several needs that the CLI doesn't:</p> <p>Visual Exploration:</p> <ul> <li>Browse datasets and files without remembering commands</li> <li>See commit history as a timeline, not just log output</li> <li>Preview file contents directly in the browser</li> </ul> <p>Team Collaboration:</p> <ul> <li>Shared understanding of dataset structure and history</li> <li>Easier onboarding for team members unfamiliar with CLI tools</li> <li>Visual representation of changes over time</li> </ul> <p>Workflow Integration:</p> <ul> <li>Quick file uploads via drag-and-drop</li> <li>Immediate visual feedback on operations</li> <li>No need to switch between terminal and file browser</li> </ul>"},{"location":"web-ui/overview/#how-it-fits-into-kirin","title":"How It Fits into Kirin","text":"<p>The web UI is one of three ways to interact with Kirin:</p>"},{"location":"web-ui/overview/#1-web-ui-this-interface","title":"1. Web UI (this interface)","text":"<ul> <li>Best for: Visual exploration, quick operations, team collaboration</li> <li>Use when: You want to browse datasets, upload files interactively, or   share views with teammates</li> </ul>"},{"location":"web-ui/overview/#2-python-api","title":"2. Python API","text":"<ul> <li>Best for: Programmatic access, automation, integration with data science   workflows</li> <li>Use when: You're writing scripts, notebooks, or applications that need to   interact with Kirin</li> </ul>"},{"location":"web-ui/overview/#3-cli","title":"3. CLI","text":"<ul> <li>Best for: Command-line workflows, automation, CI/CD pipelines</li> <li>Use when: You're comfortable with terminals and need scriptable operations</li> </ul> <p>All three interfaces work with the same underlying data. A dataset created via the web UI can be accessed via Python API or CLI, and vice versa. They're different views of the same system.</p>"},{"location":"web-ui/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"web-ui/overview/#catalogs","title":"Catalogs","text":"<p>A catalog is a connection to a storage location. Think of it as a \"data source\" that Kirin can access.</p> <p>Storage backends:</p> <ul> <li>Local filesystem: Direct access to directories on your machine</li> <li>AWS S3: Cloud storage buckets</li> <li>Google Cloud Storage: GCS buckets</li> <li>Azure Blob Storage: Azure containers</li> </ul> <p>Key properties:</p> <ul> <li>Catalogs are independent\u2014each manages its own set of datasets</li> <li>Multiple catalogs can point to different storage locations</li> <li>Catalogs handle authentication and connection details</li> </ul> <p>Mental model: A catalog is like a \"workspace\" or \"project folder\" that contains multiple datasets.</p>"},{"location":"web-ui/overview/#datasets","title":"Datasets","text":"<p>A dataset is a versioned collection of files within a catalog. It's the primary unit of organization in Kirin.</p> <p>Key properties:</p> <ul> <li>Each dataset has a name and optional description</li> <li>Files are stored with content-addressed hashing (deduplication)</li> <li>History is linear\u2014each commit has one parent</li> <li>Datasets are independent\u2014changes to one don't affect others</li> </ul> <p>Mental model: A dataset is like a git repository, but for data files instead of source code.</p>"},{"location":"web-ui/overview/#commits","title":"Commits","text":"<p>A commit is a snapshot of files at a point in time. It records what files existed, their content hashes, and a message describing the change.</p> <p>Key properties:</p> <ul> <li>Each commit has a unique hash (content-addressed)</li> <li>Commits form a linear history (no branching)</li> <li>Commit messages describe what changed and why</li> <li>You can browse files at any point in history</li> </ul> <p>Mental model: A commit is like a \"save point\" or \"checkpoint\" in your data versioning workflow.</p>"},{"location":"web-ui/overview/#files","title":"Files","text":"<p>Files in Kirin are stored by content hash, not by name. This enables:</p> <ul> <li>Deduplication: Identical content stored once, even if filenames differ</li> <li>Integrity: Content verified by hash, ensuring data hasn't changed</li> <li>Efficiency: Same file content referenced multiple times without   duplication</li> </ul> <p>Key properties:</p> <ul> <li>Files are immutable once committed</li> <li>Multiple filenames can point to the same content</li> <li>Files can have metadata (e.g., source file links for generated plots)</li> <li>Original filenames are preserved for display and access</li> </ul> <p>Mental model: Files are like \"blobs\" in git\u2014content-addressed and immutable.</p>"},{"location":"web-ui/overview/#architecture","title":"Architecture","text":""},{"location":"web-ui/overview/#client-server-model","title":"Client-Server Model","text":"<p>The web UI runs as a local web server:</p> <pre><code>Browser \u2190\u2192 Web UI Server \u2190\u2192 Storage Backend\n         (FastAPI)        (fsspec)\n</code></pre> <p>Server components:</p> <ul> <li>FastAPI application: Handles HTTP requests and routing</li> <li>Template engine: Renders HTML pages (Jinja2)</li> <li>HTMX integration: Enables dynamic updates without JavaScript</li> <li>fsspec integration: Connects to various storage backends</li> </ul> <p>Communication:</p> <ul> <li>Browser sends HTTP requests (GET for pages, POST for actions)</li> <li>Server processes requests and interacts with storage</li> <li>Responses are HTML (with HTMX for dynamic updates)</li> <li>No API endpoints\u2014everything is page-based</li> </ul>"},{"location":"web-ui/overview/#storage-abstraction","title":"Storage Abstraction","text":"<p>The web UI uses the same storage abstraction as the Python API and CLI:</p> <pre><code>Web UI \u2192 Catalog \u2192 Dataset \u2192 Storage Backend\n</code></pre> <p>Layers:</p> <ol> <li>Web UI: User interface layer</li> <li>Catalog: Connection and configuration management</li> <li>Dataset: Versioning and file management</li> <li>Storage Backend: Actual file storage (local, S3, GCS, Azure)</li> </ol> <p>This abstraction means the web UI works identically with any storage backend, without special handling for different providers.</p>"},{"location":"web-ui/overview/#authentication-flow","title":"Authentication Flow","text":"<p>For cloud storage, authentication happens at the catalog level:</p> <p>Automatic authentication:</p> <ul> <li>Catalogs can store authentication commands (e.g., <code>aws sso login --profile   production</code>)</li> <li>When operations fail due to authentication, the web UI automatically runs   the stored command</li> <li>Success/failure is communicated to the user via UI messages</li> </ul> <p>Manual authentication:</p> <ul> <li>Users can authenticate via CLI before accessing catalogs</li> <li>Authentication state persists in the user's environment</li> <li>The web UI uses existing authentication from the environment</li> </ul> <p>Security model:</p> <ul> <li>Credentials are never stored in the web UI</li> <li>Authentication commands are stored, but not credentials themselves</li> <li>The web UI executes authentication commands in the user's environment</li> </ul>"},{"location":"web-ui/overview/#when-to-use-the-web-ui","title":"When to Use the Web UI","text":"<p>Use the web UI when:</p> <ul> <li>You want to explore datasets visually</li> <li>You need to quickly upload or preview files</li> <li>You're onboarding new team members</li> <li>You want to share dataset views with non-technical stakeholders</li> <li>You prefer graphical interfaces over command-line tools</li> </ul> <p>Use the Python API when:</p> <ul> <li>You're writing scripts or notebooks</li> <li>You need programmatic access to datasets</li> <li>You're integrating Kirin into data science workflows</li> <li>You want to automate dataset operations</li> </ul> <p>Use the CLI when:</p> <ul> <li>You're comfortable with command-line tools</li> <li>You need to script operations in shell scripts</li> <li>You're working in CI/CD pipelines</li> <li>You want lightweight, fast operations</li> </ul> <p>You can mix and match: Use the web UI for exploration and the Python API for automation in the same workflow.</p>"},{"location":"web-ui/overview/#design-principles","title":"Design Principles","text":"<p>The web UI follows several design principles:</p> <p>Simplicity First:</p> <ul> <li>Linear commit history (no branching complexity)</li> <li>Clear, focused interface without overwhelming options</li> <li>Progressive disclosure\u2014advanced features don't clutter the main interface</li> </ul> <p>Fast and Responsive:</p> <ul> <li>HTMX enables dynamic updates without full page reloads</li> <li>Operations are asynchronous where possible</li> <li>Timeout protection prevents UI from hanging</li> </ul> <p>Cloud-Agnostic:</p> <ul> <li>Same interface works with any storage backend</li> <li>Storage details are abstracted away</li> <li>Users don't need to think about storage provider differences</li> </ul> <p>User-Friendly:</p> <ul> <li>Clear error messages with actionable guidance</li> <li>Visual feedback for all operations</li> <li>Helpful defaults and sensible constraints</li> </ul>"},{"location":"web-ui/overview/#limitations-and-considerations","title":"Limitations and Considerations","text":"<p>Local server:</p> <ul> <li>The web UI runs on your machine\u2014it's not a hosted service</li> <li>You need to keep the server running while using it</li> <li>Port conflicts may require using a different port</li> </ul> <p>Browser-based:</p> <ul> <li>Requires a modern web browser</li> <li>Some operations (like large file uploads) depend on browser capabilities</li> <li>Offline use is limited\u2014you need the server running</li> </ul> <p>Not for everything:</p> <ul> <li>Very large datasets (&gt;10GB) may be slow to browse</li> <li>Bulk operations are often faster via CLI or Python API</li> <li>Advanced operations may require CLI or Python API</li> </ul> <p>Security:</p> <ul> <li>The web UI runs locally and doesn't expose data to the internet</li> <li>Authentication is handled by your environment (AWS, GCS, Azure CLIs)</li> <li>No credentials are stored in the web UI itself</li> </ul>"},{"location":"web-ui/overview/#integration-with-other-tools","title":"Integration with Other Tools","text":"<p>Python API:</p> <ul> <li>The web UI shows Python code snippets for accessing datasets</li> <li>You can copy-paste code from the UI into your scripts</li> <li>Both use the same underlying Kirin library</li> </ul> <p>CLI:</p> <ul> <li>CLI operations appear in the web UI's commit history</li> <li>You can use CLI for bulk operations and web UI for exploration</li> <li>Both work with the same datasets and catalogs</li> </ul> <p>Data Science Tools:</p> <ul> <li>Datasets created in the web UI can be loaded into pandas, polars, etc.</li> <li>File previews help you understand data before loading</li> <li>Python code snippets integrate with your existing workflows</li> </ul> <p>Version Control:</p> <ul> <li>Kirin's linear history complements git's branching model</li> <li>Use git for code, Kirin for data</li> <li>Commit messages in Kirin follow similar best practices to git</li> </ul>"},{"location":"web-ui/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started Tutorial - Step-by-step guide to   your first catalog and dataset</li> <li>Web UI How-To Guide - Common workflows and tasks</li> <li>Catalog Management - Advanced catalog   configuration</li> <li>Basic Usage Guide - Core dataset   operations</li> </ul>"}]}